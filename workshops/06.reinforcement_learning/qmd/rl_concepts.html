<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.7">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Fundamentals of cognitive modeling and reinforcement learning – BayesCog - Bayesian Statistics and Hierarchical Bayesian Modeling for Psychological Science</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../workshops/06.reinforcement_learning/qmd/rl_stan.html" rel="next">
<link href="../../../workshops/06.reinforcement_learning/qmd/intro.html" rel="prev">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-fe43d0015be6fd5bd60fe62033074fe8.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-001260dd2da97ba1fc6ab87e8be34c98.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../workshops/06.reinforcement_learning/qmd/intro.html">Workshop 6: Reinforcement learning models</a></li><li class="breadcrumb-item"><a href="../../../workshops/06.reinforcement_learning/qmd/rl_concepts.html">Fundamentals of cognitive modeling and reinforcement learning</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../../index.html" class="sidebar-logo-link">
      <img src="../../../images/alpn_logo_with_text.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main tools-wide">
    <a href="https://alpnlab.github.io" title="ALPN Lab Website" class="quarto-navigation-tool px-1" aria-label="ALPN Lab Website"><i class="bi bi-globe"></i></a>
    <a href="https://github.com/alpnlab" title="ALPN Lab GitHub" class="quarto-navigation-tool px-1" aria-label="ALPN Lab GitHub"><i class="bi bi-github"></i></a>
    <a href="https://twitter.com/ALPN_Lab" title="ALPN Lab Twitter" class="quarto-navigation-tool px-1" aria-label="ALPN Lab Twitter"><i class="bi bi-twitter"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome to BayesCog!</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../course_overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course overview</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Workshop 1: R Basics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/01.R_basics/qmd/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Workshop 1: Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/01.R_basics/qmd/introduction_to_r.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to R/RStudio</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/01.R_basics/qmd/working_with_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Working with data in R</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Workshop 2: Probability and an introduction to Bayes’ theorem</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/01.R_basics/qmd/intro_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Workshop 2: Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/01.R_basics/qmd/probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability and Bayes’ theorem</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Workshop 3: Building simple models conceptually</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/02.binomial_globe/qmd/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Workshop 3: Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/02.binomial_globe/qmd/data_and_parameters.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linking data to parameters</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/02.binomial_globe/qmd/binomial_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The globe-tossing experiment</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/02.binomial_globe/qmd/mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Markov chain Monte Carlo</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Workshop 4: Introduction to building models in Stan</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/02.binomial_globe/qmd/stan_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Workshop 4: Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/02.binomial_globe/qmd/stan_binomial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Building models in Stan</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Workshop 5: Bernoulli and linear regression models</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/03.bernoulli_coin/qmd/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Workshop 5: Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/03.bernoulli_coin/qmd/stan_notes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Technical notes on Stan</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/03.bernoulli_coin/qmd/bernoulli_and_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bernoulli and linear regression models</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Workshop 6: Reinforcement learning models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/06.reinforcement_learning/qmd/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Workshop 6: Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/06.reinforcement_learning/qmd/rl_concepts.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Fundamentals of cognitive modeling and reinforcement learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/06.reinforcement_learning/qmd/rl_stan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Implementing the Rescorla-Wagner model in Stan</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Workshop 7: Hierachical Bayesian modeling</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/06.reinforcement_learning/qmd/hrch_rl_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Workshop 7: Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/06.reinforcement_learning/qmd/hierarchical_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hierarchical Bayesian models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/07.optm_rl/qmd/optim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reparameterization and optimizing Stan code</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">Workshop 8: Model comparison</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/08.compare_models/qmd/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Workshop 8: Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/08.compare_models/qmd/model_comparison.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model comparison</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Workshop 9: Debugging in Stan</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/09.debugging/qmd/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Workshop 9: Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/09.debugging/qmd/debugging.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Debugging in Stan</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/09.debugging/qmd/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A principled modeling workflow</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Bonus workshop: Introduction to model-based fMRI</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../workshops/10.model_based/qmd/model_fmri.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to model-based fMRI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Resources</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#computational-and-cognitive-modeling" id="toc-computational-and-cognitive-modeling" class="nav-link active" data-scroll-target="#computational-and-cognitive-modeling">Computational and cognitive modeling</a>
  <ul class="collapse">
  <li><a href="#cognitive-models-as-mathematical-functions" id="toc-cognitive-models-as-mathematical-functions" class="nav-link" data-scroll-target="#cognitive-models-as-mathematical-functions">Cognitive models as mathematical functions</a></li>
  </ul></li>
  <li><a href="#introduction-to-reinforcement-learning" id="toc-introduction-to-reinforcement-learning" class="nav-link" data-scroll-target="#introduction-to-reinforcement-learning">Introduction to reinforcement learning</a>
  <ul class="collapse">
  <li><a href="#reinforcement-learning-models-the-rescorla-wagner-choice-rule" id="toc-reinforcement-learning-models-the-rescorla-wagner-choice-rule" class="nav-link" data-scroll-target="#reinforcement-learning-models-the-rescorla-wagner-choice-rule">Reinforcement learning models: the Rescorla-Wagner choice rule</a></li>
  </ul></li>
  <li><a href="#modeling-the-two-choice-task-using-the-rescorla-wagner-model" id="toc-modeling-the-two-choice-task-using-the-rescorla-wagner-model" class="nav-link" data-scroll-target="#modeling-the-two-choice-task-using-the-rescorla-wagner-model">Modeling the two-choice task using the Rescorla-Wagner model</a>
  <ul class="collapse">
  <li><a href="#understanding-the-learning-rate" id="toc-understanding-the-learning-rate" class="nav-link" data-scroll-target="#understanding-the-learning-rate">Understanding the learning rate</a></li>
  </ul></li>
  <li><a href="#choice-rules" id="toc-choice-rules" class="nav-link" data-scroll-target="#choice-rules">Choice rules</a>
  <ul class="collapse">
  <li><a href="#greedy" id="toc-greedy" class="nav-link" data-scroll-target="#greedy">Greedy</a></li>
  <li><a href="#ε-greedy" id="toc-ε-greedy" class="nav-link" data-scroll-target="#ε-greedy"><span class="math inline">\(ε\)</span>-Greedy</a></li>
  <li><a href="#softmax" id="toc-softmax" class="nav-link" data-scroll-target="#softmax">Softmax</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/sohaamir/BayesCog/edit/main/workshops/06.reinforcement_learning/qmd/rl_concepts.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/sohaamir/BayesCog/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../workshops/06.reinforcement_learning/qmd/intro.html">Workshop 6: Reinforcement learning models</a></li><li class="breadcrumb-item"><a href="../../../workshops/06.reinforcement_learning/qmd/rl_concepts.html">Fundamentals of cognitive modeling and reinforcement learning</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Fundamentals of cognitive modeling and reinforcement learning</h1>
<p class="subtitle lead">Constructing a simple model of behaviour through the Rescorla-Wagner equation and choice rules</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="quote-large" style="font-size: 1.3em">
<blockquote class="blockquote">
<p>“Essentially, all the models are wrong, but some are useful.” - George E.P. Box (1976)</p>
</blockquote>
</div>
<section id="computational-and-cognitive-modeling" class="level2">
<h2 class="anchored" data-anchor-id="computational-and-cognitive-modeling">Computational and cognitive modeling</h2>
<p>For centuries, astronomers believed that planets moved in perfect circular orbits around the Sun. This mathematical model made intuitive sense and could roughly predict planetary positions, but it failed to fully capture the observed data. The breakthrough came when Johannes Kepler proposed that planets actually follow elliptical orbits; this new mathematical model not only better explained existing observations but also made more accurate predictions about planetary positions.</p>
<div style="height: 15px;">

</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../images/06.reinforcement_learning/orbit.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<div class="image-caption" style="font-size: 1.3em;">
<p>Orbiting patterns of the Earth around the Sun according to the circular and eliptical models</p>
</div>
<p>This historical example illustrates a fundamental principle of scientific modeling: <strong>we use mathematics to approximate and understand processes that we cannot directly observe.</strong></p>
<p>We essentially apply this same principle to understand human behaviour. Just as astronomers couldn’t directly see the shape of Earth’s orbit but could infer it through mathematical modeling, cognitive scientists cannot directly observe the mental processes occurring in someone’s mind. Instead, we create mathematical models that attempt to explain observed behavioural patterns – reaction times in a decision-making task, patterns of errors in memory recall, or neural activity during maze navigation.</p>
<p><strong>For instance, when we observe that people take longer to make decisions when faced with more options, or in response to making an error, we can create mathematical models that propose specific cognitive mechanisms to explain these patterns.</strong> These models generate precise predictions about behaviour under different conditions, which we can test experimentally. When a model successfully predicts behaviour across multiple scenarios, it suggests that we may have captured something meaningful about the underlying cognitive processes involved.</p>
<p>Therefore, computational modeling is nothing new, it’s specific application towards understanding human behaviour is.</p>
<p>Computational modeling in cognitive science takes two main approaches, as illustrated in the image below<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>: theory-driven (top-down) and data-driven (bottom-up).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../images/06.reinforcement_learning/theory_and_data_driven.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%"></p>
</figure>
</div>
<div class="image-caption" style="font-size: 1.4em;">
<p>Theory and data-driven approaches to computational modeling</p>
</div>
<p>Data-driven approaches - typically used for prediction and classification - start with data (e.g., behavioural or neural) and use statistical techniques to identify patterns and relationships.</p>
<p>On-the-other-hand, <strong>theory-driven approaches start with hypotheses about cognitive mechanisms, formalized into mathematical models that make specific predictions about behaviour.</strong> The predictions are then tested against empirical data, leading to refinement of both model and theory. This approach aims to provide mechanistic explanations of cognitive processes - in other words, to explain not just <em>what</em> behaviour occurs, but <em>how</em> and <em>why</em> it occurs.</p>
<p><strong>We will exclusively focus on the theory-driven approach in this course.</strong></p>
<section id="cognitive-models-as-mathematical-functions" class="level3">
<h3 class="anchored" data-anchor-id="cognitive-models-as-mathematical-functions">Cognitive models as mathematical functions</h3>
<p>The basic premise of cognitive modeling lies with <strong>understanding the brain as an information processing system that can be described mathematically.</strong> Generally, the processing of some input by a function to generate output is given as:</p>
<p><span class="math display">\[y = f(x)\]</span></p>
<p>where <span class="math inline">\(x\)</span> represents input to the system, <span class="math inline">\(f\)</span> represents the function, and <span class="math inline">\(y\)</span> represents the output.</p>
<p>To describe the brain as a mathematical ‘engine’, we can simply replace each of these with the external stimulus, cognitive processes and observed behaviour respectively.</p>
<p>But this interpretation of the brain is nothing new, in fact the psychologist Kurt Lewin in 1936<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> demonstrated a similar framework with his equation:</p>
<p><span class="math display">\[B = f(P,E)\]</span></p>
<p>where <span class="math inline">\(B\)</span> represents behavior, <span class="math inline">\(P\)</span> represents the person (their internal state and processes), and <span class="math inline">\(E\)</span> represents the environment.</p>
<p>The relationship between computational models and observable data can also be understood in terms of forward modeling and model inversion <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>:</p>
<p>The image below depicts an example participant completing a behavioural task. For example - relevant to this course - they may be completing the two-choice reversal learning task where they select which image they think is going to give them a reward by pressing the associated button.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../images/06.reinforcement_learning/cognitive_modeling.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
<div class="image-caption" style="font-size: 1.3em;">
<p>The cognitive modeling framework consists of forward models, which predict data from theory, and model inversion, which infer cognitive parameters from observations</p>
</div>
<p><strong>The forward model represents how we believe the cognitive process generates the data we observe.</strong> For instance, in reversal learning, our model might specify how a participant updates their beliefs about reward probabilities and translates these into choices. When applied to neuroimaging, it might describe how cognitive processes should manifest in observed brain activity patterns.</p>
<p><strong>Model inversion works in the opposite direction - it uses the observed data to infer the underlying parameters or states of our model.</strong> In behavioural analysis, if we observe a participant consistently choosing one option over another, model inversion helps us infer their underlying reward expectations or learning rates. In neuroimaging, when we observe different patterns of neural activity to faces versus houses, model inversion allows us to infer which brain regions are preferentially involved in processing each category.</p>
<p>The right side of the image above shows a computational model (top) that can generate simulated data (bottom). By comparing this simulated data with real observations (shown by the red horizontal arrows), we can assess how well our model captures the actual cognitive processes. If the model-generated data closely matches the observed data, we gain confidence that our model approximates the underlying cognitive mechanisms.</p>
<p><strong>This bidirectional approach of forward modeling and model inversion is powerful because it allows us to both predict behaviour based on theory and infer hidden cognitive processes from observable data.</strong></p>
<p>The quote by George Box at the top of this page is very important to remember when understanding cognitive models; that no model is correct, some are just more correct than others. In some ways, the quote can be reformulated as follows:</p>
<div class="quote-large" style="font-size: 1.3em">
<blockquote class="blockquote">
<p>“Essentially all the models are <strong>imperfect</strong>, but some are useful.”</p>
</blockquote>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Model-based fMRI: Linking computation to brain activity">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Model-based fMRI: Linking computation to brain activity
</div>
</div>
<div class="callout-body-container callout-body">
<p>Traditional fMRI analyses can tell us which brain regions are more active during different task conditions, but they can’t reveal how specific cognitive computations are implemented in the brain. <strong>Model-based fMRI solves this by linking computational models to neural activity.</strong> We first fit our model to behaviour to estimate trial-by-trial variables (like prediction errors or uncertainty), then use these estimates as regressors in our fMRI analysis to identify active brain regions.</p>
<p>An introduction to model-based fMRI is included as a bonus workshop in the course!</p>
</div>
</div>
</section>
</section>
<section id="introduction-to-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-reinforcement-learning">Introduction to reinforcement learning</h2>
<p>Humans constantly interact with their environment, making decisions and learning from their outcomes. A child learning to ride a bike, a student solving mathematical problems, or an adult learning to navigate a new city - all these scenarios involve persistent learning through trial and error. How does this learning occur? How do we know which actions lead to better outcomes?</p>
<p>Reinforcement learning (RL) has emerged as one of the most powerful frameworks for understanding how organisms learn from experience. <strong>At its core, RL describes how we learn to make decisions that maximize rewards and minimize punishments through our interactions with the environment.</strong> When an action leads to a positive outcome, we’re more likely to repeat it; when it leads to a negative outcome, we tend to avoid it in the future.</p>
<p>Reinforcement learning initially emerged in computer science, gaining popularity in the 1980s. However, it was Richard Sutton and Andrew Barto’s 1998 book “Reinforcement Learning: An Introduction”<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>, which had a significant impact on the fields of biology and psychology. Even though Sutton and Barto were computer scientists developing artificial intelligence algorithms, their mathematical framework turned out to describe biological learning with remarkable accuracy. The key concepts they formalized - such as prediction errors and value functions - aligned surprisingly well with how the brain appears to process rewards and guide behaviour.</p>
<div style="height: 20px;">

</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../images/06.reinforcement_learning/sutton_barto.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<div class="image-caption" style="font-size: 1.3em;">
<p>The first and second edition of Sutton and Barto’s influential textbook</p>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="A must read">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
A must read
</div>
</div>
<div class="callout-body-container callout-body">
<p>Whilst mathematically dense for most, <em>Reinforcement learning: An introduction</em> is nonetheless recommended reading!</p>
</div>
</div>
<p>You can see this exponential increase in popularity for cognitive modeling over time in the diagram below showing the relative frequency of PubMed entries for ‘cognitive’ and ‘cognitive and computational’ as a function of the year<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>:</p>
<div style="height: 20px;">

</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../images/06.reinforcement_learning/cognitive_modeling_boom.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
<div class="image-caption" style="font-size: 1.3em;">
<p>Note that the increase in ‘Cognitive and computational’ is almost 3x greater than `Cognitive’ alone!</p>
</div>
<section id="reinforcement-learning-models-the-rescorla-wagner-choice-rule" class="level3">
<h3 class="anchored" data-anchor-id="reinforcement-learning-models-the-rescorla-wagner-choice-rule">Reinforcement learning models: the Rescorla-Wagner choice rule</h3>
<p>To understand how humans learn from experience through reinforcement learning, scientists need controlled experimental paradigms that can measure learning and decision-making. The two-armed bandit task has emerged as a popular option because it captures fundamental aspects of behaviour in a simple, controlled setting.</p>
<p>Named after slot machines (or “one-armed bandits”), the task presents participants with two choices (like two slot machines) that deliver rewards with different probabilities or amounts. Just as a gambler must learn which slot machine pays out more frequently, participants aren’t told these properties in advance; <strong>they must discover through trial and error which option yields more reward.</strong></p>
<p>The exact properties of the bandits can be different across tasks (i.e., reward probability, magnitude), but the underlying structure remains the same.</p>
<div style="height: 20px;">

</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../images/06.reinforcement_learning/two_armed_bandit.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%"></p>
</figure>
</div>
<div class="image-caption" style="font-size: 1.3em;">
<p>Whilst the two-armed bandit does not necessarily feature slot machines, its structure reflects the same underlying properties; a choice between two options where the probability of reward is not known</p>
</div>
<p>Should you stick with an option that seems good based on your limited experience, or try the other option to gather more information? This creates an engaging dynamic where participants must balance:</p>
<ul>
<li><strong>Predicting values:</strong> Participants need to determine how rewarding each option is</li>
<li><strong>Making choices:</strong> They must decide which option to select based on their belief</li>
<li><strong>Learning from outcomes:</strong> After each choice, they update their predictions based on whether they received a reward or not</li>
</ul>
<p>But how does reinforcement learning theories explain behaviour observed in these tasks?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../images/06.reinforcement_learning/rl_diagram.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%"></p>
</figure>
</div>
<div class="image-caption" style="font-size: 1.3em;">
<p>The key principles of reinforcement learning, illustrating the basic interaction cycle between an agent and its environment and the key cognitive processes underlying decision-making</p>
</div>
<p>The left side describes the fundamental overview of reinforcement learning, which can be understood as a continuous interaction between an agent (e.g., a human) and their environment (e.g., a behavioural task)<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>:</p>
<p>This interaction follows a simple cycle:</p>
<ol type="1">
<li>The agent observes their environment</li>
<li>Based on these observations, they take actions</li>
<li>These actions affect the environment</li>
<li>The environment provides feedback (rewards or punishments)</li>
<li>The agent learns from this feedback to improve future decisions</li>
</ol>
<p><strong>This process - guided by a goal (such as maximizing rewards) - allows agents to learn optimal behavior through experience.</strong></p>
<p>To this end, RL incorporates value-based decision-making, by which learned values guide choices between different options. On the right, five basic processes of value-based decision-making are depicted<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>:</p>
<ol type="1">
<li><strong>Representation:</strong> The brain first needs to understand the decision scenario by mapping out the available choices, understanding its own internal state (like hunger or fatigue), and recognizing relevant external factors in the environment</li>
<li><strong>Valuation:</strong> Each possible action is assigned a subjective value based on previous experiences and current circumstances</li>
<li><strong>Action selection:</strong> Using these value estimates, the brain selects and executes one of the available actions</li>
<li><strong>Outcome evaluation:</strong> Once an action is taken, the brain evaluates how rewarding or beneficial the actual outcome was</li>
<li><strong>Learning:</strong> The brain uses this outcome information to refine its decision-making process - adjusting how it represents problems, values options, and makes choices in future situations</li>
</ol>
</section>
</section>
<section id="modeling-the-two-choice-task-using-the-rescorla-wagner-model" class="level2">
<h2 class="anchored" data-anchor-id="modeling-the-two-choice-task-using-the-rescorla-wagner-model">Modeling the two-choice task using the Rescorla-Wagner model</h2>
<p>How might we practically apply RL models to behavioural choice data? Let’s revisit the two-choice task, specifically the structure that we first introduced in Workshop 2.</p>
<div style="height: 20px;">

</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../images/06.reinforcement_learning/two_choice_task_stages.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<div class="image-caption" style="font-size: 1.3em;">
<p>The two alternative forced-choice task, where subjects must select between two fractals</p>
</div>
<p>Recall that in this task:</p>
<ol type="1">
<li><p>Subjects are presented with two fractals (choice presentation)</p></li>
<li><p>They select one which they think will reward them (action selection)</p></li>
<li><p>They recieve a outcome (either a reward or a loss) (outcome)</p></li>
</ol>
<p>In this task the choice and outcome are our data. From this data, we can subsequently measure basic summary statistics such as choice accuracy, which will provide us with a basic measure of subject performance. We can also compare classify performance within the group by a median split, and correlate it with other recorded variables (e.g., age, IQ).</p>
<p><strong>However, what we would like to do in cognitive modeling is to use the RL framework to describe the latent processes underlying participant choice.</strong></p>
<p>When trying to understand human behaviour through cognitive modeling, we typically need two complementary parts to our models: <strong>the cognitive model and the observation model.</strong></p>
<ul>
<li><p>A <strong>cognitive model describes the internal mental processes we think are happening</strong> - like how people calculate values, make predictions, or learn from errors. This part deals with the “hidden” or “latent” processes we can’t directly observe</p></li>
<li><p>An <strong>observation model connects these internal processes to actual measurable behaviour</strong> - like the choices people make or their reaction times</p></li>
</ul>
<p>The Rescorla-Wagner (RW) model<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> is a cognitive model; one of the most influential in psychology.</p>
<p>The Rescorla-Wagner model, developed by Robert Rescorla and Allan Wagner in 1972 to explain animal learning in classical conditioning experiments, is a cornerstone of human learning research and modern reinforcement learning theory. The base model is simple: <strong>learning is driven by prediction errors - the difference between what we expect and what actually happens.</strong> When our predictions are wrong, we learn and adjust our expectations. When they’re right, we make smaller adjustments or none at all.</p>
<p>Mathematically, the model consists of two key equations:</p>
<p>the Value Update:</p>
<p><span class="math display">\[V_t = V_{t-1} + \alpha PE_{t-1}\]</span></p>
<p>and the Prediction Error:</p>
<p><span class="math display">\[PE_{t-1} = R_{t-1} - V_{t-1}\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(V_t\)</span> is the value/expectation for the current trial <span class="math inline">\(t\)</span></p></li>
<li><p><span class="math inline">\(V_{t-1}\)</span> is the value/expectation from the previous trial</p></li>
<li><p><span class="math inline">\(\alpha\)</span> is the learning rate</p></li>
<li><p><span class="math inline">\(PE_{t-1}\)</span> is the prediction error from the previous trial</p></li>
<li><p><span class="math inline">\(R_{t-1}\)</span> is the reward received on the previous trial</p></li>
</ul>
<p>The Rescorla-Wagner model can be alternatively expressed in plain language as:</p>
<p><span class="math display">\[
\text{\scriptsize Expectations on the next trial} = \text{\scriptsize expectation on the current trial} + \text{\scriptsize learning rate} \times \text{\scriptsize prediction error}
\]</span></p>
<p>The Rescorla-Wagner model demonstrates that learning isn’t about simply forming associations - <strong>it’s about reducing prediction errors.</strong> The larger the prediction error, the more we adjust our expectations.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Expanding the Rescorla-Wagner model">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expanding the Rescorla-Wagner model
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Rescorla-Wagner model still proves remarkably versatile in understanding human behaviour, but modern applications have expanded on this foundation. For instance, more complex models might add separate learning rates for positive and negative outcomes (<span class="math inline">\(\alpha^+\)</span> and <span class="math inline">\(\alpha^-\)</span>), or additional parameters for the unchosen option. We will practially implement the latter (Counterfactual RL) in Workshop 8.</p>
</div>
</div>
<section id="understanding-the-learning-rate" class="level3">
<h3 class="anchored" data-anchor-id="understanding-the-learning-rate">Understanding the learning rate</h3>
<p>The learning rate <span class="math inline">\((α)\)</span> is a crucial parameter that determines how much we update our expectations based on new information. It ranges from 0 to 1 and acts like a filter on how much of the prediction error influences our value update.</p>
<p>You can see the influence of the learning rate on trial-by-trial value and value-updating in the graph below <a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>:</p>
<div style="height: 20px;">

</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../images/06.reinforcement_learning/learning_rates.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
<div class="image-caption" style="font-size: 1.3em;">
<p>The trial-by-trial effect of different learning rates on value updating</p>
</div>
<p>Plotted for simulated data from a 75:25 reward contingency, the graph demonstrates that:</p>
<ul>
<li><p>Higher learning rates <span class="math inline">\((α = 0.9)\)</span> show sharp, rapid changes in value</p></li>
<li><p>Lower learning rates <span class="math inline">\((α = 0.3)\)</span> show more gradual, smoother changes</p></li>
<li><p>When feedback switches, higher learning rates lead to more dramatic shifts in value</p></li>
</ul>
<p>We can mathematically determine how past outcomes are re-weighted under different learning rates by simply re-writing the Rescorla-Wagner equation as a function of the initial value and the outcome per trial:</p>
<p>From it’s original form:</p>
<p><span class="math display">\[V_t = V_{t-1} + \alpha * PE_{t-1}\]</span></p>
<p><span class="math display">\[PE_{t-1} = R_{t-1} - V_{t-1}\]</span></p>
<p>to:</p>
<p><span class="math display">\[V_t = (1-\alpha)V_{t-1} + \alpha R_{t-1}\]</span></p>
<p>which when expanded further gives:</p>
<p><span class="math display">\[V_t = (1-\alpha)^t V_0 + \sum_{i=1}^{t-i}(1-\alpha)^{i}\alpha R_i\]</span></p>
<p>This formula above describes <strong>how much outcomes in the past contribute to the current value computation.</strong> To understand what this means more clearly, we can plot the weighting of previous trials for different values of <span class="math inline">\(\alpha\)</span>:</p>
<div style="height: 20px;">

</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../images/06.reinforcement_learning/past_trial_weight.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<div class="image-caption" style="font-size: 1.3em;">
<p>The effect of different learning rates on the weights of past outcomes</p>
</div>
<p>Collectively this graph demonstrates that:</p>
<ul>
<li><p>Recent outcomes have stronger weights than distant ones</p></li>
<li><p>Higher learning rates <span class="math inline">\((\alpha = 0.9)\)</span> create a steeper recency gradient</p></li>
<li><p>Lower learning rates <span class="math inline">\((\alpha = 0.3)\)</span> create a more uniform weighting of past outcomes</p></li>
<li><p>The weight of any particular outcome decays exponentially with time, controlled by <span class="math inline">\((1-\alpha)\)</span></p></li>
</ul>
<p><strong>This explains why higher learning rates lead to more volatile value estimates - they place much more weight on recent outcomes compared to historical ones.</strong></p>
<p>But what is the ideal learning rate? Is there an ideal learning rate? The ideal learning rate ultimately depends on the environment, including the reward probability and volatility:</p>
<div style="height: 20px;">

</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../images/06.reinforcement_learning/task_dynamics.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>Whilst a higher learning rate would be more appropriate for the shorter 40-trial condition with the 85:15 reward schedule, as there’s a clear difference between options and limited time to learn, in contrast, the 100-trial condition with the more subtle 75:25 reward difference might benefit from a lower learning rate to avoid being overly influenced by random reward variations. Similarly, in a reversal learning task where the reward contingencies switch, if the reversals are sparse, lower learning rates are optimal, whereas in environments with frequent reversals, higher learning rates become necessary to quickly adapt to new contingencies.</p>
<div style="height: 20px;">

</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../images/06.reinforcement_learning/optimal_learning_rate.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p><strong>However, there are optimal learning rates for specific task environments.</strong> The graph above shows choice accuracy as a function of learning rate, demonstrating that for a non-reversing contingency of 80:20, a learning rate of around 0.18 yields the highest choice accuracy (about 85%), with performance declining for both higher and lower rates. This reflects the trade-off between stability and adaptability in learning: <strong>too low a learning rate means slow adaptation, while too high a rate leads to over-sensitivity to noise.</strong></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Running simulations">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Running simulations
</div>
</div>
<div class="callout-body-container callout-body">
<p>The graphs above were generated by running simulations, and not using actual choice data. By running simulations on different trial lengths, we can determine the ideal task environment to elicit more/less learning.</p>
</div>
</div>
</section>
</section>
<section id="choice-rules" class="level2">
<h2 class="anchored" data-anchor-id="choice-rules">Choice rules</h2>
<p>In our framework earlier, we discussed two components of our computational model: the cognitive model and the observation model. Now that we have our cognitive model in the Rescorla-Wagner choice rule, we need a way to translate these internal values into observable choices. This is where choice rules come in - they serve as the “observation model” that connects our internal value estimates to actual decisions.</p>
<p><strong>Choice rules are mathematical functions that specify how internal value differences are mapped onto choice probabilities.</strong></p>
<p>Let’s examine three common choice rules:</p>
<section id="greedy" class="level3">
<h3 class="anchored" data-anchor-id="greedy">Greedy</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../images/06.reinforcement_learning/greedy.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%"></p>
</figure>
</div>
<div class="image-caption" style="font-size: 1.3em;">
<p>The ‘greedy’ choice rule plotted using choice probability as a function of value difference</p>
</div>
<p>The simplest choice rule is the “greedy” approach:</p>
<p><span class="math display">\[p(C = a) = \begin{cases}
1, &amp; \text{if } V(a) &gt; V(b) \\
0, &amp; \text{if } V(a) &lt; V(b)
\end{cases}\]</span></p>
<p>As shown in the graph, this creates a step function where one always chooses the option with the higher value. However, this rule is unrealistic - humans rarely behave this consistently.</p>
</section>
<section id="ε-greedy" class="level3">
<h3 class="anchored" data-anchor-id="ε-greedy"><span class="math inline">\(ε\)</span>-Greedy</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../images/06.reinforcement_learning/e_greedy.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%"></p>
</figure>
</div>
<div class="image-caption" style="font-size: 1.3em;">
<p>The ‘<span class="math inline">\(ε\)</span>-greedy’ choice rule plotted using choice probability as a function of value difference</p>
</div>
<p>A slightly more sophisticated version is the “ε-Greedy” approach which adds random exploration:</p>
<p><span class="math display">\[p(C = a) = \begin{cases}
1-\varepsilon, &amp; \text{if } V(a) &gt; V(b) \\
\varepsilon, &amp; \text{if } V(a) &lt; V(b)
\end{cases}\]</span></p>
<p>In this rule, one usually chooses the higher-valued option (with probability <span class="math inline">\(1-ε\)</span>) but will sometimes explores the lower-valued option (with probability <span class="math inline">\(ε\)</span>). Whilst more realistic than the greedy approach, this still creates an unrealistic sharp transition in choice probabilities.</p>
</section>
<section id="softmax" class="level3">
<h3 class="anchored" data-anchor-id="softmax">Softmax</h3>
<div style="height: 20px;">

</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../images/06.reinforcement_learning/softmaxes.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<div class="image-caption" style="font-size: 1.3em;">
<p>The ‘softmax’ choice rule plotted using choice probability as a function of value difference</p>
</div>
<p>A commonly impored and more psychologically plausible choice rule is the softmax function:</p>
<p><span class="math display">\[p_t(A) = \frac{1}{1 + e^{-\tau(V_t(A)-V_t(B))}}\]</span></p>
<p>The softmax rule is particularly important because it creates a smooth, sigmoid relationship between value differences and choice probabilities. <strong>It does this by converting discrete choices into continuous probabilities, which fall between 0 and 1 and vary smoothly with value differences.</strong></p>
<p>Notably, the inverse temperature parameter <span class="math inline">\(τ\)</span> (also called the ‘choice consistency parameter’) controls how deterministic choices are within the softmax equation:</p>
<ul>
<li>Higher values of <span class="math inline">\(τ\)</span> (e.g., <span class="math inline">\(τ = 5\)</span>) leads to more deterministic choices, which is closer to greedy</li>
<li>Lower values of <span class="math inline">\(τ\)</span> (e.g., <span class="math inline">\(τ = 0.3\)</span>) leads to more random choices</li>
<li>When <span class="math inline">\(τ = 1,\)</span> small value differences lead to proportionally small differences in choice probabilities</li>
</ul>
<p>Whilst the softmax rule demonstrates increased choice consistency as value differences increase, <strong>it still allows for some randomness in choice even with large value differences.</strong> Ultimately, the softmax function has become the standard choice rule in cognitive modeling over greedy rules, because it captures key aspects of human decision-making: we generally choose better options more often, but our choices remain probabilistic rather than deterministic.</p>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Gauld, C., Dumas, G., Fakra, E., Mattout, J., &amp; Micoulaud-Franchi, J. A. (2021, January). The three cultures of computational psychiatry.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Lewin, K. (1936). Principles of topological psychology. McGraw-Hill.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Farrell, S., &amp; Lewandowsky, S. (2018). Computational modeling of cognition and behavior. Cambridge University Press.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Stephan, K. E., Manjaly, Z. M., Mathys, C. D., Weber, L. A., Paliwal, S., Gard, T., … &amp; Petzschner, F. H. (2016). Allostatic self-efficacy: A metacognitive theory of dyshomeostasis-induced fatigue and depression. Frontiers in human neuroscience, 10, 550.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Ahn, W. Y., Haines, N., &amp; Zhang, L. (2017). Revealing neurocomputational mechanisms of reinforcement learning and decision-making with the hBayesDM package. Computational Psychiatry (Cambridge, Mass.), 1, 24.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Sutton, R. S., &amp; Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Palminteri, S., Wyart, V., &amp; Koechlin, E. (2017). The importance of falsification in computational cognitive modeling. Trends in cognitive sciences, 21(6), 425-433.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Jain, R. (2023). RL: A gentle introduction. Medium.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Rangel, A., Camerer, C., &amp; Montague, P. R. (2008). A framework for studying the neurobiology of value-based decision making. Nature reviews neuroscience, 9(7), 545-556.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Rescorla, R. A., Wagner, A.W. (1972). A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement. Classical conditioning II: Current theory and research/Appleton-Century-Crofts.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Zhang, L., Lengersdorff, L., Mikus, N., Gläscher, J., &amp; Lamm, C. (2020). Using reinforcement learning models in social neuroscience: frameworks, pitfalls and suggestions of best practices. Social Cognitive and Affective Neuroscience, 15(6), 695-707.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/sohaamir\.github\.io\/BayesCog\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../workshops/06.reinforcement_learning/qmd/intro.html" class="pagination-link" aria-label="Workshop 6: Overview">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Workshop 6: Overview</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../workshops/06.reinforcement_learning/qmd/rl_stan.html" class="pagination-link" aria-label="Implementing the Rescorla-Wagner model in Stan">
        <span class="nav-page-text">Implementing the Rescorla-Wagner model in Stan</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Zhang &amp; Sohail, 2025</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/sohaamir/BayesCog/edit/main/workshops/06.reinforcement_learning/qmd/rl_concepts.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/sohaamir/BayesCog/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with 🧠, ☕ and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>