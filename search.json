[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Here are a selection of resources available to help with this course as well as more generally."
  },
  {
    "objectID": "resources.html#textbooks",
    "href": "resources.html#textbooks",
    "title": "Resources",
    "section": "Textbooks",
    "text": "Textbooks\n\nR Programming\n\n\n\n\n\nKabacoff, R. (2015). R in Action, 2nd Ed. Manning Publications.\nWickham, H., & Grolemund, G. (2017). R for Data Science. O’Reilly Media.\n\n\nGeneral statistics\n\n\n\n\n\nPoldrack, R. A. (2019). Statistical Thinking for the 21st Century. http://statsthinking21.org/\n\n\nBayesian statistics and modeling in Stan\n\n\n\n\n\nLambert, B. (2018). A Student’s Guide to Bayesian Statistics. Sage.\nMcElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan, 2nd Ed. CRC Press.\nKruschke, J. K. (2015). Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan, 2nd Ed. Academic Press.\nPruim, R. (2019). (Re)Doing Bayesian data analysis. https://rpruim.github.io/Kruschke-Notes/\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis, 3rd Ed. Chapman and Hall/CRC.\n\n\n\n\n\n\n\nWhich textbook should I start with?\n\n\n\nIt is recommended that beginners to Bayesian statistics should work their way down from this list starting with A Student’s Guide to Bayesian Statistics, whilst those with some experience should start with Statistical Rethinking: A Bayesian Course with Examples in R and Stan.\n\n\n\nCognitive modeling\n\n\n\n\n\nFarrell, S., & Lewandowsky, S. (2018). Computational Modeling of Cognition and Behavior. Cambridge University Press.\nLee, M. D., & Wagenmakers, E. J. (2014). Bayesian Cognitive Modeling: A Practical Course. Cambridge University Press."
  },
  {
    "objectID": "resources.html#journal-articles",
    "href": "resources.html#journal-articles",
    "title": "Resources",
    "section": "Journal articles",
    "text": "Journal articles\nAhn, W. Y., Haines, N., & Zhang, L. (2017). Revealing neurocomputational mechanisms of reinforcement learning and decision-making with the hBayesDM package. Computational Psychiatry, 1, 24-57. https://doi.org/10.1162/CPSY_a_00002\nDaw, N. D. (2011). Trial-by-trial data analysis using computational models. Decision Making, Affect, and Learning: Attention and Performance XXIII, 23, 3-38. https://doi.org/10.1093/acprof:oso/9780199600434.003.0001\nEtz, A., Gronau, Q. F., Dablander, F., Edelsbrunner, P. A., & Baribault, B. (2018). How to become a Bayesian in eight easy steps: An annotated reading list. Psychonomic Bulletin & Review, 25(1), 219-234. https://doi.org/10.3758/s13423-017-1317-5\nKruschke, J. K., & Liddell, T. M. (2018). Bayesian data analysis for newcomers. Psychonomic Bulletin & Review, 25(1), 155-177. https://doi.org/10.3758/s13423-017-1272-1\nLockwood, P. L., & Klein-Flügge, M. C. (2021). Computational modelling of social cognition and behaviour—a reinforcement learning primer. Social Cognitive and Affective Neuroscience, 16(1-2), 1-11. https://doi.org/10.1093/scan/nsaa040\nWagenmakers, E. J., Marsman, M., Jamil, T., Ly, A., Verhagen, J., Love, J., Selker, R., Gronau, Q. F., Šmíra, M., Epskamp, S., Matzke, D., Rouder, J. N., & Morey, R. D. (2018). Bayesian inference for psychology. Part I: Theoretical advantages and practical ramifications. Psychonomic Bulletin & Review, 25(1), 35-57. https://doi.org/10.3758/s13423-017-1343-3\nWilson, R. C., & Collins, A. G. E. (2019). Ten simple rules for the computational modeling of behavioral data. eLife, 8, Article e49547. https://doi.org/10.7554/eLife.49547\nZhang, L., Lengersdorff, L., Mikus, N., Gläscher, J., & Lamm, C. (2020). Using reinforcement learning models in social neuroscience: Frameworks, pitfalls and suggestions of best practices. Social Cognitive and Affective Neuroscience, 15(6), 695-707. https://doi.org/10.1093/scan/nsaa089"
  },
  {
    "objectID": "resources.html#online-cognitive-modeling-tutorials",
    "href": "resources.html#online-cognitive-modeling-tutorials",
    "title": "Resources",
    "section": "Online cognitive modeling tutorials",
    "text": "Online cognitive modeling tutorials\n\nNeuromatch Academy Computational Neuroscience Course A wide-spanning curriculum that covers most areas of computational neuroscience (Python based).\nBehavioural Modeling A two-part course covering how to model cognitive behaviour using MATLAB. By Hanneke den Ouden (Donders Institute, Nijmegen) and Jill O’Reilly (Oxford University).\nComputational Models of Human Social Behavior and Neuroscience An introductory course to the computational modeling of social behaviour for those with no prior programming experience needed (Python based). By Shawn Rhoads (Icahn School of Medicine).\nNivStan A guide to getting started in Stan with tutorials written by the Niv Lab at Princeton (uses PyStan, the Python interface to Stan)."
  },
  {
    "objectID": "resources.html#websites",
    "href": "resources.html#websites",
    "title": "Resources",
    "section": "Websites",
    "text": "Websites\n\nThe Stan Forums: the community hub where users can ask questions, share code, and discuss implementation details of Stan models\nDataCamp: a resource for interactive online courses that cover Bayesian statistics and Stan programming\nThe distribution zoo: an interactive tool to build intuitions about common probability distributions.\nProbability distribution explorer: another interactive tool on probability distributions, with code in Python and Stan.\nMichael Betancourt’s blog post: comprehensive case studies using Stan."
  },
  {
    "objectID": "course_overview.html",
    "href": "course_overview.html",
    "title": "Course overview",
    "section": "",
    "text": "This course was initially developed for master’s students at the University of Vienna, but is intended for anyone interested in learning about Bayesian statistics, and using Bayesian methods to both build and apply cognitive models. As a result, students at all levels, postdocs, as well as senior faculty have completed this course!\nThe term ‘Bayesian’ is not limited to modeling, and reflects a more general approach to probability observed in many research areas. Therefore, to avoid confusion beforehand, this course is not about ‘Bayes in the brain’ (Bayesian brain hypothesis), or why Bayesian statistics is a better alternative to frequentist statistics (even though it is).\nInstead, this course aims to develop understanding and experience in using Bayesian statistics to analyse cognitive processes by constructing models.\nThe course’s approach to understanding cognition through modeling is guided by David Marr’s influential framework of three levels of analysis1:\n\nThe Computational Level (Why): This addresses what problem the cognitive system is trying to solve and why. It focuses on the goals and logic of the computation.\nThe Algorithmic Level (What): This specifies the representation and algorithm used to solve the computational problem. It describes the rules and strategies that implement the solution.\nThe Implementation Level (How): This details how the algorithm is physically realized in neural circuits and brain structures.\n\nThroughout this course, we’ll use cognitive modeling as a bridge between these levels, particularly focusing on how algorithmic-level models can help us understand behaviour:\n\n\n\n\n\n\n\n\n\nMarr’s framework as applied to cognitive modeling\n\nThe goals of this course are to:\n\nBuild a foundational knowledge in cognitive behaviour and model-building\nLearn practical R programming\nBuild cognitive models using RStan\nEngage in open-source methods using git and GitHub (optional)\n\nSubsequently, after completing the course, you …\n\nfeel comfortable with reading mathematical equations\nconsider the implementation of the “computational modeling” section when reading scientific articles\ngain insightful understanding of Bayesian statistics and modeling\ncan apply computational modeling in your own experiments\n\nThis course particularly places a strong emphasis on hands-on experience modeling in Stan, gaining experience with the modeling workflow depicted below2:\n\n\n\n\n\nYou will learn to implement parameter and model recovery, perform posterior predictive checks, and assess competing models using model selection. Don’t worry if none of this is currently familiar; hopefully it will be after completing this course!"
  },
  {
    "objectID": "course_overview.html#pre-requisites",
    "href": "course_overview.html#pre-requisites",
    "title": "Course overview",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nTo fully benefit from the materials, you are expected to have:\n\nSome basic understanding of frequentist statistics\nSome basic knowledge of programming\n\n\n\n\n\n\n\nPre-requisites\n\n\n\nAny advanced knowledge of programming or statistics, whilst useful, is not required!\n\n\nThe course develops in complexity across the workshops, with the later material on hierarchical models and model optimization being more challenging. However, do not feel as if you need to complete all workshops within a specific time-frame! These materials should consist part of your education in cognitive modeling.\nAs a course in statistics and mathematical modeling, math equations feature throughout, albeit sparingly. Understanding the mathematical equations underlying computational models is necessary for understanding the relationship between data and parameters. However, some sections of the course feature additional math (e.g., derivations, statistical distributions) which are not strictly necessary to understand. As a general rule of thumb, most important is that you try to understand the equations describing your data and parameters, and how they are represented mathematically!\nIn any case, do not worry if you struggle during the course!"
  },
  {
    "objectID": "course_overview.html#list-of-folders-and-contents",
    "href": "course_overview.html#list-of-folders-and-contents",
    "title": "Course overview",
    "section": "List of folders and contents",
    "text": "List of folders and contents\nThe materials for the workshops are split across sub-folders within /workshops:\n\n\n\n\n\n\n\n\nFolder\nTask\nModel\n\n\n\n\n01.R_basics\nNA\nNA\n\n\n02.binomial_globe\nGlobe toss\nBinomial Model\n\n\n03.bernoulli_coin\nCoin flip\nBernoulli Model\n\n\n04.regression_height\nObserved weight and height\nLinear regression model\n\n\n05.regression_height_poly\nObserved weight and height\nPolynomial regression model\n\n\n06.reinforcement_learning\n2-armed bandit task\nSimple reinforcement learning (RL)\n\n\n07.optm_rl\n2-armed bandit task\nSimple reinforcement learning (RL)\n\n\n08.compare_models\nProbabilistic reversal learning task\nSimple and Fictitious RL models\n\n\n09.debugging\nMemory Retention\nExponential decay model\n\n\n10.model_based\nWIP\nWIP\n\n\n11.delay_discounting\nWIP\nWIP\n\n\n\nIn addition to following along with the taught material, there are a number of exercises that you can work through. The code and solutions to the exercises in some cases will be worked through, but will not in others. In all cases however, the materials to work through the exercises are provided.\nSpecifically, each folder above will typically have the two sub-folders: _data and _scripts. Each _scripts folder will also contain separate files with and without the master suffix.\n├── R_basics.Rproj\n├── _data\n├── _scripts\n│   ├── R_basics.R\n│   └── R_basics_master.R\nThe base script will contain the necessary code without the solutions, whilst the master script also contains the solutions.\nWhilst the structure of the workshops on this website does not exactly match those in the folder, details on which scripts and data to use is always given.\n\n\n\n\n\n\nExercise is optional\n\n\n\nYou do not have to work through the exercises to benefit from and complete this course, but is recommended if you are wanting to gain practical experience with programming in Stan!"
  },
  {
    "objectID": "course_overview.html#set-up",
    "href": "course_overview.html#set-up",
    "title": "Course overview",
    "section": "Set-up",
    "text": "Set-up\nThere is no additional set-up needed if you aim to work solely from the website. However, to run the analyses on your computer, several software and packages must be installed.\nFirstly, clone and change to the project directory:\ngit clone https://github.com/sohaamir/BayesCog.git\ncd BayesCog\n\nSoftware\n\nInstall the latest version of R here and RStudio here\nInstall and set-up the latest version of RStan here\n\n\n\nR packages\nIn addition to software, various analyses across the workshops require specific packages to be installed.\nThis project uses renv to manage package dependencies. To set up the environment:\n\nOpen R in the project directory (BayesCog.Rproj) and run:\n\nsource(\"setup.R\")\nAfter running this command once, the project environment will load automatically whenever you open the project.\n\n\n\n\n\n\nQuerying packages\n\n\n\nYou can always check if you are missing a certain package by clicking on the ‘Packages’ tab (next to Files/Plots tab) or by running library()."
  },
  {
    "objectID": "course_overview.html#footnotes",
    "href": "course_overview.html#footnotes",
    "title": "Course overview",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMarr, D. (1982). Vision: A computational investigation into the human representation and processing of visual information. San Francisco: W. H. Freeman.↩︎\nAdapted from Jan Gläscher’s workshop on cognitive modeling.↩︎"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "BayesCog",
    "section": "Welcome",
    "text": "Welcome\n     \nThis website12 is an adapted version of teaching materials originally made for the award winning* BayesCog seminar at the Faculty of Psychology, University of Vienna, as part of the Advanced Seminar for master’s students (Mind and Brain track; recorded during Summer Term 2020/2021). Further content from the BayesCog workshop at UKE Hamburg (2023) have also been added.\nRecording: Recordings from the original version of the course are available on YouTube (also see below). The most recent recording from the 2021 summer semester is also available on YouTube.\nOutreach: Twitter thread (being liked 700+ times on Twitter) summarizing the contents of the course.\nAward/Recognition: The original course received a commendation award from the Society for the Improvement of Psychological Science (SIPS) (also see a tweet), as well as an ECR Teaching Award from the Faculty of Psychology, University of Vienna."
  },
  {
    "objectID": "index.html#course-summary",
    "href": "index.html#course-summary",
    "title": "BayesCog",
    "section": "Course summary",
    "text": "Course summary\nComputational modeling and mathematical modeling provide an insightful quantitative framework that allows researchers to inspect latent processes and to understand hidden mechanisms. Hence, computational modeling has gained increasing attention in many areas of cognitive science through cognitive modeling. One illustration of this trend is the growing popularity of Bayesian approaches to cognitive modeling. To this end, this course teaches the theoretical and practical knowledge necessary to perform, evaluate and interpret Bayesian modeling analyses, with a specific emphasis towards modeling latent cognitive processes.\nIn the course, students will be formally grounded in key principles of cognitive modeling including Bayesian statistics, statistical distibrutions and reinforcement learning. We will use R/RStudio and a newly developed statistical computing language - Stan - to perform Bayesian analyses, ranging from simple binomial models and linear regression models to more complex hierarchical reinforcement learning (RL) models."
  },
  {
    "objectID": "index.html#contributors",
    "href": "index.html#contributors",
    "title": "BayesCog",
    "section": "Contributors",
    "text": "Contributors\n   Lei Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDr. Lei Zhang is Principal Investigator of the Adaptive Learning Psychology and Neuroscience Lab, ALP(e)N Lab, and Associate Professor at the School of Psychology, University of Birmingham. Lei obtained his BSc in Psychology from Beijing Normal University, China, his MSc in Cognitive Neuroscience at the Basque Center on Cognition, Brain and Language, Spain and his PhD (summa cum laude), along with a one-year transition postdoc, with Jan Gläscher at the Institute of Systems Neuroscience, University Medical Center Hamburg-Eppendorf, Germany. He was then a Roche intern for Scientific Exchange (RiSE) at F. Hoffmann-La Roche AG, and worked as a postdoctoral fellow with Claus Lamm at the Social Cognitive and Affective Neuroscience Unit (SCAN-Unit), University of Vienna, Austria. Lei joined the Centre for Human Brain Health, Institute of Mental Health, and School of Psychology at the University of Birmingham as an Associate Professor in 2022. His research applies knowledge from cognitive neuroscience, psychology, and computational modeling to gain a comprehensive understanding of how the brain computes values and social information when making decisions, and how they are affected in mental health disorders. He is also motivated towards fostering open and collaborative science, being a founding member of the grassroots China Open Science Network (COSN) and is the recipient of multiple awards including the BNA Individual Researcher Credibility Prize in 2024.\n\n\nAamir Sohail\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAamir Sohail is an MRC Advanced Interdisciplinary Methods (AIM) DTP PhD student based at the Centre for Human Brain Health (CHBH), University of Birmingham, where he is supervised by Lei Zhang and Patricia Lockwood. He completed a BSc in Biomedical Science at Imperial College London, followed by an MSc in Brain Imaging at the University of Nottingham. He then worked as a Junior Research Fellow at the Centre for Integrative Neuroscience and Neurodynamics (CINN), University of Reading with Anastasia Christakou. His research interests involve using a combination of behavioural tasks, computational modeling and neuroimaging to understand social decision-making, and using this knowledge to inform the precision-based treatment of mental health disorders. Outside of research, he is also passionate about facilitating inclusivity and diversity in academia, as well as promoting open and reproducible science."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "BayesCog",
    "section": "License",
    "text": "License\nThis course is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\nYou are free to share, copy, and redistribute the material in any medium or format. Additionally, you can adapt, remix, transform, and build upon the material for any purpose, including commercial use. However, you must provide appropriate attribution, including credit to the original source, a link to the license, and an indication if changes were made. Furthermore, if you remix, transform, or build upon the material, you are required to distribute your contributions under the same license as the original."
  },
  {
    "objectID": "index.html#citing",
    "href": "index.html#citing",
    "title": "BayesCog",
    "section": "Citing",
    "text": "Citing\nIf you use materials from this course in your work or research, please cite it as:\nZhang, L., & Sohail, A. (2025). BayesCog: Bayesian Statistics and Hierarchical Bayesian Modeling for Psychological Science [Online course]. Zenodo. https://doi.org/[DOI_HERE]\nFor BibTeX users:\n@online{zhang_sohail_2025,\n    title = {BayesCog: Bayesian Statistics and Hierarchical Bayesian Modeling for Psychological Science},\n    author = {Zhang, Lei and Sohail, Aamir},\n    year = {2025},\n    publisher = {Zenodo},\n    doi = {DOI_HERE},\n    url = {ZENODO_URL},\n    note = {Online course},\n    repository = {https://github.com/REPOSITORY_URL}\n}\nNote: Once this course is published on Zenodo, this citation information will be updated with the corresponding DOI and URL."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "BayesCog",
    "section": "Contact",
    "text": "Contact\nFor bug reports, issues or comments, please contact Lei Zhang, or Aamir Sohail, or open a thread on the GitHub repository."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "BayesCog",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nL.Z. created the original resources, including scripts, data, slides, recordings and GitHub repositories. A.S. created the website, and added the content by converting, editing and expanding the source material. Both L.Z. and A.S. further revised and edited the website.↩︎\nBayes’ photo credit: Burrsettles (2016). The three faces of Bayes. Slackprop. https://slackprop.wordpress.com/2016/08/28/the-three-faces-of-bayes↩︎"
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/stan_intro.html",
    "href": "workshops/02.binomial_globe/qmd/stan_intro.html",
    "title": "An introduction to modeling in Stan",
    "section": "",
    "text": "Welcome to the fourth workshop of the BayesCog course!"
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/stan_intro.html#purpose-of-this-workshop",
    "href": "workshops/02.binomial_globe/qmd/stan_intro.html#purpose-of-this-workshop",
    "title": "An introduction to modeling in Stan",
    "section": "Purpose of this workshop",
    "text": "Purpose of this workshop\nIn this tutorial, we’ll take the next step in our Bayesian journey by implementing our globe-tossing model using Stan, a powerful probabilistic programming language. We’ll revisit the same binomial problem, but instead of using grid approximation, we’ll program our model in Stan to sample from the posterior more efficiently.\nThe goals of this workshop are to: - Understand how to translate our mathematical model into Stan code - Learn the basic syntax and structure of Stan programs - Compare the efficiency of Stan’s HMC implementation with our previous approaches - Gain practical experience in fitting and diagnosing Bayesian models using modern computational tools\nBy the end of this tutorial, you’ll have the foundational skills needed to implement more complex Bayesian models using Stan’s probabilistic programming framework in future workshops.\n\n\n\n\n\n\nWorking directory for this workshop\n\n\n\nModel code and R scripts for this workshop are once again located in the (/workshops/02.binomial_globe) directory. Remember to use the R.proj file within each folder to avoid manually setting directories!\n\n\nThe copy of this workshop notes can be found on the course GitHub page."
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/intro.html",
    "href": "workshops/02.binomial_globe/qmd/intro.html",
    "title": "Linking data to parameters",
    "section": "",
    "text": "Welcome to the third workshop of the BayesCog course!\nIn this workshop, we’ll build on our understanding of probability and Bayes’ theorem to explore how we can construct and implement Bayesian models. We’ll start with a simple but powerful example - the binomial model - and use it to demonstrate key concepts in Bayesian modeling. We will explore the computational burden of Bayesian statistics - calculating posterior distributions - first through grid approximation, before exploring Markov chain Monte Carlo (MCMC) methods - powerful computational techniques which allow us to sample from complex posterior distributions that would be impossible to compute directly.\nTopics for this workshop include:\n\nUnderstanding how data and parameters are linked in Bayesian inference\nLearning about likelihood functions and how to choose them\nExploring the binomial model through a practical example\nUsing grid approximation to estimate posterior distributions\nAn introduction to Markov chain Monte Carlo\n\n\n\n\n\n\n\nWorking directory for this workshop\n\n\n\nModel code and R scripts for this workshop are located in the (/workshops/02.binomial_globe) directory. Remember to use the R.proj file within each folder to avoid manually setting directories!\n\n\nThe copy of this workshop notes can be found on the course GitHub page."
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/mcmc.html",
    "href": "workshops/02.binomial_globe/qmd/mcmc.html",
    "title": "Markov chain Monte Carlo",
    "section": "",
    "text": "In our previous chapter, we explored how to estimate the posterior distribution for a simple binomial model using grid approximation. This worked well mainly because we only had a single parameter, \\(\\theta\\) to estimate at 20 points in the grid.\nBut what happens when our models become more complex and involve multiple parameters?\nLet’s consider how the computational challenge scales with additional parameters. If we used 100 grid points for \\(\\theta\\), we would need to compute 100 values. However, with two parameters \\(\\theta_1\\) and \\(\\theta_2\\), calculating the marginal likelihood becomes a double integral:\n\\[p(data) = \\int\\int p(data,\\theta_1,\\theta_2)d\\theta_1d\\theta_2\\] This complexity grows dramatically when we move to models with many parameters. Consider a model with 100 means \\((μ)\\) and 100 standard deviations \\((σ)\\).\nThe marginal likelihood becomes:\n\\[\n\\scriptsize\np(data) = \\int_{\\mu_1}\\int_{\\sigma_1}...\\int_{\\mu_{100}}\\int_{\\sigma_{100}}\np(data|\\mu_1,\\sigma_1,...,\\mu_{100},\\sigma_{100}) \\times\np(\\mu_1,\\sigma_1,...,\\mu_{100},\\sigma_{100})\nd\\mu_1d\\sigma_1...d\\mu_{100}d\\sigma_{100}\n\\] If we use 100 grid points for each parameter in this model, the computational burden grows exponentially. Even with just two parameters, we need 100 × 100 = 10,000 computations, and with five parameters, this becomes 100⁵ = 10 billion computations!\nFortunately, there exists a solution: Markov chain Monte Carlo (MCMC).\nInstead of trying to compute the entire posterior distribution, MCMC provides a way to draw samples; approximating it. MCMC takes advantage of the proportionality:\n\\[p(\\theta|D) \\propto p(D|\\theta)p(\\theta)\\] This means we can generate samples from the posterior without having to compute the normalizing constant (the marginal likelihood) that makes the problem so computationally challenging."
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/mcmc.html#markov-chains-and-monte-carlo-methods",
    "href": "workshops/02.binomial_globe/qmd/mcmc.html#markov-chains-and-monte-carlo-methods",
    "title": "Markov chain Monte Carlo",
    "section": "Markov chains and Monte Carlo methods",
    "text": "Markov chains and Monte Carlo methods\nBefore we discuss an example of how MCMC works to sample from the posterior, let’s understand what it actually means.\n\nMarkov chains\nA Markov chain is a sequence of random states where the probability of transitioning to the next state depends only on the current state, and not on the sequence of states that preceded it.\nMathematically, for a sequence of random variables \\(X_1, X_2, ..., X_n\\), the Markov property states:\n\\[P(X_{n+1} = x | X_n = x_n, X_{n-1} = x_{n-1}, ..., X_1 = x_1) = P(X_{n+1} = x | X_n = x_n)\\] A classic example is weather prediction. If we simplify weather to just “Sunny” or “Rainy”, a Markov chain could represent the probability of tomorrow’s weather based only on today’s weather. For instance:\n\nIf it’s sunny today, there’s a 70% chance it will be sunny tomorrow\nIf it’s rainy today, there’s a 60% chance it will be rainy tomorrow\n\n\n\nMonte Carlo methods\nMonte Carlo methods use random sampling to obtain numerical results. Whilst sampling randomly may seem counter-productive, ultimately, the Law of Large Numbers1 ensures that these random samples provide good approximate solutions to numerical problems.\nFor an expected value \\(E[f(X)]\\), the Monte Carlo approximation is:\n\\[E[f(X)] \\approx \\frac{1}{n}\\sum_{i=1}^n f(x_i)\\]\nwhere \\(x_i\\) are random samples from the distribution of \\(X\\).\nThere is an excellent YouTube video visually explaining how Monte Carlo methods work, by calculating an approximation of \\(π\\). It does this by randomly dropping marbles in a grid containing a circle and a square, where the circle’s area is determined by \\(πa^2\\) and the square’s by \\(a^2\\).\n\n\n\n\n\n\nHistorical tidbits\n\n\n\nMarkov chains were first introduced by Russian mathematician Andrey Markov in 1906, initially, to analyze the alternation of consonants and vowels in Russian literature, specifically Pushkin’s “Eugene Onegin”. Monte Carlo methods were developed during the Manhattan Project by scientists including Stanislaw Ulam (the namesake of the Stan programming language) and John von Neumann. The name “Monte Carlo” was coined by Nicholas Metropolis, inspired by Ulam’s uncle who would borrow money from relatives to gamble at the Monte Carlo Casino in Monaco.\n\n\n\n\n\n\n\n\nA picture of the Monte Carlo Casino taken by A.S.\n\nWhen we combine these concepts, we get Markov chain Monte Carlo (MCMC)! With respect to sampling from the posterior distribution:\n\nThe Monte Carlo part means we’re going to use random sampling to explore the parameter space\nThe Markov chain part provides a systematic way to guide these random samples toward regions of high posterior probability\n\nIn general, here are the steps behind MCMC:\n\nStart at some initial state\nPropose a new state according to some transition rule\nAccept or reject the proposed state based on a criterion that ensures convergence to the desired distribution\nRepeat steps 2-3 many, many times\n\n\n\nA practical example of MCMC using the Metropolis algorithm\nHere’s an practical example demonstrating the idea behind MCMC, using a specific instance, the Metropolis algorithm.\n\n\n\n\n\n\nMCMC and the Metropolis algorithm\n\n\n\nMCMC refers to a specific method of sampling, of which the Metropolis algorithm is just one example. We will discuss more examples of MCMC including Hamiltonian Monte Carlo (HMC), the specific type used by Stan.\n\n\nCharles is a billionaire who made his fortune in the energy industry. Unfortunately, Charles has 10 ex-wives and wants to divide his time visiting them proportionally to how much he likes them (his posterior distribution). As it happens, his 10 ex-wives live in a line along a street, numbered 1-10 in order. His preferences are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHouse\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nPreference (%)\n20\n15\n15\n12\n10\n10\n8\n5\n3\n2\n\n\n\nBut there are complications:\n\nCharles only knows relative preferences - he can say if he likes one ex more than another, but can’t assign exact numbers\nHe can only visit nearby ex-wives (can’t teleport between distant houses)\nHe wants his total visiting pattern to reflect his true preferences\n\nHere’s how MCMC would work for Charles:\n\nStart at any ex-wife’s house (let’s say #5)\nPropose a move (Monte Carlo part). Charles can only propose visiting a nearby ex, so he randomly picks one of the neighboring houses.\nDecide whether to move (Markov chain part). If he likes the proposed ex more than current ex: Definitely move. If he likes them less:\n\n\\[\\small{\\text{Move with probability}} = \\frac{\\text{new preference}}{\\text{current preference}}\\]\nThis “random walk” along the street, with tendency to move toward and stay longer at more preferred houses, will eventually result in Charles spending time proportional to his preferences.\n\n\n\n\n\n\nThe Metropolis algorithm\n\n\n\nThere are many different types of MCMC; we used the Metropolis algorithm as it demonstrates the principles of MCMC quite simply. The Metropolis algorithm is a special case of the Metropolis-Hastings algorithm which allows for non-symmetric proposal distributions. The acceptance ratio includes an additional term to account for this asymmetry.\n\n\nHere is a visualization of the different steps of the Metropolis algorithm using an “MCMC robot”2.\n\n\n\n\n\n\n\n\n\nThe MCMC robot hard at work\n\nAnd here is a diagram showing how the Metropolis algorithm can approximate the posterior distribution of discrete data3.\n\n\n\n\n\n\n\n\n\nThe Metropolis algorithm accurately estimates the posterior distribution of discrete data\n\nIn this plot, imagine you’re trying to estimate the probability of different values of \\(\\theta\\), but you can only use whole numbers from 1 to 7. The bottom plot shows what we’re aiming for - it’s the true probability distribution we want to sample from, where higher values of \\(\\theta\\) are more likely.\nThe middle plot shows how the algorithm actually explores these values over time. It’s akin to taking a walk, where at each step you can move to a different value of \\(\\theta\\). Sometimes the algorithm stays in place (shown by horizontal lines), which happens when a proposed move is rejected. Other times it jumps to a new value. Notice how it tends to spend more time around the larger values of \\(\\theta\\) (5, 6, and 7) because these have higher probabilities in our target distribution.\nThe top plot shows the results of this exploration - it’s a histogram counting how often the algorithm visited each value of \\(\\theta\\). These frequencies end up matching our target distribution from the bottom plot. Even though we’re just taking a random walk through the possible values, guided by some simple rules about when to accept or reject moves, we end up with samples that accurately represent the probability distribution we’re interested in.\nThe plots below demonstrates how the Metropolis algorithm works for continuous data, in this case to provide an approximation for human height in a sample of participants4:\n\n\n\n\n\n\n\n\n\nThe Metropolis algorithm approximates the shape of the posterior distribution for continuous data\n\nThe first plot shows just the target posterior distribution (the dashed black line).\nIn the second plot, we see the first few MCMC samples appearing as red dots. These are our initial explorations of the parameter space, but they’re too sparse to give us a good picture of the distribution yet.\nThe third plot shows more samples accumulating (blue dots added to the red ones). We’re starting to see a pattern emerge - the samples are clustering more densely in areas where the true posterior has higher probability.\nFinally, in the fourth plot, after many more samples, we create a histogram (blue bars) from all our samples. Notice how the shape of this histogram closely matches the shape of the original dashed curve. In other words, our MCMC sampling has successfully approximated the true posterior distribution."
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/mcmc.html#multiple-chains-convergence-and-hamiltonian-monte-carlo",
    "href": "workshops/02.binomial_globe/qmd/mcmc.html#multiple-chains-convergence-and-hamiltonian-monte-carlo",
    "title": "Markov chain Monte Carlo",
    "section": "Multiple chains, convergence and Hamiltonian Monte Carlo",
    "text": "Multiple chains, convergence and Hamiltonian Monte Carlo\nAn analogy of MCMC would be trying to find the highest mountain in a huge mountain range in complete darkness. However. instead of sending just one explorer who might get stuck in the wrong place or miss the highest peak entirely, we send multiple explorers starting from different random locations. Each explorer’s path is what we call a “chain” in MCMC.\nIf all our explorers eventually find their way to the same region and report similar findings about the landscape, we can be more confident they’ve found what we’re looking for. This is why we run multiple MCMC chains - if they all converge to sampling from the same distribution, it suggests we’re getting reliable results.\nHowever, traditional MCMC methods like Metropolis, Metropolis-Hastings and Gibb’s sampling aren’t very efficient in this manner. This is where a more sophisticated method called Hamiltonian Monte Carlo (HMC) comes in. Instead of random wandering, HMC’s exploration is guided by the properties of the terrain. By using information - specifically about the gradient (the slope) of our probability distribution - HMC can make much more efficient proposals about where to move next.\nThis is the MCMC method used by the Stan programming language, one of the most popular modern approaches for statistical modeling. Stan actually uses an even more advanced version called the No-U-Turn Sampler (NUTS)."
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/mcmc.html#footnotes",
    "href": "workshops/02.binomial_globe/qmd/mcmc.html#footnotes",
    "title": "Markov chain Monte Carlo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSedor, K. (2015). The law of large numbers and its applications. Lakehead University: Thunder Bay, ON, Canada.↩︎\nLewis, P. O. (2011). Bayesian Phylogenetics [Workshop presentation]. Workshop on Molecular Evolution, Český Krumlov, Czech Republic.↩︎\nKruschke, J. K. (2015). Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan, 2nd Ed. Academic Press.↩︎\n\n(n.d.). A zero-math introduction to Markov Chain Monte Carlo methods. Towards Data Science.\n\n↩︎"
  },
  {
    "objectID": "workshops/01.R_basics/qmd/working_with_data.html",
    "href": "workshops/01.R_basics/qmd/working_with_data.html",
    "title": "Basic data management and analysis using R",
    "section": "",
    "text": "Now that you have a basic understanding how how R works, and what it is used for, let’s put our newly learned skills to practical use. In this section we will directly work with some simulated choice data from a behavioural experiment.\nWhen working with data in our studies, we can consider R to be the central software for data management, analysis and plotting. It is quite robust for this purpose, in that it also reads in data of many different types and from many different sources 1:"
  },
  {
    "objectID": "workshops/01.R_basics/qmd/working_with_data.html#working-with-behavioural-data-in-r",
    "href": "workshops/01.R_basics/qmd/working_with_data.html#working-with-behavioural-data-in-r",
    "title": "Basic data management and analysis using R",
    "section": "Working with behavioural data in R",
    "text": "Working with behavioural data in R\nWe will now work with an example dataset where participants were tasked with completing a behavioural task. Specifically, they were asked to complete a probabilistic reversal learning (PRL) task, a common paradigm used to assess cognitive flexibility and reward-based learning.\nThe task consists of three main phases:\n\nChoice Presentation: Two stimuli (a blue snowflake-like pattern and a yellow spiral pattern) are presented to the participant.\nAction Selection: The participant must select one of the stimuli (shown by the red frame indicating a selection of the blue pattern).\nOutcome: A monetary reward (20 cent coin) is shown as feedback.\n\n\n\n\n\n\n\n\n\nIn the PRL task, one stimulus is typically associated with a higher probability of reward (e.g., 80% chance of winning money), and a lower probability for a loss (e.g., 20% chance of losing money). After participants learn these associations, the contingencies are reversed, requiring the participants to adapt their choices.\nThe participants are given instructions that:\n\nOne option is better than the other.\nThis will switch at various points.\nYou should make choices to maximise your points.\n\nImportantly, the participants are not made aware of when and how many reversals take place.\nWe have example data for 10 subjects within the data subdirectory. Each one has an associated raw_data_subXX.txt file:\n_data/\n└── RL_raw_data/\n   ├── sub01/\n   │   └── raw_data_sub01.txt\n   ├── sub02/\n       └── raw_data_sub02.txt\n   ├── ...\n   └── sub10/\n       └── raw_data_sub10.txt\nWe need to firstly install and load the required packages:\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\nAnd now let’s load the data into R. We can read the data using the read.table function, specifying in the arguments that there is a header, and that the data columns are separated by a comma. We can then check the data by loading the first few lines using the head() function.\n\nrm(list=ls())\ndata_dir = ('../_data/RL_raw_data/sub01/raw_data_sub01.txt')\ndata = read.table(data_dir, header = T, sep = \",\")\nhead(data)\n\n  subjID trialID choice outcome correct\n1      1       1      1       1       1\n2      1       2      1       1       1\n3      1       3      1       1       1\n4      1       4     NA       1       1\n5      1       5      1      -1       1\n6      1       6      2      -1       1\n\n\n\n\n\n\n\n\nFile separators\n\n\n\nTwo common file types are CSV (Comma-Separated Values) files which use commas to separate values, and TSV (Tab-Separated Values) files which use tabs.\n\n\nThe data columns represent the following:\n\nsubjID - the subject ID\ntrialID - the trial ID (80 trials per subject)\nchoice - which image the subject selected (blue or yellow)\noutcome - whether the subject was rewarded (1) or was given a loss (-1)\ncorrect - whether the choice made was the ‘correct’ one (i.e., the image with the 80% reward contingency on that trial)\n\nYou may have noticed that there are some NA values contained in the data, namely within the choice column.\n\nsum(complete.cases(data))\n\n[1] 78\n\n\nThis will be in trials where the participant did not respond in time (as there is a fixed time limit to respond). We would like to remove these trials from the data.\nWe can do this by subsetting the data to only include cases where there are no NA values:\n\ndata = data[complete.cases(data),]\ndim(data[complete.cases(data),])\n\n[1] 78  5\n\n\nSo if we examine the data now, there are no NA values.\n\nany(is.na(data))\n\n[1] FALSE\n\n\n\nThere are often many ways of doing the same act in R; this is just one example!\n\n\n\n\n\n\n\nExercise 3\n\n\n\n1. Write a for loop which reads in each participant’s raw data (from _data/RL_raw_data/) and reshapes it in the ‘long format’ by each subject.\nTIP: Use the sprintf function to get the file paths of all subject data. Type ?sprintf for more information.\n\n\nClick to see the solution\n\n\n# read in all the data\nns = 10\ndata_dir = '../_data/RL_raw_data'\n\nrawdata = c()\nfor (s in 1:ns) {\n    sub_file = file.path(data_dir, sprintf('sub%02i/raw_data_sub%02i.txt',s,s))\n    sub_data = read.table(sub_file, header = T, sep = \",\")\n    rawdata = rbind(rawdata, sub_data)\n}\nrawdata = rawdata[complete.cases(rawdata),]\n\nhead(rawdata)\n\n  subjID trialID choice outcome correct\n1      1       1      1       1       1\n2      1       2      1       1       1\n3      1       3      1       1       1\n5      1       5      1      -1       1\n6      1       6      2      -1       1\n7      1       7      1       1       1\n\n\n\n2. Create a new column called accuracy in the rawdata data frame, where each row indicates whether the choice was correct (1.0) or incorrect (0.0).\nTIP: Recall that for each trial, each choice can either be 1 or 2, and whether this is accurate or not is based upon the value of correct for that trial.\n\n\nClick to see the solution\nrawdata$accuracy = (rawdata$choice == rawdata$correct) * 1.0\n\n\n3. Calculate the mean accuracy for each subject and print out the overall accuracy across all subjects.\n\n\nClick to see the solution\n\n\nacc_mean = aggregate(rawdata$accuracy, by = list(rawdata$subjID), mean)[,2]\ngroup_mean = mean(acc_mean)\n\ngroup_mean\n\n[1] 0.7348277\n\n\n\nComplete solutions are also located within the R_basics.R script.\n\n\n\n\n\n\n\n\nUsing sprintf\n\n\n\nsprintf is an important function used for formatting strings. It allows you to embed variables into a string with specific formatting options. For example in our use above:\n\nsprintf('sub%02i/raw_data_sub%02i.txt', s, s)\n\n\n%02i specifies that the integer should be displayed with at least 2 digits, padding with leading zeros if necessary.\nThe two s values are passed to sprintf and replace the %02i placeholders in the string. The first s replaces the first %02i, and the second s replaces the second %02i.\n\nIf we use %02i:\n\nFor s = 1: outputs sub01/raw_data_sub01.txt\nFor s = 2: outputs sub02/raw_data_sub02.txt\nFor s = 10: outputs sub10/raw_data_sub10.txt\n\nIf we use %03i:\n\nFor s = 1: outputs sub001/raw_data_sub001.txt\nFor s = 2: outputs sub002/raw_data_sub002.txt\nFor s = 10: outputs sub010/raw_data_sub010.txt"
  },
  {
    "objectID": "workshops/01.R_basics/qmd/working_with_data.html#basic-statistics-t-test-correlation-and-linear-regression",
    "href": "workshops/01.R_basics/qmd/working_with_data.html#basic-statistics-t-test-correlation-and-linear-regression",
    "title": "Basic data management and analysis using R",
    "section": "Basic statistics: t-test, correlation and linear regression",
    "text": "Basic statistics: t-test, correlation and linear regression\nNow that we have the accuracy for each of the 10 participants, we would like to see if their accuracies are above chance level (0.5). Chance is 0.5 because if a subject chose randomly for each trial, they would get 50% of them correct.\nRunning a one-sample t-test:\n\nt.test(acc_mean, mu = 0.5)\n\n\n    One Sample t-test\n\ndata:  acc_mean\nt = 13.788, df = 9, p-value = 2.34e-07\nalternative hypothesis: true mean is not equal to 0.5\n95 percent confidence interval:\n 0.6962988 0.7733565\nsample estimates:\nmean of x \n0.7348277 \n\n\nWe can see that the accuracy is significantly different to chance level across the participants.\nPsychologists are often interested with understanding the biological and sociological influences on task performance. A simple way of testing this is by doing a simple correlation between performance and these factors.\nLet’s load in some descriptive data which has the ‘IQ’ and ‘Age’ for the subjects, and add a column containing their accuracy:\n\nload('../_data/RL_descriptive.RData')\ndescriptive$acc = acc_mean\ndf = descriptive\nhead(descriptive)\n\n  subjID        IQ      Age       acc\n1      1 123.98691 31.07218 0.8076923\n2      2  87.63187 30.13800 0.7125000\n3      3  89.39930 23.44219 0.6875000\n4      4  84.34607 27.44848 0.6493506\n5      5 134.72208 23.30624 0.7750000\n6      6  84.60797 25.67858 0.7250000\n\n\nNow let’e see if there is a correlation between ‘IQ’ or ‘Age’ with accuracy:\n\ncor.test(df$IQ, df$acc)\n\n\n    Pearson's product-moment correlation\n\ndata:  df$IQ and df$acc\nt = 4.8347, df = 8, p-value = 0.001297\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5114810 0.9671586\nsample estimates:\n      cor \n0.8631401 \n\ncor.test(df$Age, df$acc)\n\n\n    Pearson's product-moment correlation\n\ndata:  df$Age and df$acc\nt = 0.3857, df = 8, p-value = 0.7098\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.5404909  0.7047848\nsample estimates:\n      cor \n0.1351166 \n\n\nFrom the results, we can see that there is a significantly positive correlation between IQ and accuracy (R = 0.86), whereas there is no such relationship with age.\nThis is more easily visualized by creating a graph using ggplot2():\n\n# Create IQ correlation plot\np1 &lt;- ggplot(df, aes(x = IQ, y = acc)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"blue\") +\n  theme_bw() +\n  labs(\n    x = \"IQ Score\",\n    y = \"Accuracy\",\n    title = \"IQ vs. Accuracy\"\n  ) +\n  theme(\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    plot.title = element_text(size = 16, hjust = 0.5)\n  )\n\n# Create Age correlation plot\np2 &lt;- ggplot(df, aes(x = Age, y = acc)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"blue\") +\n  theme_bw() +\n  labs(\n    x = \"Age\",\n    y = \"\",\n    title = \"Age vs. Accuracy\"\n  ) +\n  theme(\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    plot.title = element_text(size = 16, hjust = 0.5)\n  )\n\n# Combine plots side by side\np1 + p2\n\n\n\n\n\n\n\n\nBeyond running a simple correlation between two variables, we can develop a model describing their relationship. A commonly used model is a linear model which assumes that one variable (the outcome or dependent variable) can be predicted by another variable (the predictor or independent variable) using a straight line.\nThe line is described by two parameters: the intercept (where the line crosses the y-axis) and the slope (how much y changes for each unit change in x). This model allows us to make predictions about one variable based on values of the other, and quantifies the strength and direction of their relationship.\nA linear regression model is described mathematically by the following function:\n\\[y = \\beta_0 + \\beta_1x + \\epsilon\\] Where:\n\n\\(y\\) is the dependent variable\n\\(β_0\\) (beta_0) is the intercept\n\\(β_1\\) (beta_1) is the slope\n\\(x\\) is the independent variable\n\\(ε\\) (epsilon) represents the error term\n\nIn R we can run a simple linear regression using the lm function:\n\nfit1 = lm(acc ~ IQ, data = df)\nsummary(fit1)\n\n\nCall:\nlm(formula = acc ~ IQ, data = df)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.047305 -0.016277  0.007562  0.022577  0.027731 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.499292   0.049565  10.073 8.04e-06 ***\nIQ          0.002340   0.000484   4.835   0.0013 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02885 on 8 degrees of freedom\nMultiple R-squared:  0.745, Adjusted R-squared:  0.7131 \nF-statistic: 23.37 on 1 and 8 DF,  p-value: 0.001297\n\n\nLet’s break down the key findings:\n\nThe relationship is significantly positive, meaning higher IQ scores are associated with better accuracy. For each point increase in IQ, accuracy increases by about 0.0023 (this is our slope, β₁ = 0.002340).\nThe model explains about 74.5% of the variance in accuracy scores (R² = 0.745), which indicates this is quite a strong relationship.\nThe relationship is statistically significant (p = 0.0013), well below the conventional 0.05 threshold, suggesting this isn’t just due to chance.\n\nAgain, we can plot this using ggplot2():\n\nggplot(df, aes(x = IQ, y = acc)) +\n  geom_point(size = 3) +  # Data points\n  geom_smooth(method = \"lm\", color = \"blue\") +  # Regression line with confidence interval\n  theme_bw() +\n  labs(\n    x = \"IQ Score\",\n    y = \"Accuracy\",\n    title = \"Linear Regression: IQ vs Accuracy\"\n  ) +\n  theme(\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    plot.title = element_text(size = 16, hjust = 0.5)\n  )"
  },
  {
    "objectID": "workshops/01.R_basics/qmd/working_with_data.html#footnotes",
    "href": "workshops/01.R_basics/qmd/working_with_data.html#footnotes",
    "title": "Basic data management and analysis using R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKabacoff, R. I. (2022). R in action: data analysis and graphics with R and Tidyverse. Simon and Schuster.↩︎"
  },
  {
    "objectID": "workshops/01.R_basics/qmd/introduction_to_r.html",
    "href": "workshops/01.R_basics/qmd/introduction_to_r.html",
    "title": "An introduction to R/RStudio",
    "section": "",
    "text": "R is a powerful open-source programming language and software environment primarily designed for statistical computing, data analysis, and graphical visualization. R has its own interface; when you install R onto your own computer(Windows/Mac) you can open R on it’s own. However, working in this way with R (‘base R’) isn’t very user friendly!\nInstead, we use an IDE (Integrated Development Environment) to work with the R programming language. A popular example - and the one that we will be using - is RStudio.\nThe RStudio interface consists of 4 windows:\n1. Script editor: Where you write and edit your R code 2. Console: Where you run R commands and see their output 3. Environment/History: Shows your workspace variables and command history 4. Files/Plots/Packages/Help: A multi-purpose pane for viewing files, plots, managing packages, and accessing help documentation\nYou can check your versions of both R and RStudio using the following commands:\nR.version\n\n               _                           \nplatform       x86_64-apple-darwin20       \narch           x86_64                      \nos             darwin20                    \nsystem         x86_64, darwin20            \nstatus                                     \nmajor          4                           \nminor          4.1                         \nyear           2024                        \nmonth          06                          \nday            14                          \nsvn rev        86737                       \nlanguage       R                           \nversion.string R version 4.4.1 (2024-06-14)\nnickname       Race for Your Life\nRStudio.version()"
  },
  {
    "objectID": "workshops/01.R_basics/qmd/introduction_to_r.html#basic-r-operations",
    "href": "workshops/01.R_basics/qmd/introduction_to_r.html#basic-r-operations",
    "title": "An introduction to R/RStudio",
    "section": "Basic R operations",
    "text": "Basic R operations\nWe will now cover some basic operations and values within R.\n\nCalculator Functions\n\n\n\n\nWe can perform basic mathematical operations:\n\n# Addition and Subtraction\n3 + 2\n\n[1] 5\n\n3 - 2\n\n[1] 1\n\n# Multiplication and Division\n3 * 2\n\n[1] 6\n\n3 / 2\n\n[1] 1.5\n\n# Exponents\n3^2\n\n[1] 9\n\n2^3\n\n[1] 8\n\n# Constants\npi\n\n[1] 3.141593\n\nexp(1)  # base of natural logarithm\n\n[1] 2.718282\n\n\n\nSpecial values\n\n\n\n\nIn addition to numbers, variables can also take on other values:\n\n# Infinite Values\nInf\n1 + Inf\n\n# Missing Values\nNA\n1 + NA\n\n# NULL Values\nNULL\n1 + NULL\n\n\n\n\n\n\n\nMissing values in behavioural experiments\n\n\n\nAn example where you might get a missing value would be in a behavioral experiment providing a fixed window for the participant to respond (e.g., 3 seconds). If they don’t respond in time, you can code their response as a missing value (NA), instead of leaving it blank. This is useful for any subsequent analyses where you might want to exclude or perform an operation on the missing values."
  },
  {
    "objectID": "workshops/01.R_basics/qmd/introduction_to_r.html#data-types-classes-and-variables",
    "href": "workshops/01.R_basics/qmd/introduction_to_r.html#data-types-classes-and-variables",
    "title": "An introduction to R/RStudio",
    "section": "Data types, classes and variables",
    "text": "Data types, classes and variables\nThere are many types of data in R, here are some commonly used:\nNumeric - Decimal numbers (the most common numeric type)\n\n1.1\n2.0\n5.7892\n\nInteger - Whole numbers only\n\n1L  # Note: L suffix denotes integer\n2L\n3L\n\nCharacter/String - Text data in quotes\n\n\"hello world!\"\n'R programming'\n\"42\"  # Numbers in quotes are strings\n\nLogical - Boolean values for conditional logic\n\nTRUE\nFALSE\nT     # Shorthand for TRUE\nF     # Shorthand for FALSE\n\nFactor - a data type for categorical variables with fixed levels (categories).\nIn the example below, we create a vector of letters, some of which are repeated. However, the levels within are limited to each individual letter.\n\n# Define a factor using letters\nf &lt;- factor(letters[c(1, 1, 2, 2, 3:10)])  # Create factor from letters with some repetition\nf\n\n [1] a a b b c d e f g h i j\nLevels: a b c d e f g h i j\n\nclass(f)  # Check class\n\n[1] \"factor\"\n\n\n\n\n\n\n\n\nChecking your data type\n\n\n\nYou can check data types using the following commands: class, typeof and str\n\nclass(1.1)      # Shows type\n\n[1] \"numeric\"\n\ntypeof(\"text\")   # Shows underlying type\n\n[1] \"character\"\n\nstr(data)       # Shows structure\n\nfunction (..., list = character(), package = NULL, lib.loc = NULL, verbose = getOption(\"verbose\"), \n    envir = .GlobalEnv, overwrite = TRUE)  \n\n\n\n\nStoring and manipulating variables\nWe commonly assign numbers and data to variables, which we can then compute directly:\n\n# Define objects x and y with values of 3 and 2 respectively:\nx &lt;- 3\ny &lt;- 2\n\n# Some calculations with the defined objects x and y:\nx + y\n\n[1] 5\n\nx * y\n\n[1] 6\n\n\n\n\n\n\n\n\nCase sensitivity\n\n\n\nR is case sensitive, so X and x are not the same object!\n\n\n\nBasic R functions\nWe can perform basic statistics and operations on variables, such as getting the variance, standard deviation and summary statistics:\n\n# Combine variables together to form a vector (a sequence of elements of the same type)\nc(1, 3, -2)                         # Creates numeric vector\n\n[1]  1  3 -2\n\nc(\"a\", \"a\", \"b\", \"b\", \"a\")          # Creates character vector\n\n[1] \"a\" \"a\" \"b\" \"b\" \"a\"\n\n# Create a vector to work with\nx &lt;- c(1, 3, -2)\n\n# Variance and Standard Deviation\nvar(x)   # Calculates variance\n\n[1] 6.333333\n\nsd(x)    # Calculates standard deviation\n\n[1] 2.516611\n\n# Basic Statistics\nsum(x)   # Sums all elements\n\n[1] 2\n\nmean(x)  # Calculates mean\n\n[1] 0.6666667\n\n# Range Functions\nmin(x)   # Finds minimum\n\n[1] -2\n\nmax(x)   # Finds maximum\n\n[1] 3\n\n\nWe can also manipulate and combine vectors together - either as columns or rows - and perform statistics on the combined output:\n\n# Define two vectors\nx &lt;- c(1, 3, 4, 6, 8)    # First vector\ny &lt;- c(2, 3, 5, 7, 9)    # Second vector\n\n# Combine vectors as columns\ncbind(x, y)              # Creates a matrix with columns x and y\n\n     x y\n[1,] 1 2\n[2,] 3 3\n[3,] 4 5\n[4,] 6 7\n[5,] 8 9\n\n# Combine vectors as rows\nrbind(x, y)              # Creates a matrix with rows x and y\n\n  [,1] [,2] [,3] [,4] [,5]\nx    1    3    4    6    8\ny    2    3    5    7    9\n\n# Calculate correlation between x and y\ncor(x, y)                # Pearson correlation coefficient\n\n[1] 0.988765\n\n# Calculate covariance between x and y\ncov(x, y)                # Covariance\n\n[1] 7.65\n\n\n\n\n\n\n\n\nUsing cbind\n\n\n\ncbind can be used to combine data across different files. For example, we can use cbind to combine data for a single participant that was recorded across multiple platforms or studies."
  },
  {
    "objectID": "workshops/01.R_basics/qmd/introduction_to_r.html#miscellaneous-commands",
    "href": "workshops/01.R_basics/qmd/introduction_to_r.html#miscellaneous-commands",
    "title": "An introduction to R/RStudio",
    "section": "Miscellaneous commands",
    "text": "Miscellaneous commands\nHere are some other commands that will be useful when working with R more generally:\n\nDirectory and Workspace Management\n\n\n# Get current working directory\ngetwd()\n\n# Set working directory (example with different OS paths)\nsetwd(\"C:/Users/Documents/R\")           # Windows style\nsetwd(\"/home/user/Documents/R\")         # Unix/Mac style\nsetwd(\"~/Documents/R\")                  # Universal home directory\n\n# List files in current directory\ndir()                                   # Same as list.files()\n\n\nEnvironment Management\n\n\n# List objects in workspace\nls()\n\n# Remove all objects from workspace\nrm(list = ls())\n\n\nBasic Output Functions\n\n\n# Simple print statement\nprint(\"Hello World\")                    # Basic output\n\n[1] \"Hello World\"\n\n#&gt; [1] \"Hello World\"\n\n# Concatenate with spaces\ncat(\"Hello\", \"World\")                   # Prints: Hello World\n\nHello World\n\ncat(\"Hello\", \"World\", sep = \"_\")        # Prints: Hello_World\n\nHello_World\n\n# Join strings\npaste0(\"C:/\", \"Group1\")                 # Prints: C:/Group1\n\n[1] \"C:/Group1\"\n\npaste(\"C:/\", \"Group1\", sep = \"\")        # Same result\n\n[1] \"C:/Group1\"\n\n\n\nGetting Help\n\n\n# Get help on functions\n?mean                                   # Help for mean function\nhelp(\"mean\")                            # Same as above\n\n# Find functions containing keyword\nhelp.search(\"correlation\")              # Search help for \"correlation\"\n??correlation                           # Same as above\n\n\n\n\n\n\n\nEssential RStudio Shortcuts\n\n\n\nHere are some shortcuts that you can use in RStudio:\n\n\n\n\nShortcut\nAction\n\n\n\n\nCtrl + L\nClean console\n\n\nCtrl + Shift + N\nCreate a new script\n\n\n↑\nAccess command history\n\n\nCtrl(hold) + ↑\nSearch command history with current input\n\n\nCtrl + Enter\nExecute selected code in script\n\n\n\n\n\n\n\nThese shortcuts work on Windows/Linux. For Mac, replace Ctrl with Cmd (⌘)."
  },
  {
    "objectID": "workshops/01.R_basics/qmd/introduction_to_r.html#data-structures",
    "href": "workshops/01.R_basics/qmd/introduction_to_r.html#data-structures",
    "title": "An introduction to R/RStudio",
    "section": "Data structures",
    "text": "Data structures\nR offers several data structures that serve different purposes. Each structure is designed to handle specific types of data organization, from simple one-dimensional vectors to complex nested lists1.\n\n\n\n\n\nVector (1-dimensional)\n\nOne-dimensional sequence of elements\nAll elements must be of the same type (numeric, character, etc.)\nExample: c(1, 2, 3) or c(\"a\", \"b\", \"c\")\n\nMatrix (2-dimensional)\n\nTwo-dimensional arrangement of elements\nAll elements must be of the same type\nOrganized in rows and columns\nExample: matrix(1:9, nrow = 3, ncol = 3)\n\nArray (n-dimensional)\n\nExtension of matrices to multiple dimensions\nAll elements must be of the same type\nCan have 3 or more dimensions\nExample: array(1:27, dim = c(3, 3, 3))\n\nData Frame\n\nTwo-dimensional structure similar to a spreadsheet\nDifferent columns can contain different types of data\nMost common structure for statistical analysis\n\nHere are some examples of each datatype:\n\nVector\n\n\nv1 &lt;- 1:12  # Create numeric vector using sequence\nv2 &lt;- c(2,4,1,5,1,6, 13:18)  # Create numeric vector by combining values and sequence\nv3 &lt;- c(rep('aa',4), rep('bb',4), rep('cc',4))  # Create character vector with repeated values\nclass(v1)  # Check class of v1\n\n[1] \"integer\"\n\nclass(v2)  # Check class of v2\n\n[1] \"numeric\"\n\nclass(v3)  # Check class of v3\n\n[1] \"character\"\n\n\n\nMatrix and array\n\n\nm1 &lt;- matrix(v1, nrow=3, ncol=4)  # Create matrix filled by column\nm2 &lt;- matrix(v1, nrow=3, ncol=4, byrow = T)  # Create matrix filled by row\narr &lt;- array(v1, dim=c(2,2,3))  # Create 3D array\n\n\nclass(m1)  # Check class of matrix\n\n[1] \"matrix\" \"array\" \n\nclass(arr)  # Check class of array\n\n[1] \"array\"\n\n\n\nData frame\n\n\ndf &lt;- data.frame(v1=v1, v2=v2, v3=v3, f=f)  # Create dataframe from vectors\nclass(df)  # Check class of dataframe\n\n[1] \"data.frame\"\n\nstr(df)    # Display structure of dataframe\n\n'data.frame':   12 obs. of  4 variables:\n $ v1: int  1 2 3 4 5 6 7 8 9 10 ...\n $ v2: num  2 4 1 5 1 6 13 14 15 16 ...\n $ v3: chr  \"aa\" \"aa\" \"aa\" \"aa\" ...\n $ f : Factor w/ 10 levels \"a\",\"b\",\"c\",\"d\",..: 1 1 2 2 3 4 5 6 7 8 ...\n\nclass(df$v1)  # Check class of first column\n\n[1] \"integer\"\n\nclass(df$v2)  # Check class of second column\n\n[1] \"numeric\"\n\nclass(df$v3)  # Check class of third column\n\n[1] \"character\"\n\nclass(df$f)   # Check class of fourth column (factor)\n\n[1] \"factor\"\n\n\n\n\n\n\n\n\nUsing the $ Operator in R\n\n\n\nThe $ operator is used to access specific columns or elements within a list or data frame by name.\nSuppose we have a data frame of students’ grades and want to access the ‘Math’ column:\n\n# Create a data frame\nstudents &lt;- data.frame(\n  Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  Math = c(95, 88, 92),\n  Science = c(89, 94, 90)\n)\n\n# Access the Math column\nmath_scores &lt;- students$Math\nprint(math_scores)\n\n[1] 95 88 92\n\n\nOr if we wanted to get the class type for a column:\n\n# Get the class of the \"Math\" column\nmath_class &lt;- class(students$Math)\nprint(math_class)\n\n[1] \"numeric\"\n\n# Get the class of the \"Name\" column\nname_class &lt;- class(students$Name)\nprint(name_class)\n\n[1] \"character\"\n\n\n\n\n\n\n\n\n\n\nExercise 1: Basic R commands\n\n\n\n1. Open the R_basics.R script (within _scripts) and practice basic R commands and data types.\nOnly work up until the “Control Flow” section of code.\nTIP: use the class() and str commands."
  },
  {
    "objectID": "workshops/01.R_basics/qmd/introduction_to_r.html#logical-operators-control-flow-and-functions",
    "href": "workshops/01.R_basics/qmd/introduction_to_r.html#logical-operators-control-flow-and-functions",
    "title": "An introduction to R/RStudio",
    "section": "Logical operators, control flow and functions",
    "text": "Logical operators, control flow and functions\nWe commonly use logical operators in R to help make decisions in code and are essential in tasks like subsetting data, controlling loops, writing conditional statements, and filtering data.\n\n\n\n\nOperator\nSummary\n\n\n\n\n&lt;\nLess than\n\n\n&gt;\nGreater than\n\n\n&lt;=\nLess than or equal to\n\n\n&gt;=\nGreater than or equal to\n\n\n==\nEqual to\n\n\n!=\nNot equal to\n\n\n!x\nNOT x\n\n\nx | y\nx OR y\n\n\nx & y\nx AND y\n\n\n\n\n\nControl flow\nControl flow structures are fundamental building blocks in programming that determine how code executes. Two essential control flow mechanisms are if-else statements and for-loops:\n\nIf-else statements allow programs to make decisions based on conditions, enabling different code execution paths depending on whether conditions are true or false.\nFor-loops provide a way to automate repetitive tasks by executing code multiple times over a sequence of elements.\n\nHere’s a simple example involving reaction time (RT) data, which we often want to clean by removing outliers and incorrect responses. We typically exclude RTs that are too fast (suggesting anticipatory responses) or too slow (suggesting inattention), as well as trials where participants made errors.\nHere’s how we can do that using an if-else statement and a for-loop:\n\n# Sample reaction time data (in milliseconds) with accuracy (1 = correct, 0 = error)\nrt_data &lt;- c(245, 892, 123, 456, 2891, 567, 432, 345, 178, 654)\naccuracy &lt;- c(1, 1, 0, 1, 1, 1, 0, 1, 1, 1)\n\n# Initialize vector for cleaned data\nclean_rt &lt;- numeric(length(rt_data))\n\n# Process each trial\nfor (i in 1:length(rt_data)) {\n    if (accuracy[i] == 1 && rt_data[i] &gt; 200 && rt_data[i] &lt; 2000) {\n        # Include trial if:\n        # - Response was correct (accuracy = 1)\n        # - RT is above 200ms (not anticipatory)\n        # - RT is below 2000ms (not too slow)\n        clean_rt[i] &lt;- rt_data[i]\n    } else {\n        clean_rt[i] &lt;- NA  # Mark excluded trials as NA\n    }\n}\n\n# Calculate mean RT for clean trials only\nmean_rt &lt;- mean(clean_rt, na.rm = TRUE)\nmean_rt\n\n[1] 526.5\n\n\nIn this example, the for-loop processes each trial, while the if-else statement excludes responses that are too fast (&lt;200ms), too slow (&gt;2000ms), or incorrect.\n\n\nFunctions\nFunctions are operation(s) that are performed to obtain some quantity based on another quantity. They are often analagous to a ‘black box’ which processes some input \\(x\\), to generate an output \\(f(x)\\).\n\n\n\n\n\n\n\n\nThere are three broad sources of functions within R:\n\nBuilt-in functions: functions available automatically within R (e.g., mean, sum)\nExternal functions: functions written by others (e.g., as part of a package)\nUser-defined functions: functions written by the user\n\nA practical example of a user-defined functions is calculating the Standard Error of the Mean (SEM). The SEM measures the precision of a sample mean and is calculated as the standard deviation divided by the square root of the sample size minus one:\n\\[\nSEM = \\sqrt{\\frac{s^2}{n-1}}\n\\]\nHere’s how to implement this as a function in R:\n\n# function to calculate Standard Error of Mean (SEM)\nsem &lt;- function(x) {\n    # Calculate: sqrt(variance / (n-1))\n    # na.rm=TRUE removes NA values\n    # na.omit(x) gives us the actual sample size excluding NAs\n    sqrt(var(x, na.rm=TRUE) / (length(na.omit(x))-1))\n}\n\n# Example usage:\ndata &lt;- c(23, 45, 12, 67, 34, 89, 21)\nsem(data)\n\n[1] 11.31336\n\n\n\n\n\n\n\n\nExercise 2: Control flow and user-defined functions\n\n\n\n1. Write an if-else statement to do the following:\n\nGenerate a random number between 0 and 1.\nCompare it against 1/3 and 2/3\nPrint the random number and its position relative to 1/3 and 2/3\n\n\n\nClick to see the solution\n\n\nt &lt;- runif(1) # random number between 0 and 1\nif (t &lt;= 1/3) {\n    cat(\"t =\", t, \", t &lt;= 1/3. \\n\")\n} else if (t &gt; 2/3) {\n    cat(\"t =\", t, \", t &gt; 2/3. \\n\")\n} else {\n    cat(\"t =\", t, \", 1/3 &lt; t &lt;= 2/3. \\n\")\n}\n\nt = 0.973328 , t &gt; 2/3. \n\n\n\n2. Write a for-loop to do the following:\n\nGet the name of each month of this calendar year\nPrint it one by one\n\n\n\nClick to see the solution\n\n\nmonth_name &lt;- format(ISOdate(2025,1:12,1),\"%B\")\nfor (j in 1:length(month_name) ) {\n    cat()\n}\n\nprint(month_name)\n\n [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"     \n [7] \"July\"      \"August\"    \"September\" \"October\"   \"November\"  \"December\""
  },
  {
    "objectID": "workshops/01.R_basics/qmd/introduction_to_r.html#packages",
    "href": "workshops/01.R_basics/qmd/introduction_to_r.html#packages",
    "title": "An introduction to R/RStudio",
    "section": "Packages",
    "text": "Packages\nPackages are collections of functions, data sets, and documentation bundled together to extend the functionality of R. They are not part of the base R installation but can be easily added and used in your environment.\nR packages can:\n\nAdd functions: They contain pre-written functions that simplify common tasks or complex analyses. For example, packages like ggplot2 and dplyr offer powerful tools for data visualization and manipulation.\nProvide data: Some packages include data sets that can be used for testing or teaching purposes. For example, the datasets package provides a collection of sample data sets.\nEnable special features: Packages can implement specialized features like statistical models, machine learning algorithms, or tools for web scraping, reporting, and more.\n\n\nHow to use packages in R\nInstalling: You can install a package from CRAN (the Comprehensive R Archive Network) using the install.packages() function.\n\ninstall.packages(\"ggplot2\")\n\nLoading: Once installed, you can load the package into your R session with the library() function.\n\nlibrary('ggplot2')\n\nUsage: After loading the package, you can use its functions. For example, with ggplot2, you can create a plot like this:\n\nggplot(data = mtcars, aes(x = mpg, y = hp)) + \n  geom_point()\n\n\n\n\nPopular R packages include:\n\nggplot2: A powerful package for data visualization based on the grammar of graphics.\ndplyr: A package for data manipulation (filtering, selecting, grouping, etc.).\ntidyr: Used for tidying data, such as reshaping and pivoting.\nshiny: For building interactive web applications in R.\n\nYou can find and install R packages from a number of sources:\n\nCRAN: The main repository for R packages.\nBioconductor: A repository specializing in bioinformatics packages.\nGitHub: Many R developers host their packages on GitHub, which you can install using devtools or remotes packages."
  },
  {
    "objectID": "workshops/01.R_basics/qmd/introduction_to_r.html#data-visualization-using-ggplot2",
    "href": "workshops/01.R_basics/qmd/introduction_to_r.html#data-visualization-using-ggplot2",
    "title": "An introduction to R/RStudio",
    "section": "Data visualization using ggplot2()",
    "text": "Data visualization using ggplot2()\nOne of the main benefits of R is to create publication quality figures and graphs. There are a number of different functions within R that we can use:\n\nbuilt-in plotting functions e.g., plot()\nlattice - similar to built-in plotting functions\nggplot2() - making nicer plots using a layering approach\n\nWe will now briefly cover ggplot2() as it is the most versatile and used approach to create complex figures.\nggplot2 is a powerful R package for creating complex and customizable data visualizations. It provides a systematic approach to building plots by combining two main components: geometries (geom) and aesthetics (aes).\nplot = geometric (points, lines, bars) + aesthetic (color, shape, size)\nGeometries (geom): These define the type of plot or visual elements you want to display. Common geoms include:\n\ngeom_point(): Displays data points (scatter plot).\ngeom_line(): Plots lines connecting data points.\ngeom_bar(): Creates bar charts.\n\nAesthetics (aes): These define how data is mapped to visual properties. The aesthetics determine the appearance of the plot, such as:\n\ncolor: Specifies the color of the points, lines, or bars.\nshape: Defines the shape of data points (e.g., circles, squares).\nsize: Controls the size of the points or lines.\n\nImportantly, ggplot2() is built upon the layering of different components. For example, you can simply add more aes components to add a line of best fit, and standard error:\n\n\n\n\n\n\n\n\n\n\n\nThe R Graph Gallery\n\n\n\nYou can create many, many, many different types of graphs and plots using ggplot2(). You can check out it’s versatility by seeing examples at the R Graph Gallery."
  },
  {
    "objectID": "workshops/01.R_basics/qmd/introduction_to_r.html#footnotes",
    "href": "workshops/01.R_basics/qmd/introduction_to_r.html#footnotes",
    "title": "An introduction to R/RStudio",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKabacoff, R. I. (2022). R in action: data analysis and graphics with R and Tidyverse. Simon and Schuster.↩︎"
  },
  {
    "objectID": "workshops/03.bernoulli_coin/qmd/stan_notes.html",
    "href": "workshops/03.bernoulli_coin/qmd/stan_notes.html",
    "title": "Technical details on the Stan programming language",
    "section": "",
    "text": "Before we move onto further modeling in Stan, we need to understand a bit more about the language itself. This will help us both when constructing our Bernoulli and linear regression models in this exercise, and beyond."
  },
  {
    "objectID": "workshops/03.bernoulli_coin/qmd/stan_notes.html#why-use-stan",
    "href": "workshops/03.bernoulli_coin/qmd/stan_notes.html#why-use-stan",
    "title": "Technical details on the Stan programming language",
    "section": "Why use Stan?",
    "text": "Why use Stan?\nStan provides some advantages over previous probabilistic programming languages like BUGS (Bayesian inference Using Gibbs Sampling) and JAGS (Just Another Gibbs Sampler). This includes:\n1. Computational efficiency - The time to run models is quicker, particularly for more complex models\n2. Memory optimization - Stan uses only 1-10% of the memory required by BUGS/JAGS\n3. Advanced language features - You can overwrite variables, and Stan implements vectorization\n\n\n\n\n\n\nVectorization in Stan\n\n\n\nVectorization allows you to perform operations on entire arrays or matrices at once, rather than using loops. In Stan, this not only makes code more readable but also significantly improves computational efficiency.\n\n\nThese advantages have made Stan particularly valuable for running complex hierarchical models, and models with many parameters.\nFor sampling, Stan uses the No U-Turn Sampler (NUTS) as it’s sampling method. You can see it’s efficiency versus two other MCMC methods (Metropolis and Gibbs sampling) in the graph below:\n\n\n\n\n\n\nAn example sampling distribution for various MCMC methods, with 1 million draws thinned to 1,000 (Metropolis, Gibbs) and 1,000 draws (NUTS) compared to the ground truth\n\nCompared to Metropolis and Gibbs sampling, NUTS is able to perform a much better exploration of the parameter space, even with 1,000 times fewer draws! These samples ultimately look much more like the independent draws, which is a reference how the ideal sampling should look.\n\n\n\n\n\n\nWhat about these NUTS?\n\n\n\nNUTS is an extension of Hamiltonian Monte Carlo (HMC) that automatically tunes the sampler’s parameters. It uses information about the gradient of the posterior distribution to make informed proposals for where to sample next, rather than the random walks used by Metropolis and Gibbs samplers.\n\n\nYou can learn more about Stan’s advantages in the “Selling Stan” discussion."
  },
  {
    "objectID": "workshops/03.bernoulli_coin/qmd/stan_notes.html#variable-types-and-declaration",
    "href": "workshops/03.bernoulli_coin/qmd/stan_notes.html#variable-types-and-declaration",
    "title": "Technical details on the Stan programming language",
    "section": "Variable types and declaration",
    "text": "Variable types and declaration\nStan is a rather ‘un-intuitive’ programming language compared to R, Python or MATLAB, particularly due to its strict typing system and declaration rules.\nHere is an overview of some of it’s key properties:\n\nVariables must be explicitly declared with both their type and scope before use\nAll declarations must occur at the beginning of their respective blocks\nTypes are static and strictly enforced throughout the program\n\nUnlike flexible languages like R or Python, Stan strictly enforces type matching, with each variable having a type (static type; scalar, vector, matrix etc.).\nStan supports several basic types:\n\nint: Integer values\nreal: Real (decimal) values\nvector: One-dimensional arrays of reals\nmatrix: Two-dimensional arrays of reals\n\nStan has two primary scalar types:\nreal: For continuous numeric values\ndata {\n  real y;  // Unbounded continuous value\n}\nint: For integer values\ndata {\n  int n;  // Unbounded integer value\n}\nStan also supports vector types for one-dimensional arrays of real numbers:\ndata {\n  vector[3] v;  // Vector with 3 elements\n}\nand matrix types for two-dimensional arrays of real numbers:\ndata {\n  matrix[3,2] m;  // Matrix with 3 rows and 2 columns\n}\nBut types need to be correctly matched. The code below demonstrates that a vector cannot be assigned to a variable declared as a scalar type real:\n// This works\nreal x;\nx = 2.5;\n\n// Whereas this would cause an error\nreal y;\ny = [1, 2, 3];  // Can't assign vector to scalar\nStan allows you to constrain variables using lower and upper bounds. These constraints are particularly useful for ensuring counts are positive (lower=0) and restricting probabilities (lower=0, upper=1).\ndata {\n   int&lt;lower=1&gt; m;                   // Integer ≥ 1\n   int&lt;lower=0,upper=1&gt; n;           // Binary integer (0 or 1)\n   real&lt;lower=0&gt; x;                  // Positive real number\n   real&lt;upper=0&gt; y;                  // Negative real number\n   real&lt;lower=-1,upper=1&gt; rho;       // Real number between -1 and 1\n}\nStan also provides several types for handling multi-dimensional data such as vectors and matrices:\nvector[3] a;                    // 3-element column vector\nrow_vector[4] b;               // 4-element row vector\nmatrix[3,4] A;                 // A is a 3×4 matrix, A[1] returns a 4-element row vector\n\n// Constrained versions\nvector&lt;lower=0,upper=1&gt;[5] rhos;      // 5-element probability vector\nrow_vector&lt;lower=0&gt;[4] sigmas;        // 4-element positive row vector\nmatrix&lt;lower=-1,upper=1&gt;[3,4] Sigma;  // 3×4 matrix with bounded elements"
  },
  {
    "objectID": "workshops/03.bernoulli_coin/qmd/stan_notes.html#global-and-local-scope",
    "href": "workshops/03.bernoulli_coin/qmd/stan_notes.html#global-and-local-scope",
    "title": "Technical details on the Stan programming language",
    "section": "Global and local scope",
    "text": "Global and local scope\nEach block in Stan has distinct properties that determine how variables can be used. While all blocks require variable declarations, their scope and behaviour varies:\n\n\n\n\n\n\n\n\n\nThe data, transformed data, parameters, and transformed parameters blocks have global scope, meaning these variables are accessible throughout the program.\nIn contrast, variables in the model and generated quantities blocks have local scope. Variables declared within these blocks can only be accessed and used within this same block. They aren’t visible or usable in other parts of the Stan program.\nOnly parameters, transformed parameters, and generated quantities are saved in Stan’s output.\nThe model block is unique in that it can modify the posterior distribution, while the generated quantities block is the only place where random variables can be generated.\n\nHere’s some example Stan code demonstrating global versus local scope:\ndata {\n  real x;      // Global: accessible everywhere\n}\n\nparameters {\n  real theta;  // Global: accessible everywhere\n}\n\nmodel {\n  real temp;   // Local: only exists in model block\n  temp = x * 2;\n  \n  theta ~ normal(temp, 1);  // Can use both global (x, theta) and local (temp)\n}\n\ngenerated quantities {\n  real pred;   // Local: only exists in generated quantities\n  // temp not accessible here - it was local to model block\n  \n  pred = theta * x;  // Can use global variables (theta, x)\n}\nIn this example, x and theta are global and accessible everywhere, while temp and pred are local variables that can only be used in their respective blocks."
  },
  {
    "objectID": "workshops/03.bernoulli_coin/qmd/stan_notes.html#control-flow",
    "href": "workshops/03.bernoulli_coin/qmd/stan_notes.html#control-flow",
    "title": "Technical details on the Stan programming language",
    "section": "Control flow",
    "text": "Control flow\nStan also provides familiar control flow structures similar to R.\nFor example with if-else statements:\n// Simple if\nif (condition) {\n    statement;\n}\n\n// If-else\nif (condition) {\n    statement;\n} else {\n    statement;\n}\n\n// If-else if-else\nif (condition) {\n    statement;\n} else if (condition) {\n    statement;\n} else {\n    statement;\n}\nAnd for-loops:\n// Single for loop\nfor (j in 1:J) {\n    statement;\n}\n\n// Nested for loops\nfor (j in 1:J) {\n    for (k in 1:K) {\n        statement;\n    }\n}\n\n\n\n\n\n\nSemicolon Requirements\n\n\n\nRemember that unlike R, Stan requires semicolons (;) at the end of each statement within control structures!"
  },
  {
    "objectID": "workshops/03.bernoulli_coin/qmd/bernoulli_and_regression.html",
    "href": "workshops/03.bernoulli_coin/qmd/bernoulli_and_regression.html",
    "title": "Bernoulli and linear regression models in Stan",
    "section": "",
    "text": "So far we have only constructed a single model in Stan, the binomial model. In this section of the workshop we will construct two more, a Bernoulli model and a linear regression model."
  },
  {
    "objectID": "workshops/03.bernoulli_coin/qmd/bernoulli_and_regression.html#the-bernoulli-model",
    "href": "workshops/03.bernoulli_coin/qmd/bernoulli_and_regression.html#the-bernoulli-model",
    "title": "Bernoulli and linear regression models in Stan",
    "section": "The Bernoulli model",
    "text": "The Bernoulli model\nA Bernoulli model is an ideal next step from the binomial model, because they both describe the same underlying process - binary outcomes.\nWhile a binomial model aggregates multiple binary trials into a single count of successes, a Bernoulli model works with individual binary trials directly. Each Bernoulli trial has only two possible outcomes (success or failure, 0 or 1) and is characterized by a single parameter \\(θ\\), which represents the probability of success.\nIn fact, a binomial distribution can be thought of as the sum of \\(n\\) independent Bernoulli trials.\nThe binomial distribution models the number of successes \\(y\\) in \\(n\\) independent trials, where each trial has probability of success \\(\\theta\\). We write this as:\n\\[ y \\sim \\text{Binomial}(n, \\theta) \\]\nwith probability mass function:\n\\[ P(y|n,\\theta) = \\binom{n}{y}\\theta^y(1-\\theta)^{n-y}, \\quad y \\in {0,1,...,n} \\]\nHowever, the Bernoulli distribution is a special case where we model a single trial (\\(n=1\\)). We instead write this as:\n\\[ y \\sim \\text{Bernoulli}(\\theta) \\]\nwith probability mass function:\n\\[ P(y|\\theta) = \\theta^y(1-\\theta)^{1-y}, \\quad y \\in {0,1} \\]\nAn alternative definition of the Bernoulli distribution is that it is simply a special case of the binomial distribution where \\(n=1\\).\nFor example, if you flip a coin 10 times and count the total number of heads, you’re using a binomial model. However, if you’re recording each flip individually as heads (1) or tails (0), you’re working with Bernoulli trials. The underlying probability parameter \\(θ\\) remains the same in both cases.\nUsing the notation we learned earlier1, the Bernoulli model can be depicted as follows:\n\n\n\n\n\n\n\n\nIn this model, we have two parameters: \\(θ\\) and \\(h\\).\n\nThe parameter \\(θ\\) is an unobserved continuous parameter that represents the underlying probability of success, and in this example is given a uniform prior distribution between 0 and 1.\nThe parameter \\(h\\) is the outcome, an observed discrete variable that can only take values of 0 or 1, and follows a Bernoulli distribution with parameter \\(θ\\).\n\nThe diagram ultimately shows the model structure, where the unobserved \\(θ\\) influences the observed outcome \\(h\\) through the Bernoulli distribution.\n\nFitting the Bernoulli model in Stan\nWe can now fit fit the Bernoulli model ourselves to some example data. Within the 03.bernoulli_coin/_data folder, there is an example dataset for you to model: flip.RData.\n&gt; flip\n [1] 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1\nThe data consists of 20 coin flips stored in the vector flip, where each element is either 0 (tails) or 1 (heads). There are 15 heads (1’s) and 5 tails (0’s). The data is stored as an integer vector, which is appropriate since Bernoulli outcomes must be binary integers.\n\n\n\n\n\n\nExercise 6\n\n\n\n1. From scratch, try to create a Bernoulli model in Stan for the flip dataset, where we want to estimate the probability of getting heads.\nMake sure your model correctly specifies the input data:\n\nAccount for both the sample size and the flip outcomes\nInclude appropriate data types and bounds\n\ndefines the probability parameter:\n\nChoose an appropriate variable type and set valid bounds\n\nand links the data to the parameter using the Bernoulli distribution.\n2. In a corresponding R script, load in the data flip.RData and correctly assign it to a data list.\n\n\n\n\n\n\n\n\nWalkthrough of Exercise 6\n\n\n\nWe will go through Exercise 6 now, so please do not read ahead if you would like to have a go yourself!\n\n\nHere is an example Stan model from 03.bernoulli_coin/_scripts/bernoulli_coin_model1.stan:\ndata {\n  int&lt;lower=0&gt; N;\n  int&lt;lower=0,upper=1&gt; flip[N];\n}\n\nparameters {\n  real&lt;lower=0,upper=1&gt; theta;\n}\n\nmodel {  \n  for (n in 1:N) {\n    flip[n] ~ bernoulli(theta);\n  }\n}\nRemember that we need three main blocks: data, parameters and model:\ndata:\n\nWe declare N as an integer representing the number of observations\nWe declares flip as an array of N integers, constrained between 0 and 1\n\nparameters:\n\nWe declare a single parameter, theta, as a real number\ntheta is constrained between 0 and 1, as it represents a probability\n\nmodel:\n\nWe use a for loop to iterate through each flip in the data\nEach flip is modeled as a Bernoulli trial with probability theta\n\nThe model assumes each flip is independent and has the same underlying probability of success, theta, which we’re trying to estimate from the data.\nOk, so now we can fit the model:\nFirstly, we just need to load in the data and assign it to a data list (the data type that Stan can read):\nload('_data/flip.RData')\nN &lt;- length(flip)\ndataList &lt;- list(flip=flip, N=N)\nGreat! So now we can actually run the model (model1 in scripts/bernoulli_coin_main.R) and print the posterior density distribution of theta. The model output should look like this:\n&gt; print(fit_coin1)\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n        mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\ntheta   0.73    0.00 0.09   0.52   0.67   0.73   0.79   0.88  1522    1\nlp__  -13.39    0.02 0.68 -15.39 -13.56 -13.13 -12.94 -12.89  1693    1\nWith the posterior density distribution for \\(θ\\) being:\n\n\n\n\n\n\n\n\nYou may have noticed that there are two Stan models for the Bernoulli example. They are almost identical (and in fact produce the same results), but with a slight difference in how the model block is defined:\nModel 1 takes 61 seconds to compile/execute:\nmodel {  \n  for (n in 1:N) {\n    flip[n] ~ bernoulli(theta);\n  }\n}\nWhereas Model 2 takes 53 seconds to compile/execute:\nmodel {  \n  flip ~ bernoulli(theta);\n}\nThe difference is that the second version takes advantage of vectorization in Stan. This vectorized version is more efficient because it applies the Bernoulli distribution to the entire vector flip at once, rather than processing each element individually through a loop.\nStan is optimized for these vectorized operations - it can process operations on entire vectors or arrays more efficiently than element-by-element operations. Just as sum(x) in R is faster than adding elements in a for loop, Stan’s vectorized operations are computationally more efficient."
  },
  {
    "objectID": "workshops/03.bernoulli_coin/qmd/bernoulli_and_regression.html#linear-regression",
    "href": "workshops/03.bernoulli_coin/qmd/bernoulli_and_regression.html#linear-regression",
    "title": "Bernoulli and linear regression models in Stan",
    "section": "Linear regression",
    "text": "Linear regression\nUp to this point we have created Stan models involving the binomial and the Bernoulli distribution. In both cases, each of these involves estimating the values of a single unknown parameter \\(θ\\). We will now do the same for an instance where we will estimate multiple unknown parameters, using a linear regression model.\n\n\n\n\n\n\nLinear regression directory\n\n\n\nThe code and data for this exercise are located within the /04.regression_height directory, so switch accordingly!\n\n\nIn this instance, we will be following an example provided in McElreath’s Statistical Rethinking2, which aims to model partial census data for the !Kung San tribe from the Dobe region of the Kalahari desert3.\nIf we look at the data (_data/height.RData) we can see it contains four columns: height, weight, age and male:\n   height   weight age male\n1 151.765 47.82561  63    1\n2 139.700 36.48581  63    0\n3 136.525 31.86484  65    0\n4 156.845 53.04191  41    1\n5 145.415 41.27687  51    0\n6 163.830 62.99259  35    1\nLinear regression in principle allows us to model the relationship between these variables, such that given any known value for one, we can predict the value of the other.\nRunning the model on two variables height and weight creates the graph below, which models the relationship between the two:\n\n\n\n\n\n\nLinear regression model for height against weight in the !KungSan tribe\n\nThe results of this regression demonstrate a significant positive relationship between height and weight, where each unit increase in weight is associated with a 0.91 unit increase in height, with the model explaining about 57% of the variation in height \\((R² = 0.57)\\):\n&gt; summary(L)\n\nCall:\nlm(formula = height ~ weight, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.7464  -2.8835   0.0222   3.1424  14.7744 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 113.87939    1.91107   59.59   &lt;2e-16 ***\nweight        0.90503    0.04205   21.52   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 5.086 on 350 degrees of freedom\nMultiple R-squared:  0.5696,    Adjusted R-squared:  0.5684 \nF-statistic: 463.3 on 1 and 350 DF,  p-value: &lt; 2.2e-16\nBut, how can we run this same analysis in a Bayesian approach?\n\nRethinking the regression model\nRunning a linear regression in a Bayesian approach requires thinking about what the likelihood function is. Typically, running the regression (as we did above) does not require us to think about this. But the regression model itself is not a likelihood function. We therefore need to determine which statistical distribution underlies the regression model.\nIn this case, this is the normal (Gaussian) distribution.\nWhy? Well, in a standard linear regression, we typically model the relationship between variables using two components: a systematic part and a random part. The systematic part is expressed as:\n\\[\\mu_i = \\alpha + \\beta x_i\\]\nwhere:\n\n\\(\\mu_i\\) represents our predicted value,\n\\(\\alpha\\) is our intercept,\nand \\(\\beta\\) is our slope coefficient.\n\nThe random part is then added as an error term:\n\\[y_i = \\mu_i + \\epsilon\\]\nwhere \\(\\epsilon\\) represents random error, traditionally assumed to be normally distributed with mean \\(0\\) and some variance \\(\\sigma^2\\):\n\\[\\epsilon \\sim Normal(0, \\sigma^2)\\]\nHowever, rather than separating our model into these two parts, we can directly express our observations \\(y_i\\) as being drawn from a normal distribution centered at our expected value \\(\\mu_i\\):\n\\[y_i \\sim Normal(\\mu_i, \\sigma)\\]\nThe image below visually depicts this 4:\n\n\n\n\n\n\n\n\n\nRethinking the linear regression model in a Bayesian context\n\nConventionally, our regression model generates a prediction for any given value of \\(x\\): the value of \\(y\\). This prediction is the red line. However, we can also see that for any given value, there is always some error term. Looking at \\(x_0\\), the predicted value of \\(y(x_0)\\) - indicated by the green dashed line - is actually slightly below the observed value (blue dot). This is the case for all values of \\(x\\), which can be modeled as being normally distirbuted around the true value (this is depicted for \\(x_0\\)).\nThis formulation is more natural in a Bayesian context as it directly specifies our likelihood function. It captures our belief that observations should be symmetrically distributed around our expected value \\((\\mu_i)\\), with smaller deviations being more likely than larger ones. The parameter \\(\\sigma\\) then represents our uncertainty about how far observations typically deviate from this expected value.\nThis is mathematically equivalent to the traditional approach, but aligns with Bayesian inference.\nTo better understand the relationships between the different variables in our Bayesian approach to linear regression, we can plot this as a directed acrylic graph:\n\n\n\n\n\n\n\n\n\nThe linear regression model in graphical notation\n\nIn this diagram we can see that:\n\nAll of the variables are continuous, and that the data \\(x_i\\) and \\(y_i\\) are known.\nThe slope \\(\\alpha\\) and intercept \\(\\beta\\) together with the known value of \\(x_i\\) generate the predicted value for the other data \\(\\mu_i\\)\nHowever, the true value of the other data \\(y_i\\) is normally distributed around \\(\\mu_i\\) with a standard deviation \\(\\sigma\\)\n\nThe double circle around \\(\\mu_i\\) means that it is a deterministic variable, i.e., completely determined by other variables. Specifically, \\(\\mu_i\\) is determined by the equation \\(\\mu_i = \\alpha + \\beta x_i\\), meaning once we know the values of \\(\\alpha\\), \\(\\beta\\), and \\(x_i\\), there’s no uncertainty about what \\(\\mu_i\\) will be.\n\n\nBuilding a linear regression model in Stan\nHow would we go about writing a linear regression model in Stan?\nLet’s first describe the model block. It makes sense at first to write something like this:\nmodel {\n  vector[N] mu;\n  for (i in 1:N) {\n    mu[i] = alpha + beta * weight[i];\n    height[i] ~ normal(mu[i], sigma);\n  }\n}\nIn this case, we’re translating the diagram directly into code. For each observation \\(i\\):\n\nWe calculate the expected height mu[i] using our linear equation alpha + beta * weight[i]\nWe then model the actual height as being normally distributed around this expected value\n\nWhile this is perfectly valid, we can make it more efficient. Remember that because Stan allows us to perform calculations on entire vectors at once (vectorization), we can get rid of the for loop:\nmodel {\n  vector[N] mu;\n  mu = alpha + beta * weight;\n  height ~ normal(mu, sigma);\n}\nThis second version does exactly the same thing, but instead of calculating each mu[i] separately, it does all calculations at once, and is therefore more computationally efficient.\nWe can take this one step further. Since mu is just an intermediate calculation, we can insert it directly into our normal distribution:\nmodel {\n  height ~ normal(alpha + beta * weight, sigma);\n}\nThis final version is the most elegant - it directly expresses that our heights are normally distributed around alpha + beta * weight.\n\n\n\n\n\n\nElegance at a cost\n\n\n\nWhilst the third version is certainly the most elegant, whether it is ‘better’ than the second version is debatable. Ultimately, whether you write your model code as in the second or third example depends on your proficiency and confidence with Stan. Sometimes having more detail and knowing exactly which calculations were performed in the code can be useful!\n\n\nAs we are implementing a Bayesian approach, we need to provide priors for our model.\nWhat priors do we set? Which distributions should we use?\nFor our intercept \\(\\alpha\\) and slope \\(\\beta\\), we use normal distributions.\n\\[\\alpha \\sim Normal(170, 100)\\]\n\\[\\beta \\sim Normal(0, 20)\\]\nSince \\(\\alpha\\) represents the expected height when weight is zero, we center it around 170 (a reasonable height in cm) with a large standard deviation of 100 to keep it weakly informative. For the slope, we center around 0 with standard deviation 20, allowing for both positive and negative relationships between height and weight.\nFor the standard deviation (\\(\\sigma\\)) prior, we use a half-Cauchy distribution:\n\\[\\sigma \\sim halfCauchy(0, 20)\\]\nThe half-Cauchy is a distribution particularly suitable for standard deviation parameters because of it’s properties. Looking at the graph below, you can see how it differs from a normal distribution - it has “fatter tails”, meaning it assigns more probability to larger values.\n\n\n\n\n\n\nThe normal and half-Cauchy distributions plotted for positive values of \\(x\\)\n\nWhere \\(x = 5\\), notice how the area under the Cauchy curve extends further into the tails compared to the normal distribution. This makes the half-Cauchy weakly informative by places reasonable probability on moderate standard deviations while still allowing for larger ones if the data supports them.\nLet’s now create our specific model to run in Stan:\nWe will firstly want to load in the data into our data block. Remember that we have three variables:\n\nN as the number of observations,\nheight and weight as vectors of length N.\n\nWe should also ensure that these measurements can’t be negative. So our data block will look like this:\ndata {\n  int&lt;lower=0&gt; N;\n  vector&lt;lower=0&gt;[N] height;\n  vector&lt;lower=0&gt;[N] weight;\n}\nIn the parameters block, we should define the parameters we want Stan to estimate. We have three parameters:\n\nalpha (intercept) and beta (slope) are declared as real (continuous) numbers without constraints\nsigma (standard deviation) is also a real number, but should be constrained to be positive\n\nOur parameters block will therefore be:\nparameters {\n  real alpha;\n  real beta;\n  real&lt;lower=0&gt; sigma;\n}\nThe model block contains two key components both of which we have discussed above. This consists:\n\nOur prior distributions for our parameters, using the distributions we discussed earlier\nThe likelihood function, which specifies our linear regression model\n\nmodel {\n  # set priors\n  alpha ~ normal(170, 100);\n  beta  ~ normal(0, 20);\n  sigma ~ cauchy(0, 20);\n\n  # likelihood function\n  height ~ normal(alpha + beta * weight, sigma);\n}\nIn addition to this, we might also want to run a posterior predictive check. We will learn more about posterior predictive checks in future workshops, but for now it is simply enough to know that posterior predictive checks helps us assess how well our model captures the patterns in our data by generating new data from our fitted model and comparing it to our actual observations.\nIn our Stan model, we can add a posterior predictive check by adding a generated quantities block:\ngenerated quantities {\n  vector[N] height_bar;\n  for (n in 1:N) {\n      height_bar[n] = normal_rng(alpha + beta * weight[n], sigma);\n  }\n}\nThe generated quantities block is executed after Stan has finished sampling and is used for computing additional quantities of interest from the posterior samples. It can be used for tasks like posterior predictive checks, calculating derived parameters, or computing prediction intervals.\nIn this case, the block generates new height values (height_bar) for each weight in our dataset. For each observation, it draws a random value from a normal distribution using our estimated parameters \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\). These simulated values represent what our model thinks the data should look like given our parameter estimates.\n\n\n\n\n\n\nExercise 7\n\n\n\n1. Run the linear regression model and plot the posterior density for a single generated value and the posterior predictive check for the provided data.\nThe R script for you to run the linear regression model is regression_height_ppc_main.R which runs the Stan model regression_height_ppc_model.stan.\n\n\nThe posterior density for a single value, and posterior predictive check should look like this:\n\n\n\n\n\nThis plot specifically shows the posterior predictive distribution for a single predicted height value height_bar[1], which corresponds to the first weight observation in the dataset (47.8 kg).\nNow let’s plot our regression model itself:\n\n\n\n\n\n\n\n\n\nThe full regression model fit with 95% HDI shaded\n\nIn the plot above, the red line shows our model’s predicted relationship between height and weight. The grey shaded area represents the Highest Density Interval (HDI), the region where we expect most of our observations - in this case 95% - to fall according to our model.\n\n\n\n\n\n\nHighest density intervals\n\n\n\nThe HDI (which could be 80%, 89%, 90%, or 95% as shown) represents the range containing that percentage of the predicted values. This helps us assess whether our model’s predictions align well with our observed data and gives us a sense of our prediction uncertainty."
  },
  {
    "objectID": "workshops/03.bernoulli_coin/qmd/bernoulli_and_regression.html#footnotes",
    "href": "workshops/03.bernoulli_coin/qmd/bernoulli_and_regression.html#footnotes",
    "title": "Bernoulli and linear regression models in Stan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLee, M. D., & Wagenmakers, E. J. (2014). Bayesian cognitive modeling: A practical course. Cambridge university press.↩︎\nMcElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan, 2nd Ed. CRC Press.↩︎\nHowell, N. (2010). Life Histories of the Dobe !Kung: Food, Fatness, and Well-being over the Life-span. Origins of Human Behavior and Culture. University of California Press.↩︎\nBishop, C. M., & Nasrabadi, N. M. (2006). Pattern recognition and machine learning (Vol. 4, No. 4, p. 738). New York: Springer.↩︎"
  },
  {
    "objectID": "workshops/09.debugging/qmd/intro.html",
    "href": "workshops/09.debugging/qmd/intro.html",
    "title": "Debugging in Stan",
    "section": "",
    "text": "Welcome to the ninth and final workshop of the BayesCog course!\nCollectively, over this course we have built up our knowledge and ability to formalise and write cognitive models in Stan. We have done so using multiple examples, from basic probability models like the binomial and Bernoulli, to linear regression, and then explored various reinforcement learning models - from simple Rescorla-Wagner models to more complex hierarchical models.\nAll the while, things have gone relatively smoothly; we have not encountered major problems when writing or running our Stan models. However, it is worth to bear in mind that the materials (including the Stan models) for this course were carefully constructed and checked to work correctly without issue. In this final workshop, we’ll focus on debugging and fixing common errors in Stan code, and discuss best coding practices.\nThe goals of this workshop are to:\n\nUnderstand common errors and warnings in Stan and how to resolve them\nLearn best practices for writing and debugging Stan code\nGain practical experience debugging a memory retention model\nUnderstand the cognitive modeling framework through an example study\n\n\n\n\n\n\n\nWorking directory for this workshop\n\n\n\nModel code and R scripts for this workshop are located in the (/workshops/09.debugging) directory. Remember to use the R.proj file within each folder to avoid manually setting directories!\n\n\nThe copy of this workshop notes can be found on the course GitHub page."
  },
  {
    "objectID": "workshops/06.reinforcement_learning/qmd/hrch_rl_intro.html",
    "href": "workshops/06.reinforcement_learning/qmd/hrch_rl_intro.html",
    "title": "Hierarchial Bayesian models",
    "section": "",
    "text": "Welcome to the seventh workshop of the BayesCog course!\nIn our last workshop, we explored how to implement the Rescorla-Wagner model in Stan; estimating learning rates and inverse temperatures for both individual subjects and groups. When modeling a population of participants, our ultimate goal is to accurately estimate each individual’s true learning parameters while accounting for uncertainty and noise in our measurements. Currently, we either pool all participants together (assuming everyone is identical) or fit each person independently (ignoring information from the group that could help constrain our estimates). Hierarchical Bayesian modeling provides a solution by using group-level information to improve individual parameter estimates, particularly when data is limited or noisy for some participants.\nIn this workshop, we’ll firstly explore how hierarchical models work and implement them in our reinforcement learning model. We will then learn how to optimize Stan code through reparameterization and by changing Stan’s sampling parameters.\nThe goals of this workshop are to:\n\nUnderstand the limitations of both fixed effects and independent parameter estimation approaches\nLearn how hierarchical Bayesian modeling provides a principled approach for analyzing group-level data\nImplement a hierarchical version of the Rescorla-Wagner model in Stan\nUnderstand and implement reparameterization to optimize Stan code\nKnow how and when to change Stan’s sampling parameters where necessary\n\n\n\n\n\n\n\nWorking directory for this workshop\n\n\n\nModel code and R scripts for this workshop are located in the (/workshops/06.reinforcement_learning) and (/workshops/07.optm_rl) directories. Remember to use the R.proj file within each folder to avoid manually setting directories!\n\n\nThe copy of this workshop notes can be found on the course GitHub page."
  },
  {
    "objectID": "workshops/06.reinforcement_learning/qmd/rl_concepts.html",
    "href": "workshops/06.reinforcement_learning/qmd/rl_concepts.html",
    "title": "Fundamentals of cognitive modeling and reinforcement learning",
    "section": "",
    "text": "“Essentially, all the models are wrong, but some are useful.” - George E.P. Box (1976)"
  },
  {
    "objectID": "workshops/06.reinforcement_learning/qmd/rl_concepts.html#computational-and-cognitive-modeling",
    "href": "workshops/06.reinforcement_learning/qmd/rl_concepts.html#computational-and-cognitive-modeling",
    "title": "Fundamentals of cognitive modeling and reinforcement learning",
    "section": "Computational and cognitive modeling",
    "text": "Computational and cognitive modeling\nFor centuries, astronomers believed that planets moved in perfect circular orbits around the Sun. This mathematical model made intuitive sense and could roughly predict planetary positions, but it failed to fully capture the observed data. The breakthrough came when Johannes Kepler proposed that planets actually follow elliptical orbits; this new mathematical model not only better explained existing observations but also made more accurate predictions about planetary positions.\n\n\n\n\n\n\n\n\n\nOrbiting patterns of the Earth around the Sun according to the circular and eliptical models\n\nThis historical example illustrates a fundamental principle of scientific modeling: we use mathematics to approximate and understand processes that we cannot directly observe.\nWe essentially apply this same principle to understand human behaviour. Just as astronomers couldn’t directly see the shape of Earth’s orbit but could infer it through mathematical modeling, cognitive scientists cannot directly observe the mental processes occurring in someone’s mind. Instead, we create mathematical models that attempt to explain observed behavioural patterns – reaction times in a decision-making task, patterns of errors in memory recall, or neural activity during maze navigation.\nFor instance, when we observe that people take longer to make decisions when faced with more options, or in response to making an error, we can create mathematical models that propose specific cognitive mechanisms to explain these patterns. These models generate precise predictions about behaviour under different conditions, which we can test experimentally. When a model successfully predicts behaviour across multiple scenarios, it suggests that we may have captured something meaningful about the underlying cognitive processes involved.\nTherefore, computational modeling is nothing new, it’s specific application towards understanding human behaviour is.\nComputational modeling in cognitive science takes two main approaches, as illustrated in the image below1: theory-driven (top-down) and data-driven (bottom-up).\n\n\n\n\n\n\nTheory and data-driven approaches to computational modeling\n\nData-driven approaches - typically used for prediction and classification - start with data (e.g., behavioural or neural) and use statistical techniques to identify patterns and relationships.\nOn-the-other-hand, theory-driven approaches start with hypotheses about cognitive mechanisms, formalized into mathematical models that make specific predictions about behaviour. The predictions are then tested against empirical data, leading to refinement of both model and theory. This approach aims to provide mechanistic explanations of cognitive processes - in other words, to explain not just what behaviour occurs, but how and why it occurs.\nWe will exclusively focus on the theory-driven approach in this course.\n\nCognitive models as mathematical functions\nThe basic premise of cognitive modeling lies with understanding the brain as an information processing system that can be described mathematically. Generally, the processing of some input by a function to generate output is given as:\n\\[y = f(x)\\]\nwhere \\(x\\) represents input to the system, \\(f\\) represents the function, and \\(y\\) represents the output.\nTo describe the brain as a mathematical ‘engine’, we can simply replace each of these with the external stimulus, cognitive processes and observed behaviour respectively.\nBut this interpretation of the brain is nothing new, in fact the psychologist Kurt Lewin in 19362 demonstrated a similar framework with his equation:\n\\[B = f(P,E)\\]\nwhere \\(B\\) represents behavior, \\(P\\) represents the person (their internal state and processes), and \\(E\\) represents the environment.\nThe relationship between computational models and observable data can also be understood in terms of forward modeling and model inversion 345:\nThe image below depicts an example participant completing a behavioural task. For example - relevant to this course - they may be completing the two-choice reversal learning task where they select which image they think is going to give them a reward by pressing the associated button.\n\n\n\n\n\n\nThe cognitive modeling framework consists of forward models, which predict data from theory, and model inversion, which infer cognitive parameters from observations\n\nThe forward model represents how we believe the cognitive process generates the data we observe. For instance, in reversal learning, our model might specify how a participant updates their beliefs about reward probabilities and translates these into choices. When applied to neuroimaging, it might describe how cognitive processes should manifest in observed brain activity patterns.\nModel inversion works in the opposite direction - it uses the observed data to infer the underlying parameters or states of our model. In behavioural analysis, if we observe a participant consistently choosing one option over another, model inversion helps us infer their underlying reward expectations or learning rates. In neuroimaging, when we observe different patterns of neural activity to faces versus houses, model inversion allows us to infer which brain regions are preferentially involved in processing each category.\nThe right side of the image above shows a computational model (top) that can generate simulated data (bottom). By comparing this simulated data with real observations (shown by the red horizontal arrows), we can assess how well our model captures the actual cognitive processes. If the model-generated data closely matches the observed data, we gain confidence that our model approximates the underlying cognitive mechanisms.\nThis bidirectional approach of forward modeling and model inversion is powerful because it allows us to both predict behaviour based on theory and infer hidden cognitive processes from observable data.\nThe quote by George Box at the top of this page is very important to remember when understanding cognitive models; that no model is correct, some are just more correct than others. In some ways, the quote can be reformulated as follows:\n\n\n“Essentially all the models are imperfect, but some are useful.”\n\n\n\n\n\n\n\n\nModel-based fMRI: Linking computation to brain activity\n\n\n\nTraditional fMRI analyses can tell us which brain regions are more active during different task conditions, but they can’t reveal how specific cognitive computations are implemented in the brain. Model-based fMRI solves this by linking computational models to neural activity. We first fit our model to behaviour to estimate trial-by-trial variables (like prediction errors or uncertainty), then use these estimates as regressors in our fMRI analysis to identify active brain regions.\nAn introduction to model-based fMRI is included as a bonus workshop in the course!"
  },
  {
    "objectID": "workshops/06.reinforcement_learning/qmd/rl_concepts.html#introduction-to-reinforcement-learning",
    "href": "workshops/06.reinforcement_learning/qmd/rl_concepts.html#introduction-to-reinforcement-learning",
    "title": "Fundamentals of cognitive modeling and reinforcement learning",
    "section": "Introduction to reinforcement learning",
    "text": "Introduction to reinforcement learning\nHumans constantly interact with their environment, making decisions and learning from their outcomes. A child learning to ride a bike, a student solving mathematical problems, or an adult learning to navigate a new city - all these scenarios involve persistent learning through trial and error. How does this learning occur? How do we know which actions lead to better outcomes?\nReinforcement learning (RL) has emerged as one of the most powerful frameworks for understanding how organisms learn from experience. At its core, RL describes how we learn to make decisions that maximize rewards and minimize punishments through our interactions with the environment. When an action leads to a positive outcome, we’re more likely to repeat it; when it leads to a negative outcome, we tend to avoid it in the future.\nReinforcement learning initially emerged in computer science, gaining popularity in the 1980s. However, it was Richard Sutton and Andrew Barto’s 1998 book “Reinforcement Learning: An Introduction”6, which had a significant impact on the fields of biology and psychology. Even though Sutton and Barto were computer scientists developing artificial intelligence algorithms, their mathematical framework turned out to describe biological learning with remarkable accuracy. The key concepts they formalized - such as prediction errors and value functions - aligned surprisingly well with how the brain appears to process rewards and guide behaviour.\n\n\n\n\n\n\n\n\n\nThe first and second edition of Sutton and Barto’s influential textbook\n\n\n\n\n\n\n\nA must read\n\n\n\nWhilst mathematically dense for most, Reinforcement learning: An introduction is nonetheless recommended reading!\n\n\nYou can see this exponential increase in popularity for cognitive modeling over time in the diagram below showing the relative frequency of PubMed entries for ‘cognitive’ and ‘cognitive and computational’ as a function of the year7:\n\n\n\n\n\n\n\n\n\nNote that the increase in ‘Cognitive and computational’ is almost 3x greater than `Cognitive’ alone!\n\n\nReinforcement learning models: the Rescorla-Wagner choice rule\nTo understand how humans learn from experience through reinforcement learning, scientists need controlled experimental paradigms that can measure learning and decision-making. The two-armed bandit task has emerged as a popular option because it captures fundamental aspects of behaviour in a simple, controlled setting.\nNamed after slot machines (or “one-armed bandits”), the task presents participants with two choices (like two slot machines) that deliver rewards with different probabilities or amounts. Just as a gambler must learn which slot machine pays out more frequently, participants aren’t told these properties in advance; they must discover through trial and error which option yields more reward.\nThe exact properties of the bandits can be different across tasks (i.e., reward probability, magnitude), but the underlying structure remains the same.\n\n\n\n\n\n\n\n\n\nWhilst the two-armed bandit does not necessarily feature slot machines, its structure reflects the same underlying properties; a choice between two options where the probability of reward is not known\n\nShould you stick with an option that seems good based on your limited experience, or try the other option to gather more information? This creates an engaging dynamic where participants must balance:\n\nPredicting values: Participants need to determine how rewarding each option is\nMaking choices: They must decide which option to select based on their belief\nLearning from outcomes: After each choice, they update their predictions based on whether they received a reward or not\n\nBut how does reinforcement learning theories explain behaviour observed in these tasks?\n\n\n\n\n\n\nThe key principles of reinforcement learning, illustrating the basic interaction cycle between an agent and its environment and the key cognitive processes underlying decision-making\n\nThe left side describes the fundamental overview of reinforcement learning, which can be understood as a continuous interaction between an agent (e.g., a human) and their environment (e.g., a behavioural task)8:\nThis interaction follows a simple cycle:\n\nThe agent observes their environment\nBased on these observations, they take actions\nThese actions affect the environment\nThe environment provides feedback (rewards or punishments)\nThe agent learns from this feedback to improve future decisions\n\nThis process - guided by a goal (such as maximizing rewards) - allows agents to learn optimal behavior through experience.\nTo this end, RL incorporates value-based decision-making, by which learned values guide choices between different options. On the right, five basic processes of value-based decision-making are depicted9:\n\nRepresentation: The brain first needs to understand the decision scenario by mapping out the available choices, understanding its own internal state (like hunger or fatigue), and recognizing relevant external factors in the environment\nValuation: Each possible action is assigned a subjective value based on previous experiences and current circumstances\nAction selection: Using these value estimates, the brain selects and executes one of the available actions\nOutcome evaluation: Once an action is taken, the brain evaluates how rewarding or beneficial the actual outcome was\nLearning: The brain uses this outcome information to refine its decision-making process - adjusting how it represents problems, values options, and makes choices in future situations"
  },
  {
    "objectID": "workshops/06.reinforcement_learning/qmd/rl_concepts.html#modeling-the-two-choice-task-using-the-rescorla-wagner-model",
    "href": "workshops/06.reinforcement_learning/qmd/rl_concepts.html#modeling-the-two-choice-task-using-the-rescorla-wagner-model",
    "title": "Fundamentals of cognitive modeling and reinforcement learning",
    "section": "Modeling the two-choice task using the Rescorla-Wagner model",
    "text": "Modeling the two-choice task using the Rescorla-Wagner model\nHow might we practically apply RL models to behavioural choice data? Let’s revisit the two-choice task, specifically the structure that we first introduced in Workshop 2.\n\n\n\n\n\n\n\n\n\nThe two alternative forced-choice task, where subjects must select between two fractals\n\nRecall that in this task:\n\nSubjects are presented with two fractals (choice presentation)\nThey select one which they think will reward them (action selection)\nThey recieve a outcome (either a reward or a loss) (outcome)\n\nIn this task the choice and outcome are our data. From this data, we can subsequently measure basic summary statistics such as choice accuracy, which will provide us with a basic measure of subject performance. We can also compare classify performance within the group by a median split, and correlate it with other recorded variables (e.g., age, IQ).\nHowever, what we would like to do in cognitive modeling is to use the RL framework to describe the latent processes underlying participant choice.\nWhen trying to understand human behaviour through cognitive modeling, we typically need two complementary parts to our models: the cognitive model and the observation model.\n\nA cognitive model describes the internal mental processes we think are happening - like how people calculate values, make predictions, or learn from errors. This part deals with the “hidden” or “latent” processes we can’t directly observe\nAn observation model connects these internal processes to actual measurable behaviour - like the choices people make or their reaction times\n\nThe Rescorla-Wagner (RW) model10 is a cognitive model; one of the most influential in psychology.\nThe Rescorla-Wagner model, developed by Robert Rescorla and Allan Wagner in 1972 to explain animal learning in classical conditioning experiments, is a cornerstone of human learning research and modern reinforcement learning theory. The base model is simple: learning is driven by prediction errors - the difference between what we expect and what actually happens. When our predictions are wrong, we learn and adjust our expectations. When they’re right, we make smaller adjustments or none at all.\nMathematically, the model consists of two key equations:\nthe Value Update:\n\\[V_t = V_{t-1} + \\alpha PE_{t-1}\\]\nand the Prediction Error:\n\\[PE_{t-1} = R_{t-1} - V_{t-1}\\]\nWhere:\n\n\\(V_t\\) is the value/expectation for the current trial \\(t\\)\n\\(V_{t-1}\\) is the value/expectation from the previous trial\n\\(\\alpha\\) is the learning rate\n\\(PE_{t-1}\\) is the prediction error from the previous trial\n\\(R_{t-1}\\) is the reward received on the previous trial\n\nThe Rescorla-Wagner model can be alternatively expressed in plain language as:\n\\[\n\\text{\\scriptsize Expectations on the next trial} = \\text{\\scriptsize expectation on the current trial} + \\text{\\scriptsize learning rate} \\times \\text{\\scriptsize prediction error}\n\\]\nThe Rescorla-Wagner model demonstrates that learning isn’t about simply forming associations - it’s about reducing prediction errors. The larger the prediction error, the more we adjust our expectations.\n\n\n\n\n\n\nExpanding the Rescorla-Wagner model\n\n\n\nThe Rescorla-Wagner model still proves remarkably versatile in understanding human behaviour, but modern applications have expanded on this foundation. For instance, more complex models might add separate learning rates for positive and negative outcomes (\\(\\alpha^+\\) and \\(\\alpha^-\\)), or additional parameters for the unchosen option. We will practially implement the latter (Counterfactual RL) in Workshop 8.\n\n\n\nUnderstanding the learning rate\nThe learning rate \\((α)\\) is a crucial parameter that determines how much we update our expectations based on new information. It ranges from 0 to 1 and acts like a filter on how much of the prediction error influences our value update.\nYou can see the influence of the learning rate on trial-by-trial value and value-updating in the graph below 11:\n\n\n\n\n\n\n\n\n\nThe trial-by-trial effect of different learning rates on value updating\n\nPlotted for simulated data from a 75:25 reward contingency, the graph demonstrates that:\n\nHigher learning rates \\((α = 0.9)\\) show sharp, rapid changes in value\nLower learning rates \\((α = 0.3)\\) show more gradual, smoother changes\nWhen feedback switches, higher learning rates lead to more dramatic shifts in value\n\nWe can mathematically determine how past outcomes are re-weighted under different learning rates by simply re-writing the Rescorla-Wagner equation as a function of the initial value and the outcome per trial:\nFrom it’s original form:\n\\[V_t = V_{t-1} + \\alpha * PE_{t-1}\\]\n\\[PE_{t-1} = R_{t-1} - V_{t-1}\\]\nto:\n\\[V_t = (1-\\alpha)V_{t-1} + \\alpha R_{t-1}\\]\nwhich when expanded further gives:\n\\[V_t = (1-\\alpha)^t V_0 + \\sum_{i=1}^{t-i}(1-\\alpha)^{i}\\alpha R_i\\]\nThis formula above describes how much outcomes in the past contribute to the current value computation. To understand what this means more clearly, we can plot the weighting of previous trials for different values of \\(\\alpha\\):\n\n\n\n\n\n\n\n\n\nThe effect of different learning rates on the weights of past outcomes\n\nCollectively this graph demonstrates that:\n\nRecent outcomes have stronger weights than distant ones\nHigher learning rates \\((\\alpha = 0.9)\\) create a steeper recency gradient\nLower learning rates \\((\\alpha = 0.3)\\) create a more uniform weighting of past outcomes\nThe weight of any particular outcome decays exponentially with time, controlled by \\((1-\\alpha)\\)\n\nThis explains why higher learning rates lead to more volatile value estimates - they place much more weight on recent outcomes compared to historical ones.\nBut what is the ideal learning rate? Is there an ideal learning rate? The ideal learning rate ultimately depends on the environment, including the reward probability and volatility:\n\n\n\n\n\n\n\n\nWhilst a higher learning rate would be more appropriate for the shorter 40-trial condition with the 85:15 reward schedule, as there’s a clear difference between options and limited time to learn, in contrast, the 100-trial condition with the more subtle 75:25 reward difference might benefit from a lower learning rate to avoid being overly influenced by random reward variations. Similarly, in a reversal learning task where the reward contingencies switch, if the reversals are sparse, lower learning rates are optimal, whereas in environments with frequent reversals, higher learning rates become necessary to quickly adapt to new contingencies.\n\n\n\n\n\n\n\n\nHowever, there are optimal learning rates for specific task environments. The graph above shows choice accuracy as a function of learning rate, demonstrating that for a non-reversing contingency of 80:20, a learning rate of around 0.18 yields the highest choice accuracy (about 85%), with performance declining for both higher and lower rates. This reflects the trade-off between stability and adaptability in learning: too low a learning rate means slow adaptation, while too high a rate leads to over-sensitivity to noise.\n\n\n\n\n\n\nRunning simulations\n\n\n\nThe graphs above were generated by running simulations, and not using actual choice data. By running simulations on different trial lengths, we can determine the ideal task environment to elicit more/less learning."
  },
  {
    "objectID": "workshops/06.reinforcement_learning/qmd/rl_concepts.html#choice-rules",
    "href": "workshops/06.reinforcement_learning/qmd/rl_concepts.html#choice-rules",
    "title": "Fundamentals of cognitive modeling and reinforcement learning",
    "section": "Choice rules",
    "text": "Choice rules\nIn our framework earlier, we discussed two components of our computational model: the cognitive model and the observation model. Now that we have our cognitive model in the Rescorla-Wagner choice rule, we need a way to translate these internal values into observable choices. This is where choice rules come in - they serve as the “observation model” that connects our internal value estimates to actual decisions.\nChoice rules are mathematical functions that specify how internal value differences are mapped onto choice probabilities.\nLet’s examine three common choice rules:\n\nGreedy\n\n\n\n\n\n\nThe ‘greedy’ choice rule plotted using choice probability as a function of value difference\n\nThe simplest choice rule is the “greedy” approach:\n\\[p(C = a) = \\begin{cases}\n1, & \\text{if } V(a) &gt; V(b) \\\\\n0, & \\text{if } V(a) &lt; V(b)\n\\end{cases}\\]\nAs shown in the graph, this creates a step function where one always chooses the option with the higher value. However, this rule is unrealistic - humans rarely behave this consistently.\n\n\n\\(ε\\)-Greedy\n\n\n\n\n\n\nThe ‘\\(ε\\)-greedy’ choice rule plotted using choice probability as a function of value difference\n\nA slightly more sophisticated version is the “ε-Greedy” approach which adds random exploration:\n\\[p(C = a) = \\begin{cases}\n1-\\varepsilon, & \\text{if } V(a) &gt; V(b) \\\\\n\\varepsilon, & \\text{if } V(a) &lt; V(b)\n\\end{cases}\\]\nIn this rule, one usually chooses the higher-valued option (with probability \\(1-ε\\)) but will sometimes explores the lower-valued option (with probability \\(ε\\)). Whilst more realistic than the greedy approach, this still creates an unrealistic sharp transition in choice probabilities.\n\n\nSoftmax\n\n\n\n\n\n\n\n\n\nThe ‘softmax’ choice rule plotted using choice probability as a function of value difference\n\nA commonly impored and more psychologically plausible choice rule is the softmax function:\n\\[p_t(A) = \\frac{1}{1 + e^{-\\tau(V_t(A)-V_t(B))}}\\]\nThe softmax rule is particularly important because it creates a smooth, sigmoid relationship between value differences and choice probabilities. It does this by converting discrete choices into continuous probabilities, which fall between 0 and 1 and vary smoothly with value differences.\nNotably, the inverse temperature parameter \\(τ\\) (also called the ‘choice consistency parameter’) controls how deterministic choices are within the softmax equation:\n\nHigher values of \\(τ\\) (e.g., \\(τ = 5\\)) leads to more deterministic choices, which is closer to greedy\nLower values of \\(τ\\) (e.g., \\(τ = 0.3\\)) leads to more random choices\nWhen \\(τ = 1,\\) small value differences lead to proportionally small differences in choice probabilities\n\nWhilst the softmax rule demonstrates increased choice consistency as value differences increase, it still allows for some randomness in choice even with large value differences. Ultimately, the softmax function has become the standard choice rule in cognitive modeling over greedy rules, because it captures key aspects of human decision-making: we generally choose better options more often, but our choices remain probabilistic rather than deterministic."
  },
  {
    "objectID": "workshops/06.reinforcement_learning/qmd/rl_concepts.html#footnotes",
    "href": "workshops/06.reinforcement_learning/qmd/rl_concepts.html#footnotes",
    "title": "Fundamentals of cognitive modeling and reinforcement learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGauld, C., Dumas, G., Fakra, E., Mattout, J., & Micoulaud-Franchi, J. A. (2021, January). The three cultures of computational psychiatry.↩︎\nLewin, K. (1936). Principles of topological psychology. McGraw-Hill.↩︎\nFarrell, S., & Lewandowsky, S. (2018). Computational modeling of cognition and behavior. Cambridge University Press.↩︎\nStephan, K. E., Manjaly, Z. M., Mathys, C. D., Weber, L. A., Paliwal, S., Gard, T., … & Petzschner, F. H. (2016). Allostatic self-efficacy: A metacognitive theory of dyshomeostasis-induced fatigue and depression. Frontiers in human neuroscience, 10, 550.↩︎\nAhn, W. Y., Haines, N., & Zhang, L. (2017). Revealing neurocomputational mechanisms of reinforcement learning and decision-making with the hBayesDM package. Computational Psychiatry (Cambridge, Mass.), 1, 24.↩︎\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.↩︎\nPalminteri, S., Wyart, V., & Koechlin, E. (2017). The importance of falsification in computational cognitive modeling. Trends in cognitive sciences, 21(6), 425-433.↩︎\nJain, R. (2023). RL: A gentle introduction. Medium.↩︎\nRangel, A., Camerer, C., & Montague, P. R. (2008). A framework for studying the neurobiology of value-based decision making. Nature reviews neuroscience, 9(7), 545-556.↩︎\nRescorla, R. A., Wagner, A.W. (1972). A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement. Classical conditioning II: Current theory and research/Appleton-Century-Crofts.↩︎\nZhang, L., Lengersdorff, L., Mikus, N., Gläscher, J., & Lamm, C. (2020). Using reinforcement learning models in social neuroscience: frameworks, pitfalls and suggestions of best practices. Social Cognitive and Affective Neuroscience, 15(6), 695-707.↩︎"
  },
  {
    "objectID": "workshops/06.reinforcement_learning/qmd/rl_stan.html",
    "href": "workshops/06.reinforcement_learning/qmd/rl_stan.html",
    "title": "Implementing the Rescorla-Wagner model in Stan",
    "section": "",
    "text": "We now have the underlying structure of our computational model, consisting of the Rescorla-Wagner model and the softmax choice rule:\nLet’s now implement this practically in Stan. Again, we we have done in previous cases, let’s first visually depict our model in the graphical format:\nWithin our model we have the following:\nParameters (white circles):\n\\(α\\) (learning rate): A continuous parameter bounded between 0 and 1\n\\(τ\\) (temperature): A continuous parameter bounded between 0 and \\(x\\), where \\(x &lt; \\infty\\) (in this example 3)\nObserved variables (grey squares):\nDeterministic variables (double circles):"
  },
  {
    "objectID": "workshops/06.reinforcement_learning/qmd/rl_stan.html#constructing-the-rescorla-wagner-model-for-a-single-subject",
    "href": "workshops/06.reinforcement_learning/qmd/rl_stan.html#constructing-the-rescorla-wagner-model-for-a-single-subject",
    "title": "Implementing the Rescorla-Wagner model in Stan",
    "section": "Constructing the Rescorla-Wagner model for a single subject",
    "text": "Constructing the Rescorla-Wagner model for a single subject\nLet’s firstly fit this model to a single participant. The data (_data/rl_sp_ss.RData) is structured as follows:\n&gt; head(rl_ss)\n     [,1] [,2]\n[1,]    2   -1\n[2,]    1    1\n[3,]    1    1\n[4,]    1    1\n...\n[97,]    1   -1\n[98,]    2   -1\n[99,]    1    1\n[100,]   1    1\nWe just have data for 100 trials consisting of two unnamed columns corresponding to choice (1 or 2) and reward (-1 or +1). Recall that the behavioural task in this instance does not feature reversals.\nA simple implementation of a RL model - consisting the RW update and softmax choice rule - for this single participant is located within the following script (_scripts/my_1st_rw.stan).\nLet’s break it down block-by-block:\nIn the data block we declare what data we’ll feed into our model:\ndata {\n    int&lt;lower=1&gt; nTrials;\n    int&lt;lower=1,upper=2&gt; choice[nTrials];\n    int&lt;lower=-1,upper=1&gt; reward[nTrials];\n} \n\nnTrials: An integer declaring the number of trials. We bound it to be at least 1 since we need data to fit the model.\nchoice: An array of integers with length nTrials. Each element must be either 1 or 2, representing which option was chosen.\nreward: An array of integers with length nTrials. Each element must be either -1 or 1, representing losses and wins respectively.\n\nThen in the parameters block we declare the parameters we want to estimate:\nparameters {\n    real&lt;lower=0,upper=1&gt; alpha; // learning rate\n    real&lt;lower=0,upper=20&gt;tau; // softmax inv.temp.\n}\n\nalpha: The learning rate, bounded between 0 and 1\ntau: The inverse temperature parameter, bounded between 0 and 20\n\nFinally, the model block implements the actual Rescorla-Wagner model:\nmodel {\n    real pe;\n    vector[2] v;\n    vector[2] p;\n    \n    for (t in 1:nTrials) {\n        p = softmax( tau * v); // action probability computed via softmax\n        choice[t] ~ categorical(p);\n        \n        pe = reward[t] - v[choice[t]]; // compute pe for chosen value only\n        v[choice[t]] = v[choice[t]] + alpha * pe; // update chosen V\n    }\n    \n}\nFirst, we declare the local variables:\n\npe: A scalar to store the prediction error\nv: A vector of length 2 to store value estimates for both options\np: A vector of length 2 to store choice probabilities\n\nThe main loop then iterates through trials and implements our RL model:\n\np = softmax(tau * v): Converts values to probabilities using the softmax function, multiplying by tau to implement the temperature scaling\nchoice[t] ~ categorical(p): Tells Stan that choices are distributed according to our computed probabilities\npe = reward[t] - v[choice[t]]: Computes the prediction error for the chosen option\nv[choice[t]] = v[choice[t]] + alpha * pe: Updates the value estimate using the Rescorla-Wagner equation\n\n\n\n\n\n\n\nLayering complexity\n\n\n\nMost RL models build upon the RW equation, but retain it at their core. Take for example the case where we aim to incorporate separate learning rates, for negative and positive outcomes. In our model, we would make the following changes:\n\nRename the existing learning rate parameter to alpha_pos and introduce a new learning rate for negative outcomes alpha_neg.\n\nparameters {\n    real&lt;lower=0,upper=1&gt; alpha_pos; // rename existing learning rate\n    real&lt;lower=0,upper=1&gt; alpha_neg; // add negative-trial learning rate\n    real&lt;lower=0,upper=20&gt;tau;\n}\n\nAnd then in the model block, create a for loop to calculate v[choice[t]] with the appropriate learning rate depending on the outcome:\n\nmodel {\n    real pe;\n    vector[2] v;\n    vector[2] p;\n    \n    for (t in 1:nTrials) {\n        p = softmax(tau * v);\n        choice[t] ~ categorical(p);\n        \n        pe = reward[t] - v[choice[t]];\n        \n        if (reward[t] &gt; 0)\n            v[choice[t]] = v[choice[t]] + alpha_pos * pe; // update with positive learning rate\n        else \n            v[choice[t]] = v[choice[t]] + alpha_neg * pe; // update with negative learning rate\n    }\n}\nWe will demonstrate this by developing more complex models in future workshops.\n\n\nSo now that we have our Stan model completed, we can run it in R.\nThe script to do so is (_scripts/reinforcement_learning_single_parm_main.R).\nWithin this script, we need to firstly load the data and perform some pre-processing (renaming columns) before saving it as a data list:\n# load the data and assign\nload('_data/rl_sp_ss.RData')\nsz &lt;- dim(rl_ss)\nnTrials &lt;- sz[1]\n        \ndataList &lt;- list(nTrials=nTrials, \n                 choice=rl_ss[,1], \n                 reward=rl_ss[,2])\nLooking at the data now, it makes more sense than before:\n&gt; str(dataList)\nList of 3\n $ nTrials: int 100\n $ choice : num [1:100] 2 1 1 1 2 1 1 1 1 1 ...\n $ reward : num [1:100] -1 1 1 1 -1 1 1 -1 -1 1 ...\nWe just need to declare which model we want to use. For now, we want to run the model for a single subject:\nmodelFile &lt;- '_scripts/reinforcement_learning_sp_ss_model.stan`\nwith the rest of the script setting the various components of our sampling approach as encountered in earlier workshops.\nThe single-subject Stan model that we are using in the script (_scripts/reinforcement_learning_sp_ss_model.stan) is slightly different to the one explained above. In this version we:\nProvide initial choice values of 0 in a transformed data block:\ntransformed data {\n  vector[2] initV;  // initial values for V\n  initV = rep_vector(0.0, 2);\n}\nKeep track of values at every trial:\nmodel {\n  vector[2] v[nTrials+1];   // Creates an array of nTrials+1 vectors, each vector has 2 elements\n  ...\n  v[1] = initV;             // Sets initial values\n  for (t in 1:nTrials) {    \n    ...\n    v[t+1] = v[t];         // Copies current trial's values to next trial\n    v[t+1, choice[t]] = v[t, choice[t]] + lr * pe[t];  // Updates chosen value for next trial\n  }\n}\nThere are smaller changes (i.e., the categorical_logit), but these are the main two of note.\n\n\n\n\n\n\nRunning the model\n\n\n\nThe R script is enveloped in a wrapper function, which runs the model either for an individual or for multiple subjects. You can simply run it for individuals as it is (i.e., with multiSubj = FALSE), or simply by highlighting and running the three components for the individual subject model.\nFor example, to load and assign the data to a data list, just highlight and run the following code inside the if-else statement:\nload('_data/rl_sp_ss.RData')\nsz &lt;- dim(rl_ss)\nnTrials &lt;- sz[1]\n        \ndataList &lt;- list(nTrials=nTrials, \n                  choice=rl_ss[,1], \n                  reward=rl_ss[,2])\n\n\n\n\n\n\n\n\nExercise 8\n\n\n\n1. Run the R script specifically for the model reinforcement_learning_sp_ss_model.stan. Examine the traceplot and posterior density plot for the learning rate and inverse temperature.\n\n\nAfter running the model, you should get the following posterior density distribution for the learning model and inverse temperature parameters:\n\n\n\n\n\n\nPosterior density distributions for the learning rate and inverse temperature parameters (single subject)"
  },
  {
    "objectID": "workshops/06.reinforcement_learning/qmd/rl_stan.html#fitting-multiple-subjects",
    "href": "workshops/06.reinforcement_learning/qmd/rl_stan.html#fitting-multiple-subjects",
    "title": "Implementing the Rescorla-Wagner model in Stan",
    "section": "Fitting multiple subjects",
    "text": "Fitting multiple subjects\nThe model above estimates the latent parameters for a single individual. Of course, most studies involve more than one person, and so we would like to estimate parameters for a group of individuals rather than a single subject.\nThere are several ways to do this.\n\n\n\n\n\n\n\n\n\nTwo methods for implementing the Rescorla-Wagner model across multiple subjects\n\nFirstly, you could implement a subject-loop depicted on the left diagram above. However, this approach assumes a single learning rate and single inverse temperature for all participants. It essentially forces the model to find “average” parameters that might not represent any actual subject well. This is equivalent to modeling a hypothetical same-person completing the task \\(n\\) times.\nOn-the-other-hand, you could also fit multiple participants independently (right side), which assumes that each person has their own learning rate and inverse temperature. This is clearly a much better approach as it takes into account individual differences in cognition.\n\n\n\n\n\n\nExercise 9\n\n\n\n1. Examine the Stan code and run both models using the provided scripts and data. Examine the posterior density distribution and MCMC trace plots for each.\nUse the following scripts:\nStan models:\n\nSubject-loop: _scripts/reinforcement_learning_sp_ms_model.stan\nIndividual fitting: _scripts/reinforcement_learning_mp_indv_model.stan\n\nR scripts:\n\nSubject-loop: _scripts/reinforcement_learning_single_parm_main.R (remember to load in rl_sp_ms.RData)\nIndividual fitting: _scripts/reinforcement_learning_multi_parm_main.R\n\n2. How does the posterior density plots differ between running the model for a single participant and for multiple subjects using a subject-loop?\n\n\n\n\n\n\nClick to reveal the density plots\n\n\n\n\n\n\n\n\n\n\nYou can see the HDI is narrower for both parameters when observing the results from 10 subjects versus a single subject.\n\n\n\n3. How does the posterior density plots differ between running the model for multiple subjects using a subject-loop and individually fitting parameters?\n\n\n\n\n\n\nClick to reveal the density plots\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere doesn’t look like much of a difference, but we will examine this further in the next workshop."
  },
  {
    "objectID": "workshops/06.reinforcement_learning/qmd/rl_stan.html#footnotes",
    "href": "workshops/06.reinforcement_learning/qmd/rl_stan.html#footnotes",
    "title": "Implementing the Rescorla-Wagner model in Stan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nZhang, L., Lengersdorff, L., Mikus, N., Gläscher, J., & Lamm, C. (2020). Using reinforcement learning models in social neuroscience: frameworks, pitfalls and suggestions of best practices. Social Cognitive and Affective Neuroscience, 15(6), 695-707.↩︎"
  },
  {
    "objectID": "workshops/07.optm_rl/qmd/optim.html",
    "href": "workshops/07.optm_rl/qmd/optim.html",
    "title": "Optimizing Stan models",
    "section": "",
    "text": "Working directory for this workshop\n\n\n\nModel code and R scripts for this workshop are located in the (/workshops/07.optm_rl) directory.\nRunning the hierarchical model will generate the posterior density plots below:\nRemember that these analyses are ran on simulated data generated from a set of learning rates and inverse temperatures. In other words, we know the ground truth for these parameter values. We can therefore compare the results from both the individually fitting model and the hierarchical model to the ground truth:\nThe analyses which were ran above should generate the latter two; the individual-subject plot is taken from an earlier workshop.\nNote that the variance is reduced for both parameters when comparing the individually fitted results to the hierarchical structure. However, this variance is lower than the observed variance in the ground-truth results. This is because the code running the hierarchical model is not optimized.\nFor example, when running the hierarchical model, immediately after after the sampling has completed, you should see the following message in the R console:\nThis reflects the issues we highlighted earlier with putting bounds on the normal distribution; the sampler is trying to go beyond a closed door."
  },
  {
    "objectID": "workshops/07.optm_rl/qmd/optim.html#optimization-in-stan",
    "href": "workshops/07.optm_rl/qmd/optim.html#optimization-in-stan",
    "title": "Optimizing Stan models",
    "section": "Optimization in Stan",
    "text": "Optimization in Stan\nMore generally, this reflects the practice of optimizing your modeling framework. This can be done in four main ways: pre-processing, specification, vectorization, and re-parameterization. The first two should be implemented outside of Stan, whilst the second two specifically involve optimizing your Stan code.\n\n\n\n\n\n\n\n\n\nFour key strategies to optimize Stan models\n\n\nPre-processing\nStan is a programming language for model estimation. It does not perform the pre-processing; which should be done outside Stan where possible.\nTo demonstrate this, let’s look at a concrete example involving a quadratic regression:\n\\[height = \\alpha + \\beta_1 * weight + \\beta_2 * weight^2\\]\nWe can either calculate the squared term in R before passing to Stan:\nd$weight_sq &lt;- d$weight^2\nOr calculate the squared term inside Stan directly:\ntransformed data {\n  vector[N] weight_sq;\n  for (i in 1:N) {\n    weight_sq[i] = weight[i]^2;\n  }\n}\nWhile the latter would give the same results, it’s less efficient and makes the Stan code more complex than necessary. Ultimately, you should save Stan for the actual statistical modeling, and do as much data preparation as possible in your primary programming environment.\n\n\nModel specification\nThe goal of computational modeling is to uncover latent cognitive processes underlying our observed data. However, this goal can only be achieved if we specify a model that adequately captures the underlying processes that generated our data in the first place. A model that inherently fails to capture important aspects of these processes will lead to poor inferences and unreliable conclusions, no matter how sophisticated our statistical methods may be.\nModel mis-specification occurs when our statistical model fails to capture important aspects of the data generating process. For example, let’s say one is studying the relationship between study time and test scores.\nWe might assume this relationship is linear and write:\n\\[score = \\beta_0 + \\beta_1 * study_time + \\epsilon\\]\nHowever, in reality, the relationship might be non-linear - perhaps there are diminishing returns where studying beyond a certain point yields smaller improvements in scores. If we force a linear model on this non-linear relationship, our model is misspecified and could lead to poor predictions and incorrect inferences.\nThere are four main guidelines for proper model specification:\n1. Visualize your data: Plot your raw data before building any models, looking for patterns, relationships, and potential outliers\n2. Follow the literature: Build on existing theoretical frameworks in your field and review similar studies to understand common modeling approaches\n3. Start simple and build complexity: As we have done in this course, begin with the simplest reasonable model that could explain your data, and add complexity only when justified by theory or data. Subsequently test whether added complexity improves model fit thorugh model comparison\n4. Simulate data and run model recovery: Test your model by generating simulated data, and check if it can recover the true parameters used to generate the data\n\n\nVectorization\nAs mentioned in earlier workshops, Stan will automatically perform operations on entire vectors/arrays at once through vectorization. We have already seen some examples:\n\nBernoulli model\n\n// Non-vectorized version\nmodel {\n  for (n in 1:N) {\n    flip[n] ~ bernoulli(theta);\n  }\n}\n\n// Vectorized version\nmodel {\n  flip ~ bernoulli(theta);\n}\n\nLinear regression model\n\n// Non-vectorized version\nmodel {\n  vector[N] mu;\n  for (i in 1:N) {\n    mu[i] = alpha + beta * weight[i];\n    height[i] ~ normal(mu[i], sigma);\n  }\n}\n\n// Partially vectorized version\nmodel {\n  vector[N] mu;\n  mu = alpha + beta * weight;\n  height ~ normal(mu, sigma);\n}\n\n// Fully vectorized version\nmodel {\n  height ~ normal(alpha + beta * weight, sigma);\n}\n\nHierarchical Rescorla-Wagner model\n\nWhere we use:\nmodel {\n  lr ~ normal(lr_mu, lr_sd);     // Vectorized normal distribution for learning rates\n  tau ~ normal(tau_mu, tau_sd);  // Vectorized normal distribution for tau parameters\n}\nInstead of:\nmodel {\n  for (i in 1:nSubjects) {\n    lr[i] ~ normal(lr_mu, lr_sd);\n    tau[i] ~ normal(tau_mu, tau_sd);\n  }\n}\nThis does not necessarily mean that you should immediately program with vectorization in mind; it may be more useful to program with for loops. You can then optimize the code afterwards after running the model and examining various things (e.g., compiling time).\n\n\nReparameterization\nStan’s sampler can be slow in sampling from distributions with difficult posterior geometries. One way to speed up such models is through reparameterization1.\nIn some cases, reparameterization can dramatically increase the effective sample size for the same number of iterations or for non-converging chains. When working with hierarchical models, particularly those with limited data, a useful technique involves specifically transforming from centered to non-centered parameterizations2. This transformation helps by decoupling the prior relationships between hierarchical and lower-level parameters.\n\n\n\n\n\n\nThe Matt trick\n\n\n\nThis parameterization came to be known as the “Matt trick” after Matt Hoffman, who independently came up with it while fitting hierarchical models in Stan.\n\n\nNeal’s funnel distribution3 illustrates the sampling challenges of hierarchical models:\nIn this case, we take a distribution defined over \\(y \\in \\mathbb{R}\\) and \\(x \\in \\mathbb{R}^9\\) with the density:\n\\[p(y,x) = \\text{Normal}(y|0,3) \\times \\prod_{n=1}^9 \\text{Normal}(x_n|0,\\exp(y/2))\\]\nThis distribution creates ten-dimensional funnel-shaped probability contours. The funnel becomes particularly narrow due to the exponential transformation of \\(y\\). The plot below displays the log marginal density for \\(y\\) and the first dimension \\(x_1\\):\n\n\n\n\n\n\nNeal’s funnel demonstrating samples which do not explore the full parameter space\n\nThe visualization demonstrates how the variance of \\(x_1\\) dramatically shrinks as \\(y\\) becomes more negative, creating a challenging sampling space.\nThe funnel can be implemented directly in Stan as:\nparameters {\n  real y;\n  vector[9] x;\n}\nmodel {\n  y ~ normal(0, 3);\n  x ~ normal(0, exp(y/2));\n}\nA non-math explanation is that when the model is expressed this way, Stan has trouble sampling from the neck of the funnel, where \\(y\\) is small and thus \\(x\\) is constrained to be near 0. This is because the density’s scale changes with \\(y\\), so that a step size that works well in the body will be too large for the neck and a step size that works in the neck will be inefficient in the body.\nUltimately, hierarchical models where one parameter controls the variance of another (like Neal’s funnel) are inherently difficult to sample from because they create regions where the parameter space changes dramatically. This is why reparameterization is important - it transforms the problem into a form that’s easier to sample from.\nLet’s get our head around this by using the example below:\n\n\n\n\n\n\n\n\nWhen reparameterizing, we need a reference distribution where we reparameterize our complex distribution relative to this reference. In the example above, our original centered parameterization is:\n\\[\\theta \\sim \\text{Normal}(\\mu_\\theta, \\sigma_\\theta)\\]\nThis states that our parameter \\(θ\\) follows a normal distribution with mean \\(\\mu_\\theta\\) and standard deviation \\(\\sigma_\\theta\\).\nBut this is problematic as sampling becomes difficult when the parameters are highly correlated or when the variance changes dramatically across the parameter space. That’s why we reparameterize where the sampling is more straightforward, and then transform those samples to get back to our desired distribution.\nIn this example, we reparameterize it to the standard normal distribution (a popular option in Stan).\nAt first we sample from our reference distribution:\n\\[\\tilde{\\theta} \\sim \\text{Normal}(0, 1)\\]\nThen transform to the target distribution:\n\\[\\theta = \\mu_\\theta + \\sigma_\\theta\\tilde{\\theta}\\]\nThe diagram illustrates this transformation:\n\nThe blue curve shows the standard normal distribution \\((Normal(0,1))\\) that we sample \\(\\tilde{\\theta}\\) from\nThe red curve shows the resulting transformed distribution for \\(\\theta\\)\n\nMathematically, this works as multiplying by \\(\\sigma_\\theta\\) changes the width of the distribution, whilst adding \\(\\mu_\\theta\\) shifts the distribution (positive values shift right, negative values shift left).\nImportantly, we’re not changing the model - it remains mathematically equivalent. What we’re doing is improving the sampling efficiency by expressing our complex distribution in terms of a simpler reference distribution that’s easier for Stan to work with.\nThis is the reparameterization for Neal’s funnel represented in Stan:\nparameters {\n  real y_raw;\n  vector[9] x_raw;\n}\n\ntransformed parameters {\n  real y;\n  vector[9] x;\n\n  y = 3.0 * y_raw;\n  x = exp(y/2) * x_raw;\n}\nmodel {\n  y_raw ~ normal(0,1); \n  x_raw ~ normal(0,1); \n}\nIn this second model, the parameters block defines x_raw and y_raw which are sampled as independent standard normals, and is easy for Stan to do. These are then transformed into samples from the funnel in the transformed parameters block. The model then tells Stan to sample both raw parameters from standard normal distributions - the simplest kind of normal distribution."
  },
  {
    "objectID": "workshops/07.optm_rl/qmd/optim.html#stans-sampling-parameters",
    "href": "workshops/07.optm_rl/qmd/optim.html#stans-sampling-parameters",
    "title": "Optimizing Stan models",
    "section": "Stan’s sampling parameters",
    "text": "Stan’s sampling parameters\nWhen optimizing our Stan code, we also need to consider how to set Stan’s sampling parameters to ensure efficient sampling from our model. Even with a well-parameterized model, the sampling process itself needs to be properly configured to explore the parameter space effectively.\nStan provides several key sampling parameters that can be adjusted to improve MCMC performance:\n\n\n\n\n\n\n\n\n\nStan’s sampling parameters, with default values\n\niterations: (Default: 2000)\n\nDetermines how many MCMC samples to draw per chain. Whilst more iterations allow better exploration of the parameter space and helps achieve better convergence it also increases computational time\n\ndelta (\\(δ\\)): (Default: 0.80)\n\nControls the target acceptance rate for the sampler. Higher values (closer to 1) make the sampler more conservative, with 0.99(9) being common values\n\nstepsize (\\(ε\\)): (Default: 2.0)\n\nSets the initial step size. Smaller steps (like 1.0) allow for more careful exploration\n\nmax_treedepth (\\(L\\)): (Default: 10)\n\nControls the maximum number of steps the sampler can take in each iteration. Higher values allow the sampler to explore more distant regions\n\nTypical adjustments may be to increase iterations, delta and max_treedepth, and decrease stepsize.\n\n\n\n\n\n\nExercise 11\n\n\n\nLet’s now apply our understanding on optimization in Stan practically.\nNavigate inside the _scripts directory, where you will see a number of files:\n\nfunnel.stan and funnel_reparam.stan are the normal and reparameterized Stan models for Neal’s funnel.\nfunnel_example.R is the R script to run these Stan models.\n\nSpecifically, in funnel_example.R there are three analyses to run: the original model with default Stan sampling parameters, the original but setting adapt_delta = 0.999 and max_treedepth=20, and the re-parameterized model.\n1. Run the funnel_example.R script and analyse the output. How does the performance differ between the three analyses?\nTIP: Print the stanfit object and examine the traceplots.\n\n\nAn example summary of the output is provided below. Note that this is a direct comparison for the first compilation on a specific machine, which will vary.\n\n\n\n\n\n\n\n\n\nSampling statistics for the direct, adjusted direct and reparameterized models\n\nOverall, the direct model performs poorly, with low effective samples, high divergence, and poor mixing as shown by its \\(Rhat\\) of 1.22 (1.0 is the target value).\nWhile adjusting the sampling parameters improves performance somewhat - eliminating divergences and increasing efficiency to 0.82 samples per second - it still struggles with relatively poor mixing (\\(Rhat\\) = 1.1).\nIn contrast, the reparameterized model demonstrates remarkable improvement across almost all metrics. Despite a similar runtime to the other approaches, it achieves perfect mixing \\((Rhat = 1.0)\\), generates no divergent transitions, and produces over 200 times more effective samples per second (77.53) compared to the direct model (0.37). This improvement in sampling efficiency is visible in the trace plots, where the reparameterized model shows consistent exploration of the parameter space compared to the direct approaches.\nIn this case, we have specifically reparameterized an unbounded parameter. However, when dealing with bounded parameters - i.e., if we have a probability parameter that must be between 0 and 1 - we can’t use the simple linear transformation we used before, as it could generate values outside these bounds.\nTo handle this, we can use the probit (inverse cumulative normal) transformation. This takes our unbounded normal distribution and transforms it to the \\([0,1]\\) interval. The probit transformation is particularly useful because it maintains a smooth mapping and preserves the normal distribution’s properties while respecting the bounds.\nBelow is a summary of how different parameter constraints map to their reparameterizations:\n\n\n\n\n\n\n\nConstraint\nReparameterization\n\n\n\n\n\\(\\theta \\in (-\\infty, +\\infty)\\)\n\\(\\theta = \\mu_\\theta + \\sigma_\\theta\\tilde{\\theta}\\)\n\n\n\\(\\theta \\in [0, N]\\)\n\\(\\theta = \\text{Probit}^{-1}(\\mu_\\theta + \\sigma_\\theta\\tilde{\\theta}) \\times N\\)\n\n\n\\(\\theta \\in [M, N]\\)\n\\(\\theta = \\text{Probit}^{-1}(\\mu_\\theta + \\sigma_\\theta\\tilde{\\theta}) \\times (N-M) + M\\)\n\n\n\\(\\theta \\in (0, +\\infty)\\)\n\\(\\theta = \\exp(\\mu_\\theta + \\sigma_\\theta\\tilde{\\theta})\\)\n\n\n\nTo summarise:\n\nThe unconstrained case uses a simple linear transformation\nFor bounded intervals \\([0,N]\\), we scale the probit transformation\nFor bounded intervals \\([M,N]\\), we scale and shift the probit transformation\nFor positive parameters, we use an exponential transformation\n\nIn all cases, we start with \\(\\tilde{\\theta} \\sim \\text{Normal}(0,1)\\) as our reference distribution and transform it appropriately given the parameter constraints."
  },
  {
    "objectID": "workshops/07.optm_rl/qmd/optim.html#reparameterizing-the-hierarchical-rescorla-wagner-model",
    "href": "workshops/07.optm_rl/qmd/optim.html#reparameterizing-the-hierarchical-rescorla-wagner-model",
    "title": "Optimizing Stan models",
    "section": "Reparameterizing the hierarchical Rescorla-Wagner model",
    "text": "Reparameterizing the hierarchical Rescorla-Wagner model\nLet’s now apply what we have learned about reparameterization and optimization to our hierarchical RL model. The key idea is to transform our bounded parameters (learning rates and inverse temperatures) to work with unbounded parameters that are easier for Stan to sample from.\nIn our original model, we directly declared our group-level and subject-level parameters with their constraints:\nparameters {\n  real&lt;lower=0,upper=1&gt; lr_mu;            // Group mean learning rate\n  real&lt;lower=0,upper=3&gt; tau_mu;           // Group mean inverse temperature\n  real&lt;lower=0&gt; lr_sd;                    // Group SD learning rate\n  real&lt;lower=0&gt; tau_sd;                   // Group SD inverse temperature\n  real&lt;lower=0,upper=1&gt; lr[nSubjects];    // Individual learning rates\n  real&lt;lower=0,upper=3&gt; tau[nSubjects];   // Individual inverse temperatures\n}\nIn our reparameterized version, we instead work with unbounded raw parameters:\nparameters {\n  // Group-level raw parameters\n  real lr_mu_raw;                  // Unbounded group mean learning rate\n  real tau_mu_raw;                 // Unbounded group mean inverse temperature\n  real&lt;lower=0&gt; lr_sd_raw;         // Group SD learning rate (still positive)\n  real&lt;lower=0&gt; tau_sd_raw;        // Group SD inverse temperature (still positive)\n  \n  // Subject-level raw parameters\n  vector[nSubjects] lr_raw;        // Unbounded individual learning rates\n  vector[nSubjects] tau_raw;       // Unbounded individual inverse temperatures\n}\nWe then add a transformed parameters block that converts these unbounded raw parameters into our actual bounded parameters. This is where the probit transformation (Phi_approx) transforms our unbounded parameters to the \\([0,1]\\) interval. For the inverse temperature, we simply multiply by 3 to get the \\([0,3]\\) range we defined as our prior earlier.\ntransformed parameters {\n  vector&lt;lower=0,upper=1&gt;[nSubjects] lr;\n  vector&lt;lower=0,upper=3&gt;[nSubjects] tau;\n  \n  for (s in 1:nSubjects) {\n    // Transform learning rates to [0,1] range\n    lr[s] = Phi_approx(lr_mu_raw + lr_sd_raw * lr_raw[s]);\n    \n    // Transform inverse temperatures to [0,3] range\n    tau[s] = Phi_approx(tau_mu_raw + tau_sd_raw * tau_raw[s]) * 3;\n  }\n}\nIn the model block, we now specify standard normal distributions for our raw parameters:\nmodel {\n  // Priors for group-level raw parameters\n  lr_mu_raw ~ normal(0,1);\n  tau_mu_raw ~ normal(0,1);\n  lr_sd_raw ~ cauchy(0,3);\n  tau_sd_raw ~ cauchy(0,3);\n  \n  // Priors for individual-level raw parameters\n  lr_raw ~ normal(0,1);\n  tau_raw ~ normal(0,1);\n  \n  // Rest of the model remains the same...\n}\nAnd finally, we add a generated quantities block to transform our raw group-level means back to their original scales.\ngenerated quantities {\n  real&lt;lower=0,upper=1&gt; lr_mu;\n  real&lt;lower=0,upper=3&gt; tau_mu;\n  \n  lr_mu = Phi_approx(lr_mu_raw);\n  tau_mu = Phi_approx(tau_mu_raw) * 3;\n}\n\n\n\n\n\n\nExercise 12\n\n\n\n1. Complete the “Matt trick” at line 29 in the reparameterized hierarchical RL Stan model reinforcement_learning_mp_hrch_optm_model.stan.\n\n\n\n\n\n\nClick to reveal the solution\n\n\n\n\n\nThis is the “Matt trick” section which samples individual subject parameters from standard normal distributions, then transforms them using the group means and standard deviations.\nfor (s in 1:nSubjects) {\n    lr[s]  = Phi_approx( lr_mu_raw  + lr_sd_raw * lr_raw[s] );\n    tau[s] = Phi_approx( tau_mu_raw + tau_sd_raw * tau_raw[s] ) * 3;\n  }\n\n\n\n2. Open the R script reinforcement_learning_hrch_main.R. Run the analyses for both the non-optimized and optimized (i.e., reparameterized) hierarchical models, and examine the posterior density plots for both. How do they differ?\nNon-optimized model: reinforcement_learning_mp_hrch_model.stan\nOptimized model: reinforcement_learning_mp_hrch_optm_model_ppc.stan\nTIP: To run each model separately, you can simply highlight the relevant sections of code. For example, run the line below to select the optimized model…\nmodelFile &lt;- '_scripts/reinforcement_learning_mp_hrch_optm_model_ppc.stan'\nThe model reinforcement_learning_mp_hrch_optm_model.stan will not run without the Matt trick completed!\n\n\nThe density plots generated show that the parameter space is more thoroughly sampled in the parameterized model, demonstrating its benefit:"
  },
  {
    "objectID": "workshops/07.optm_rl/qmd/optim.html#footnotes",
    "href": "workshops/07.optm_rl/qmd/optim.html#footnotes",
    "title": "Optimizing Stan models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStan Development Team. Reparameterization. In Stan User’s Guide (Version 2.18).↩︎\nPapaspiliopoulos, Omiros, Gareth O. Roberts, and Martin Sköld. 2007. “A General Framework for the Parametrization of Hierarchical Models.” Statistical Science 22 (1): 59–73.↩︎\nNeal, Radford M. 2003. “Slice Sampling.” Annals of Statistics 31 (3): 705–67.↩︎"
  },
  {
    "objectID": "workshops/08.compare_models/qmd/intro.html",
    "href": "workshops/08.compare_models/qmd/intro.html",
    "title": "Model comparison",
    "section": "",
    "text": "Welcome to the eighth workshop of the BayesCog course!\nIn our previous workshop, we explored how to optimize Stan models through reparameterization and by changing the values of Stan’s sampling parameters. We learned that while all models are approximations, we can make them more efficient and reliable through proper optimization techniques. We then ran different variants of the hierarchical RL model, and could observe that - in terms of sampling - the optimized version performed better than the un-optimized version.\nIndeed, when developing cognitive models more generally, we will need to objectively compare them to see which is a more suitable candidate. But how do we choose between different candidate models?\nIn this workshop, we’ll explore various approaches to model comparison, with a particular focus on the Widely Applicable Information Criterion (WAIC). We will also examine our models by performing posterior predictive checks, an important tool for model validation. We’ll see how these tools can help us make more informed decisions about selecting which models best describe the latent cognitive processes we are ultimately interested in.\nThe goals of this workshop are to:\n\nUnderstand the fundamental trade-off between model complexity and goodness of fit\nLearn about different information criteria and their theoretical foundations\nImplement model comparison techniques through WAIC using the loo package\nUse posterior predictive checks to validate model performance visually\n\n\n\n\n\n\n\nWorking directory for this workshop\n\n\n\nModel code and R scripts for this workshop are located in the (/workshops/08.compare_models) directory. Remember to use the R.proj file within each folder to avoid manually setting directories!\n\n\nThe copy of this workshop notes can be found on the course GitHub page."
  },
  {
    "objectID": "workshops/08.compare_models/qmd/model_comparison.html",
    "href": "workshops/08.compare_models/qmd/model_comparison.html",
    "title": "Model comparison",
    "section": "",
    "text": "Let’s think back to the quote by George Box mentioned earlier in the course:\nThis is particularly relevant in cognitive modeling, where no model can perfectly capture human cognition. However, some models provide more useful approximations than others.\nConsider our reinforcement learning example: we might have several competing hypotheses about how people learn from rewards. Perhaps some participants use a simple learning strategy with a single learning rate, while others might use different learning rates for positive and negative outcomes. Or maybe some individuals incorporate uncertainty into their learning process. Each of these hypotheses can be formalized as a different computational model, but which one best describes our participants’ behaviour?\nModel comparison provides us with systematic tools to answer this question. Rather than simply accepting the first model that seems to fit our data reasonably well, we can fit multiple models and compare which is best. In relation to Box’s quote, we want to see which model is the least wrong!"
  },
  {
    "objectID": "workshops/08.compare_models/qmd/model_comparison.html#model-fitting-and-the-goldilocks-principle",
    "href": "workshops/08.compare_models/qmd/model_comparison.html#model-fitting-and-the-goldilocks-principle",
    "title": "Model comparison",
    "section": "Model fitting and the Goldilocks principle",
    "text": "Model fitting and the Goldilocks principle\nHave a look at the graph below, which shows polynomials of different orders being fitted to some data, where \\(M\\) represents the order of the polynomial:\n\n\n\n\n\n\n\n\nLooking at the model fits, which order-polynomial is ‘best’?\nAt first glance, you might think the \\(M = 7\\) model is ‘best’ since it passes exactly through each data point. However, this intuition leads us to the concept of overfitting - where a model learns to fit the noise in the specific data provided rather than explaining the underlying pattern more generally.\nOverfitting can be humorously described by a John von Neumann quote recalled by Enrico Fermi:\n\n\n“With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.”\n\n\nElephants aside, his point was that with enough parameters, you can fit any pattern in your data - even if that pattern is just random noise.\nThis brings us to model-fitting, a fundamental concept in model selection illustrated in the figure below1:\n\n\n\n\n\n\n\n\n\nModel fitting is an example of the Goldilocks principle: it needs to be just right\n\nThe graph shows that as we increase model complexity, we initially see improvements in how well our model fits the data. However, there comes a point where adding more complexity leads to overfitting.\nThe figure shows two crucial curves:\n\nThe “Goodness of fit” curve shows how well the model fits the observed data, which continues to improve with complexity.\nThe “Generalizability” curve shows how well the model performs on new, unseen data. This curve peaks and then declines as the model becomes too complex.\n\nLooking back at our polynomial example, while \\(M = 7\\) undoubedtly gives us the best fit to our observed data points, it would likely perform poorly if we tried to use it to predict new data. The model has accurately but specifically learned the noise in our sample rather than the more general relationship we’re trying to understand.\nUltimately, we need to find a sweet spot - a balance between underfitting and overfitting.\nBut how do we determine which model is best? A more philosophical rule follows Ockham’s razor, which (in one variation) states that:\n\n\n“When having two competing theories that make exactly the same predictions, the simpler one is the better.” - original maxim attributed to William of Ockham (c. 1285 - 1347)\n\n\nSpecifically concerning model selection, we should therefore prefer simpler models when they provide similar explanatory power or make fewer assumptions; \\(M = 2\\) or \\(M = 4\\) might provide the best balance - capturing the main trend in the data without overfitting."
  },
  {
    "objectID": "workshops/08.compare_models/qmd/model_comparison.html#information-criteria",
    "href": "workshops/08.compare_models/qmd/model_comparison.html#information-criteria",
    "title": "Model comparison",
    "section": "Information criteria",
    "text": "Information criteria\nAs a result, we would like to know which model best fits the data but also generalizes to new unobserved data.\nBut how do we define and implement this objectively? One way is through cross-validation.\n\n\n\n\n\n\n\n\n\nCross-validation (\\(k\\)-fold) involves fitting a model to most of the data and testing it on the remainder \\(k\\) times\n\nThe basic principle is straightforward: we divide our dataset into two parts:\n1. A training set that we use to fit our model\n2. A validation set that we use to test how well our model predicts new, unseen data\nThe figure above shows a specific type: \\(k\\)-fold cross-validation, where the data is divided into \\(k\\) equal sections, and each section takes a turn being the validation set while the remaining sections form the training set.\n\n\n\n\n\n\nLeave-One-Out Cross Validation (LOOCV)\n\n\n\nLeave-One-Out Cross Validation (LOOCV) is a special case of \\(k\\)-fold cross-validation where \\(k\\) equals the number of data points in the dataset, meaning you leave out just one data point for validation while training on all others, repeating for every point in the dataset. While LOOCV provides nearly unbiased estimates of model performance, it can be computationally expensive for large datasets since you need to train the model \\(n\\) times (where \\(n\\) is your sample size), compared to \\(k\\)-fold which only requires training \\(k\\) times.\n\n\nCross-validation is particularly useful when we have a fixed amount of data and won’t be collecting more. Rather than using all our data to fit the model, we deliberately hold some back to simulate how well our model might perform on future observations. This helps us detect overfitting - if a model performs much better on the training data than the validation data, it’s likely overfitting.\nHowever, cross-validation comes with some practical limitations. We need to perform this process for each model we want to compare, which can be computationally intensive. For complex models or large datasets, this can make cross-validation quite time-consuming.\nIn the Bayesian context, there’s an interesting alternative approach. While nothing prevents us from using cross-validation with Bayesian models, holding out data makes our posterior distributions more diffuse, since we’re using less information to estimate our parameters. Instead, Bayesians typically condition on all available data and use “information criteria” to evaluate how well a model is expected to predict out of sample. While cross-validation directly tests this by holding out data, information criteria provide mathematical approximations to cross-validation that are computationally more efficient.\nInformation criteria are mathematical formulas that balance two components:\n\nHow well the model fits the observed data (goodness of fit)\nA penalty term for model complexity (to prevent overfitting)\n\nCommonly used information criteria are:\n\nAkaike Information Criterion (AIC)\n\n\n\n\nThe original information criterion, introduced by Hirotugu Akaike, is calculated as:\n\\[AIC = -2(\\log\\mathcal{L}) + 2k\\]\nwhere:\n\\(\\mathcal{L}\\) is the maximum likelihood , and \\(k\\) is the number of parameters in the model.\n\nDeviance Information Criterion (DIC)\n\n\n\n\nDIC was developed specifically for Bayesian models, extending AIC to handle hierarchical models:\n\\[DIC = -2(\\log\\mathcal{L}) + 2p_D\\]\nwhere:\n\\(p_D\\) is the effective number of parameters, accounting for how parameters might be constrained by the hierarchical structure.\n\nWidely Applicable Information Criterion (WAIC)\n\n\n\n\nWAIC is currently considered the most accurate approximation to leave-one-out (LOO) cross-validation. It computes the predictive accuracy for each data point and includes a correction for effective number of parameters:\n\\[WAIC = -2(lpd - p_{WAIC})\\]\nwhere:\n\\(lpd\\) is the log pointwise predictive density, and \\(p_{WAIC}\\) accounts for model complexity, determined by:\n\\[lpd = \\sum_{i=1}^n \\log \\left(\\frac{1}{S}\\sum_{s=1}^S p(y_i|\\theta^s)\\right)\\]\n\\[p_{WAIC} = \\sum_{i=1}^n V_{s=1}^S(\\log p(y_i|\\theta^s))\\]\nwhere:\n\n\\(S\\) is the number of posterior samples,\n\\(\\theta^s\\) are the parameter values in sample \\(s\\),\nand \\(V\\) represents the sample variance over the \\(S\\) samples.\n\nWe will be using the WAIC for model comparison in this course, as it provides the best approximation to leave-one-out cross-validation. Unlike AIC and DIC, WAIC uses the full posterior distribution rather than just point estimates, making it more appropriate for Bayesian models.\nWe can see this in the table below, which compares the accuracy of different information criteria to the leave-one-out cross-validation (LOO-CV) ground truth2. As highlighted, the WAIC performs best relative to the AIC and DIC.\n\n\n\n\n\n\n\n\n\nWAIC outperforms AIC and DIC when approximating LOOCV\n\nHow is the WAIC calculated? Remember that the likelihood tells us how probable our observed data is under a particular model, with higher likelihood values suggesting the model does a better job of explaining the observed data. However, simply choosing the model with the highest likelihood can be misleading as this would always favour more complex models.\nWAIC estimates how well our model would predict new data (out-of-sample prediction) by:\n\nUsing the likelihood to measure how well the model fits each individual data point\nAccounting for uncertainty in our parameter estimates by averaging across many possible parameter values (our posterior samples)\nPenalizing models that make very different predictions when parameters change slightly (a sign of overfitting)\n\nThe resulting WAIC score is on the deviance scale - lower values indicate better predictive models. For example, if Model A has a WAIC of 500 and Model B has a WAIC of 600, Model A is predicted to do better at generalizing to new data. When comparing models however, we typically look at WAIC differences.\n\n\n\n\n\n\nBayesian Information Criterion\n\n\n\nNote that while another type, Bayesian Information Criterion (BIC) is also commonly used, it has a different goal: finding the “true” model under specific assumptions about the data-generating process. Since in cognitive modeling we typically assume all models are approximations, WAIC’s focus on predictive accuracy is more appropriate for our purpose."
  },
  {
    "objectID": "workshops/08.compare_models/qmd/model_comparison.html#model-comparison-using-waic",
    "href": "workshops/08.compare_models/qmd/model_comparison.html#model-comparison-using-waic",
    "title": "Model comparison",
    "section": "Model comparison using WAIC",
    "text": "Model comparison using WAIC\nLet’s now apply the WAIC practically in Stan to compare model performance across multiple reinforcement learning models.\nWe have already utilized several blocks beyond the basic data, parameters, and model blocks in Stan:\n\ntransformed data for pre-processing data before sampling begins.\ntransformed parameters for implementing parameter transformations, including reparameterization.\n\nWhen calculating the log-likelihood, we use another optional block - generated quantities for post-processing our posterior samples after the main sampling is complete. It’s ideal for computing quantities we need for model comparison (like log-likelihoods) or predictions.\nSo, building from our existing reparameterized hierarchical RL model from the last workshop, we simply add a generated quantities block, where we calculate log-likelihoods for each subject:\ngenerated quantities {\n  real&lt;lower=0,upper=1&gt; lr_mu; \n  real&lt;lower=0,upper=3&gt; tau_mu;\n  \n  real log_lik[nSubjects];\n  \n  lr_mu  = Phi_approx(lr_mu_raw);\n  tau_mu = Phi_approx(tau_mu_raw) * 3;\n\n  { // local section, this saves time and space\n    for (s in 1:nSubjects) {\n      vector[2] v; \n      real pe;    \n\n      log_lik[s] = 0;\n      v = initV;\n\n      for (t in 1:nTrials) {    \n        log_lik[s] = log_lik[s] + categorical_logit_lpmf(choice[s,t] | tau[s] * v);    \n              \n        pe = reward[s,t] - v[choice[s,t]];      \n        v[choice[s,t]] = v[choice[s,t]] + lr[s] * pe; \n      }\n    }    \n  }\n}\nIn this block, we declare log-likelihoods for each subject:\nreal log_lik[nSubjects];  // Declare log-likelihood array\ninitialize their log-likelihood to zero:\nlog_lik[s] = 0;\nand loop through trials, accumulating log-likelihood:\nlog_lik[s] = log_lik[s] + categorical_logit_lpmf(choice[s,t] | tau[s] * v);\nThis line above is important and calculates how likely (the log-probability) it was that the participant made the choice they actually made, according to our model.\nLet’s break this line down further:\nRecall that our data (choice[s,t]) is discrete - participants choose either option 1 or 2. We therefore can’t use continuous distributions like normal for discrete data. Instead, we use the categorical distribution, which gives probabilities for discrete outcomes.\n\nSpecifically we use the categorical_logit_lpmf (categorical log probability mass function) because we’re working with discrete data and log-odds.\nThe vertical bar | in choice[s,t] | tau[s] * v represents Bayesian conditioning. It reads as “the probability of observing choice[s,t] given tau[s] * v”. This directly reflects the Bayesian relationship between our data, parameters and our model’s predictions.\n\nSo to summarise:\n\nOur model predicts values for each option (stored in v)\nBased on these values and the participant’s inverse temperature (tau[s]), we can calculate how likely they were to choose each option\nWe then look at what they actually chose (choice[s,t])\nThe function calculates the log probability that they would make that specific choice\nWe then add this log probability to a running total (log_lik[s]) for that participant. By doing this for every trial, we get a measure of how well our model predicted that participant’s entire sequence of choices.\n\nOnce we have the log likelihoods, we can then calculate model comparison metrics using the loo package in R3:\nThis is rather simple and firstly involves extracting the log-likelihood matrix from our Stan fit object:\nLL1 &lt;- extract_log_lik(stanfit)\nThen we can simple compute either the WAIC or LOO-CV (Leave-One-Out Cross-Validation) by Pareto Smoothed Importance Sampling (PSIS):\nwaic1 &lt;- waic(LL1)  # WAIC calculation\nloo1 &lt;- loo(LL1)    # PSIS-LOO calculation\n\n\n\n\n\n\nWAIC or PSIS-LOO?\n\n\n\nWAIC and PSIS-LOO are both methods for estimating out-of-sample prediction accuracy, but they approach it differently. WAIC approximates leave-one-out cross-validation using the entire dataset, while PSIS-LOO uses importance sampling to approximate true leave-one-out cross-validation. In practice, they often give very similar results, and both are valid choices for model comparison.\n\n\nThe (hypothetical) output below shows us several key metrics computed from the loo package, in the case below as a 4000 x 20 log-likelihood matrix (representing 4000 posterior samples for 20 subjects):\nComputed from 4000 by 20 log-likelihood matrix\n\n         Estimate  SE\nelpd_loo    -29.5 3.3 \np_loo         2.7 1.0\nlooic        58.9 6.7\nWhere:\n\nelpd: expected log predictive density (how well a model is expected to predict new, unseen data)\np_loo/p_waic: effective number of parameters\nlooic/waic: information criterion value\n\nWhat we are most interested in is the looic estimate, which in this case is 58.9. Remember that lower values mean a better model!\n\nIntroducing the Fictitious RL model\nNow that we know what to expect, let’s run our model comparison. However for this exercise, we will use a different dataset to what we have used prior.\nSo far, we’ve worked with reinforcement learning tasks where reward probabilities remained constant. However, in real life, reward contingencies often change - what was once rewarding might become unfavorable, and vice versa. The reversal learning task4 captures this dynamic nature of learning.\n\n\n\n\n\n\n\n\n\nAn example reversal learning reward contingency, with reversals every 8-12 trials\n\nIn a conventional reversal learning task, participants choose between two stimuli, where initially, one stimulus might have a higher reward probability (say 70%) while the other has a lower probability (30%). However, after a certain number of trials, these probabilities switch or “reverse” - the previously better option becomes the worse option and vice versa. This creates a more challenging learning environment where participants must detect and adapt to these changes. The block length, or the number of trials between each reversal, is also variable; in the example above it is constrained to occur between every 8-12 trials.\nOf course, to perform model comparison, we need more than one model! We already have our standard Rescorla-Wagner (RW) model that we’ve used before, so now we will compare it with a variant - the Fictitious (or Counterfactual) RL model.\nRecall that the standard RW model updates only the value of the chosen option based on the received reward:\n\\[V_{t+1}^c = V_t^c + \\alpha \\cdot PE\\]\nwith the prediction error \\(PE\\) defined as:\n\\[PE = R_t - V_t^c\\]\nwhere:\n\n\\(V_t^c\\) is the value of the chosen option,\n\\(\\alpha\\) is the learning rate.\n\nThe Fictitious RL model extends the basic Rescorla-Wagner by also updating the value of the unchosen option. The key insight is that in binary choice tasks with complementary rewards (like our reversal learning task), not receiving a reward on one option provides information about what might have happened had we chosen the other option.\nThe model captures this through these equations:\n\\[V_{t+1}^c = V_t^c + \\alpha \\cdot PE\\] and simultaneously:\n\\[V_{t+1}^{nc} = V_t^{nc} + \\alpha \\cdot PE_{nc}\\]\nwhere:\n\\[PE = R_t - V_t^c\\] and:\n\\[PE_{nc} = -R_t - V_t^{nc}\\]\nThe crucial difference are in the second and fourth equations: the model updates the value of the non-chosen option (\\(V_t^{nc}\\)) using a counterfactual prediction error (\\(PE_{nc}\\)).\nThe negative sign before \\(R_t\\) in \\(PE_{nc}\\) in the last equation reflects the complementary nature of the rewards - if one option gives a reward, the other option would have given no reward, and vice versa. This counterfactual learning might be particularly useful in reversal learning tasks, as it allows participants to learn about both options on every trial, potentially enabling faster adaptation to reversals.\nTo better understand the difference between the two models, the graphs below plot the value update calculated by both models with the learning rate (\\(η = 0.35\\)) and inverse temperature (\\(τ = 1.2\\)) parameters fixed to be the same for both:\n\n\n\n\n\n\n\n\n\nValue and choice predicted by the Fictitious and Rescorla-Wagner (Simple) RL models with fixed learning rates and inverse temperatures\n\nThe crucial difference between the models is visible in how the unchosen option’s value changes. In the Simple RL model, the unchosen option’s value remains flat/unchanged until it is chosen again. This creates “plateaus” in the value lines when an option isn’t chosen, for example in the orange line around trials 20-30, where it remains completely flat while option A is being chosen.\nConversely, for the Fictitious RL model, where both options’ values are updated on every trial, no flat plateaus are visible. Subsequently, the value difference between the options for both models, highlights how this is - on average, across all trials - larger for the Fictitious RL model compared to the Simple RL model. As a result, because larger value differences could translate into an easier choice between the two options, the Fictitious RL represents a potential cognitive process that people use.\nNow that we have two models, we can compare them directly using the WAIC, and test whether participants actually use this additional information when making decisions in the reversal learning task.\nThe Stan implementation of the Fictitious RL can be found in _scripts/comparing_models_model2.stan. Comparing this to the Simple RL comparing_models_model1.stan, we can’t see much of a difference. In fact, the only major difference is in the model block.\nIn the Simple RL, we have a simple update rule that only modifies the value of the chosen option:\npe = reward[s,t] - v[choice[s,t]];      \nv[choice[s,t]] = v[choice[s,t]] + lr[s] * pe;\nThis shows that on each trial, the model:\n\nCalculates a prediction error (pe) for the chosen option\nUpdates only the value of the chosen option using this prediction error\n\nThe second model extends this by also updating the unchosen option’s value:\npe = reward[s,t] - v[choice[s,t]];   \npenc = -reward[s,t] - v[3 - choice[s,t]];\n      \nv[choice[s,t]] = v[choice[s,t]] + lr[s] * pe; \nv[3 - choice[s,t]] = v[3 - choice[s,t]] + lr[s] * penc;\nAfter declaring a new variable penc for the counterfactual prediction error real penc;:\n\nWe use 3 - choice[s,t] to index the unchosen option (since if choice is 1, we want 2, and if choice is 2, we want 1)\nWe calculate penc using the negative of the reward (-reward[s,t])\nWe update both the chosen and unchosen options’ values on each trial\n\n\n\n\n\n\n\nExercise 13\n\n\n\n1. Run both models using the R script compare_models_main.R, and examine the LOOIC values for each. Which is best?\n\n\n\n\n\n\nClick to reveal the solution\n\n\n\n\n\nThe results should look (roughly5) like this:\n&gt; LL1 &lt;- extract_log_lik(fit_rl1)\n&gt; ( loo1 &lt;- loo(LL1) )\nComputed from 4000 by 10 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -389.4 15.4\np_loo         3.5  0.7\nlooic       778.7 30.8\n\n&gt; ( loo2 &lt;- loo(LL2) )\nComputed from 4000 by 10 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -281.6 17.5\np_loo         3.8  0.5\nlooic       563.2 35.1\nThe lower LOOIC value for the second model (Fictitious RL) (~563 vs ~779) suggests it provides a better fit to the data than the Rescorla-Wagner model."
  },
  {
    "objectID": "workshops/08.compare_models/qmd/model_comparison.html#posterior-predictive-checks",
    "href": "workshops/08.compare_models/qmd/model_comparison.html#posterior-predictive-checks",
    "title": "Model comparison",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\nWe have just compared our two RL models objectively using information criteria, suggesting that the Fictitious RL model will better predict new, unseen data from our reversal learning task and more accurately captures the underlying latent cognitive processes that generated the observed data. But while measurements like WAIC and LOO help us compare models quantitatively, posterior predictive checks allow us to visually assess how well our models actually capture patterns in the observed data.\nThe basic idea of posterior predictive checks is simple: we ask our model to generate simulated data. If our model is good, data simulated from it should look similar to our actual data. Not only does running posterior predictive checks allow us to validate good fitting models, it also can be used to falsify bad fitting ones.\nTo perform posterior predictive checks, we need to add a section to our Stan models that generates simulated data. We do this in the generated quantities block.\nImportantly, because we are using a Bayesian approach, we don’t just simulate from the mean parameter estimates - instead, we simulate new data using every posterior sample of our parameters. This is crucial because it allows us to incorporate our uncertainty about the parameters into our predictions.\nFrom our Stan model, we get posterior samples for all parameters - in our case, 4000 draws from the posterior distribution of each parameter (learning rates, inverse temperatures, etc.). For each of these 4000 parameter sets, we then simulate an entire experiment worth of data using those specific parameters. This is what’s happening in the corresponding generated quantities block:\ngenerated quantities {\n  // ... existing code for log likelihood ...\n  \n  // Generate predicted choices\n  array[nSubjects, nTrials] int y_pred;\n  \n  { // local section\n    for (s in 1:nSubjects) {\n      vector[2] v;\n      real pe;\n      v = initV;\n      \n      for (t in 1:nTrials) {\n        // Generate predicted choice from current values\n        y_pred[s,t] = categorical_logit_rng(tau[s] * v);\n        \n        // Update values using actual reward\n        pe = reward[s,t] - v[choice[s,t]];\n        v[choice[s,t]] = v[choice[s,t]] + lr[s] * pe;\n      }\n    }\n  }\n}\nThis generates 4000 different simulated datasets. Subsequently, we need to then compare the accuracy of these datasets using R.\n\n\n\n\n\n\nExercise 14\n\n\n\n1. Plot the posterior predictive checks for both models (Simple and Fictitious RL) using the R script comparing_models_ppc.R and examine the output.\n\n\nYou should generate the two plots below:\n\n\n\n\n\n\n\n\n\nPosterior predictive checks for both models: trial-by-trial choice accuracy (left) and distribution of overall accuracy (right)\n\nFor the time course plot on the left, for each trial, we take all 4000 simulated experiments and calculate the proportion of correct choices at that trial across all subjects.\nFor the distribution plot on the right, for each of the 4000 simulated experiments, we calculate the overall proportion of correct choices across all trials and subjects. The histograms show the distribution of these 4000 overall proportions, whilst the vertical line shows the actual overall proportion from the real data.\nAgain, you can see that the straight blue line is closer to the peak for the Fictitious RL compared to the simple RL, demonstrating that this model better captures the true choice behaviour patterns in the data."
  },
  {
    "objectID": "workshops/08.compare_models/qmd/model_comparison.html#footnotes",
    "href": "workshops/08.compare_models/qmd/model_comparison.html#footnotes",
    "title": "Model comparison",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPitt, M. A., & Myung, I. J. (2002). When a good fit can be bad. Trends in cognitive sciences, 6(10), 421-425.↩︎\nGelman, A., Hwang, J., & Vehtari, A. (2014). Understanding predictive information criteria for Bayesian models. Statistics and computing, 24, 997-1016.↩︎\nVehtari, A., Gabry, J., Magnusson, M., Yao, Y., Bürkner, P., Paananen, T., & Gelman, A. (2024). loo: Efficient leave-one-out cross-validation and WAIC for Bayesian models (Version 2.8.0) [R package]. Retrieved from https://mc-stan.org/loo/↩︎\nZhang, L., & Gläscher, J. (2020). A brain network supporting social influences in human decision-making. Science advances, 6(34), eabb4159.↩︎\nEven though the Stan model code in R uses the same seed, differences in hardware across computers will produce slightly different end results.↩︎"
  },
  {
    "objectID": "workshops/10.model_based/qmd/model_fmri.html",
    "href": "workshops/10.model_based/qmd/model_fmri.html",
    "title": "An introduction to model-based fMRI",
    "section": "",
    "text": "So far in this course, we have focused on using computational models to uncover latent variables from behavioural data - specifically choice data collected when participants complete experimental tasks on a computer. However, the principles of computational modeling are incredibly versatile and can be applied to many different types of data. Beyond behavioral choices, we can use these same approaches to analyze physiological measures like skin conductance responses and eye movements, as well as neural data from brain imaging. In this bonus workshop, we will briefly explore how we can combine our computational models with functional magnetic resonance imaging (fMRI) through “model-based fMRI”."
  },
  {
    "objectID": "workshops/10.model_based/qmd/model_fmri.html#what-is-model-based-fmri",
    "href": "workshops/10.model_based/qmd/model_fmri.html#what-is-model-based-fmri",
    "title": "An introduction to model-based fMRI",
    "section": "What is model-based fMRI?",
    "text": "What is model-based fMRI?\nModel-based fMRI12 allows us to link computational processes to brain activity. Rather than just looking at which brain areas activate during a task, this approach lets us identify which regions might be computing specific variables in our computational model. The stages of running a model-based fMRI analysis are depicted below3:\n\n\n\n\n\n\n\n\n\nThe model-based fMRI pipeline, as applied to a prediction error parameter\n\nLet’s break down how this works using our reinforcement learning model as an example:\n1. First, we fit our computational model to participants’ behavioural data as we’ve done before, giving us trial-by-trial estimates of key variables:\n\nAction values \\((V)\\) for each choice option\nPrediction errors \\((PE)\\) when outcomes are received\n\nThese trial-by-trial estimates form time series that capture how these variables evolve over the course of the experiment.\nFor example, in reinforcement learning we commonly track:\nThe value update:\n\\[V_{t+1} = V_t + \\alpha PE_t\\]\nand the prediction error:\n\\[PE_t = R_t - V_t\\]\n2. We then take these trial-by-trial estimates and convolve them with the haemodynamic response function to account for the nature of the BOLD signal.\n\nThis creates a model-based regressor that predicts how brain activity should vary if a region is computing that variable.\n\n3. These processed regressors are then included in a general linear model (GLM) analysis which tests how well each regressor explains the BOLD signal at each voxel in the brain.\n\nUltimately, this creates a statistical map showing regions where activity significantly correlates with our model variables.\n\nWhen visualizing the results, parameter estimates can show the strength of correlation between the model regressor and BOLD activity in different brain regions (bottom middle), whilst plotting the trial-by-trial event-related BOLD response (bottom right) can show how brain activity varies with different levels of the model variable (e.g., high vs medium vs low prediction errors)."
  },
  {
    "objectID": "workshops/10.model_based/qmd/model_fmri.html#dissociating-prediction-errors-from-reward-signals-using-model-based-fmri",
    "href": "workshops/10.model_based/qmd/model_fmri.html#dissociating-prediction-errors-from-reward-signals-using-model-based-fmri",
    "title": "An introduction to model-based fMRI",
    "section": "Dissociating prediction errors from reward signals using model-based fMRI",
    "text": "Dissociating prediction errors from reward signals using model-based fMRI\nTo this end, model-based fMRI is a useful tool for determining the specific computational variables that brain regions may be encoding. We can investigate specific hypotheses regarding neural computation not possible using fMRI as a stand-alone tool.\nTo demonstrate this, let’s ask ourselves the question as to whether striatal activity truly reflects prediction error computation rather than simply encoding reward value.\nLooking at the Rescorla-Wagner equation for the prediction error:\n\\[PE_t = R_t - V_t\\]\nWe can see that prediction errors are inherently related to reward \\((R_t)\\). When rewards are received, they contribute positively to the prediction error, which might explain why striatal activity correlates with reward valence. This raises an important question: How can we be confident that striatal activity represents prediction error computation rather than just reward processing?\nLooking at the equation again, a true prediction error signal should show a positive correlation with reward \\((R_t)\\) but a negative correlation with expected value \\((V_t)\\). This pattern is depicted in the image below on the left.\nSubsequently, model-based fMRI gives us a way to specifically test whether striatal activity reflects genuine prediction error computation:\n\nIf the BOLD signal is only positively correlated with reward \\((R_t)\\), this would suggest the region simply processes reward valence\nBut, if the BOLD signal is positively correlated with reward \\((R_t)\\) AND negatively correlated with expected value \\((V_t)\\), this provides stronger evidence for a specific role with computing the prediction error\n\n\n\n\n\n\n\n\n\n\nModel-based fMRI suggests that striatal activity represents prediction error computation rather than just responding to reward valence\n\nAs shown on the right in the figure above, model-based fMRI4 reveals exactly this pattern in the ventral striatum/nucleus accumbens, where activity was found to increase with reward but decreases with expected value, matching the computational signature of a prediction error signal. This dissociation provides compelling evidence that the striatum is computing prediction errors rather than merely responding to rewards."
  },
  {
    "objectID": "workshops/10.model_based/qmd/model_fmri.html#footnotes",
    "href": "workshops/10.model_based/qmd/model_fmri.html#footnotes",
    "title": "An introduction to model-based fMRI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGläscher, J. P., & O’Doherty, J. P. (2010). Model‐based approaches to neuroimaging: combining reinforcement learning theory with fMRI data. Wiley Interdisciplinary Reviews: Cognitive Science, 1(4), 501-510.↩︎\nO’Doherty, J. P., Hampton, A., & Kim, H. (2007). Model‐based fMRI and its application to reward learning and decision making. Annals of the New York Academy of sciences, 1104(1), 35-53.↩︎\nGläscher, J. P., & O’Doherty, J. P. (2010). Model‐based approaches to neuroimaging: combining reinforcement learning theory with fMRI data. Wiley Interdisciplinary Reviews: Cognitive Science, 1(4), 501-510.↩︎\nZhang, L., & Gläscher, J. (2020). A brain network supporting social influences in human decision-making. Science advances, 6(34), eabb4159.↩︎"
  },
  {
    "objectID": "workshops/06.reinforcement_learning/qmd/hierarchical_modeling.html",
    "href": "workshops/06.reinforcement_learning/qmd/hierarchical_modeling.html",
    "title": "Hierarchical Bayesian modeling in Stan",
    "section": "",
    "text": "Working directory for this workshop\n\n\n\nModel code and R scripts for this workshop are located in the (/workshops/06.reinforcement_learning) directory.\nAt the end of the last workshop, we explored two approaches to modeling multiple subjects in reinforcement learning: using a subject-loop with fixed parameters across all subjects, and fitting each subject independently. While both approaches have their merits, they also have limitations when accurately modeling subjects.\nLet’s examine why we need a more sophisticated approach by looking at some simulation results1:\nThe diagram shows results from a simulation analysis ultimately demonstrating that when estimated with hierarchical Bayesian analysis, parameter estimates (for a loss aversion parameter \\(\\lambda\\)) show much less discrepancy with actual values compared to those estimated with maximum likelihood estimation (MLE). Compared to MLE, hierarchical Bayesian analysis can also be sensitive to individual differences when there is enough information.\nAs we can see from these results, treating all subjects as identical (fixed effects) or completely independent (random effects) represents two extremes of a spectrum (depicted below):\nWhen we use a subject-loop with fixed parameters (as we did in our previous workshop), we’re essentially saying that everyone learns and makes decisions in exactly the same way. This approach forces us to find “average” parameters that might not actually represent any real person’s behaviour well.\nOn the other hand, when we fit each subject independently (random effects), we ignore an important fact: while people are different, they’re not completely different. As a consequence, we can end up with very noisy parameter estimates, especially if we have limited data per person."
  },
  {
    "objectID": "workshops/06.reinforcement_learning/qmd/hierarchical_modeling.html#hierarchical-models-an-ideal-middle-ground",
    "href": "workshops/06.reinforcement_learning/qmd/hierarchical_modeling.html#hierarchical-models-an-ideal-middle-ground",
    "title": "Hierarchical Bayesian modeling in Stan",
    "section": "Hierarchical models: an ideal middle ground",
    "text": "Hierarchical models: an ideal middle ground\nHierarchical (Bayesian) modeling offers a solution by finding a middle ground between these two extremes, by taking into account that whilst people are different, their parameters are likely to come from some common distribution.\nHierarchical models:\n\nAcknowledge individual differences by giving each person their own parameters\nRecognize similarities between people by assuming these parameters come from a shared group-level distribution\nUse group-level information to improve individual estimates, especially when data is limited\n\nWe can demonstrate the structure of this hierarchical approach in the diagram below:\n\n\n\n\n\n\n\n\n\nParameters and observed data in the hierarchical structure\n\nThe hierarchical structure consists of three main levels:\n\nAt the bottom level, we have \\(y_1\\) through \\(y_N\\), representing the observed data from each individual subject.\nIn the middle level, we have \\(\\theta_1\\) through \\(\\theta_N\\), representing the subject-level parameters (like learning rates or inverse temperatures for each person).\nAt the top level, we have \\(\\phi\\), representing the population-level parameters that describe how individual parameters are distributed across the group.\n\nWhat makes this approach powerful is how these levels interact during parameter estimation, with the estimation of individual-level parameters \\((\\theta)\\) informed by two sources simultaneously: the subject’s own data \\((y)\\) and the group-level distribution \\((\\phi)\\). Similarly, the group-level parameter (\\(\\phi\\)) are estimated based on the patterns observed across all individual parameter estimates. This creates a dynamic interaction where each level simultaneously informs the others during the estimation process.\n\n\n\n\n\n\nGroup level parameters in a reinforcement learning model\n\n\n\nIn our reinforcement learning context, we might assume that learning rates across the population follow a normal distribution. In this case, we can replace \\(\\phi\\) with two parameters:\n\n\\(\\mu_\\theta\\) would represent the average learning rate in the population,\nwhile \\(\\sigma_\\theta\\) would represent how much variation exists between individuals.\n\nHowever, we could choose different distributions if we had reason to believe the parameters follow a different pattern at the group level.\n\n\nThis simultaneous estimation at all levels allows the model to find the right balance between individual differences and group-level patterns, leading to more robust parameter estimates than we could achieve by analyzing each subject in isolation or treating all subjects as identical.\nWe can also describe the hierarchical approach in terms of Bayes’ theorem. Recall that in Bayesian analysis, we aim to update our prior beliefs about parameters using observed data to arrive at posterior estimates. With hierarchical modeling, we’re simply extending this approach to handle multiple levels of parameters - both individual and group-level - simultaneously.\nThis multilevel structure described through Bayes’ theorem is:\n\\[P(\\Theta, \\Phi | D) = \\frac{P(D|\\Theta, \\Phi)P(\\Theta, \\Phi)}{P(D)} \\propto P(D|\\Theta)P(\\Theta|\\Phi)P(\\Phi)\\]\nThis equation captures the three levels of our hierarchical structure:\n\\(P(D|\\Theta)\\) the data level: the likelihood of observing our choice data given each subject’s parameters\n\\(P(\\Theta|\\Phi)\\) the individual parameter level: how individual parameters \\((\\Theta, \\Phi)\\) vary around the population parameters\n\\(P(\\Phi)\\) the population level: our prior beliefs about the population-level parameters (e.g., group mean and variance)\nThe multiplication of these components during estimation reflects how information flows between levels in our hierarchical structure, with each level informing the others simultaneously.\nAnother (more intuitive) way to understand the hierarchical appraoch is to understand the relationship between the distributions of both individual and group-level parameters:\n\n\n\n\n\n\n\n\n\nThe relationship between individual and group-level parameters in hierarchical models\n\nA) This plot shows that each individual’s parameter estimate isn’t just a single point - it’s a full distribution, as we use a Bayesian approach. This captures our uncertainty about each person’s true parameter value. The overlapping distributions at the bottom represent different individuals’ parameter estimates, while the upper curve represents the group-level distribution from which these individual parameters are drawn.\nB) This plot shows how individual estimates are nested within the group-level distribution. The \\(\\mu\\) represents the group mean, and \\(\\sigma\\) represents the group-level variance. Each individual’s distribution is centered at a different point, but all are constrained by the overarching group-level distribution shape.\nC) This diagram shows how information flows through the hierarchy. At the top, we set hyperpriors (priors for group-level distributions) for \\(\\mu\\) and \\(\\sigma^2\\) which shape the group-level distribution in the middle, which then influence individual parameter distributions \\((R_1, R_2, R_3)\\) at the bottom."
  },
  {
    "objectID": "workshops/06.reinforcement_learning/qmd/hierarchical_modeling.html#building-a-hierarchical-rescorla-wagner-model",
    "href": "workshops/06.reinforcement_learning/qmd/hierarchical_modeling.html#building-a-hierarchical-rescorla-wagner-model",
    "title": "Hierarchical Bayesian modeling in Stan",
    "section": "Building a hierarchical Rescorla-Wagner model",
    "text": "Building a hierarchical Rescorla-Wagner model\nGiven our current progress with applying RL models in Stan, making the model hierarchical isn’t too difficult (conceptually at least!)\nThe hierarchical model - and how it builds upon our previous model which fits multiple individuals independently - is depicted graphically below:\n\n\n\n\n\n\n\n\n\nThe hierarchical Rescorla-Wagner model with hyperparameters highlighted in red\n\nWe now simply state that the two parameters of interest, the learning rate \\(\\alpha\\), and inverse temperature \\(τ\\), come from a group level distribution. We will assume in this case that this is normally-distributed at the population level, and so define four hyperparameters:\n\nLearning rate: with group-level mean \\((\\mu_\\alpha)\\) and group-level standard deviation \\((\\sigma_\\alpha)\\)\nInverse temperature: with group-level mean \\((\\mu_\\tau)\\) and group-level standard deviation \\((\\sigma_\\tau)\\)\n\nModel building both conceptually and in Stan is best acheived iteratively. We start with simple models and layer complexity. You can see that in the diagram below:\n\n\n\n\n\n\n\n\n\nThe evolution of our RL models from single subject to hierarchical multi-subject\n\nWe started with the basic Rescorla-Wagner model in a single subject (far left) then adapted the model to run across subjects in a fixed effects way (second from left) and then by fitting them all independently (third from left). We now look to implement a hierarchical structure (far right). Remember that in all cases, the underlying cognitive model is the same! We are simply changing how the parameters are estimated.\nLooking at the hierarchical model, we can see that this model now consists of six priors, the value update and softmax choice rule:\n\n\n\n\n\n\n\n\n\nThe structure, parameters and priors of the hierarchical model\n\nRemember that the core computational mechanisms of our reinforcement learning model - the value update equation (Rescorla-Wagner learning) and the softmax choice rule - remain unchanged in the hierarchical version:\nValue update:\n\\[V_{i,t+1}^c = V_{i,t}^c + \\alpha_i(R_{i,t} - V_{i,t}^c)\\] Choice probability:\n\\[p_{i,t}(C=A) = \\frac{1}{1 + e^{\\tau_i(V_{i,t}(B)-V_{i,t}(A))}}\\]\nWhat changes is how we structure the parameters \\(\\alpha\\) (learning rate) and \\(\\tau\\) (inverse temperature) across subjects. Instead of estimating these parameters independently for each subject, we create a hierarchical structure with multiple levels.\n\nGroup-level parameters\nAt the highest level, we have our group-level parameters:\nFor the group means \\((\\mu_\\alpha\\) and \\(\\mu_\\tau)\\) we use uniform distributions because we want to remain uncertain about where the average learning rate and inverse temperature might lie within their possible ranges. This is known as a ‘non-informative’ or ‘flat’ prior, as we’re saying all values within the defined ranges are equally likely.\n\\[\\mu_\\alpha \\sim \\text{Uniform}(0,1)\\] \\[\\mu_\\tau \\sim \\text{Uniform}(0,3)\\]\nFor the group standard deviations \\((\\sigma_\\alpha)\\) and \\((\\sigma_\\tau)\\) we use the half-Cauchy distribution because it’s particularly well-suited for standard deviation parameters. Remember that while similar to the normal distribution, the half-Cauchy has heavier tails.\n\\[\\sigma_\\alpha \\sim \\text{halfCauchy}(0,1)\\] \\[\\sigma_\\tau \\sim \\text{halfCauchy}(0,3)\\]\n\n\nIndividual-level parameters\nEach subject’s individual parameters are drawn from a normal distribution centered on the group mean \\((\\mu)\\) with spread determined by the group standard deviation \\((\\sigma)\\).\n\\[\\alpha_i \\sim \\text{Normal}(\\mu_\\alpha, \\sigma_\\alpha)_{\\mathcal{T}(0,1)}\\] \\[\\tau_i \\sim \\text{Normal}(\\mu_\\tau, \\sigma_\\tau)_{\\mathcal{T}(0,3)}\\]\nHowever, notice the subscript \\(\\mathcal{T}\\) with these distributions - this indicates these are truncated normal distributions. We need this truncation because the normal distribution extends infinitely in both directions, so truncating them ensures our parameters stay bounded.\n\n\n\n\n\n\nBounding parameters and MCMC sampling\n\n\n\nBounding parameters - whilst often theoretically necessary (like keeping learning rates between 0 and 1) - can sometimes create challenges for MCMC sampling. When a parameter gets close to its boundary, many of the MCMC’s proposed steps will be rejected because they would land outside the valid range. This is like trying to take random walks near a wall - many of the steps would simply hit the wall. Monitoring your chain mixing, convergence diagnostics and parameter estimates can help to determine whether this is the case.\n\n\n\n\n\n\n\n\nSetting priors\n\n\n\nThe priors used in this tutorial are chosen for pedagogical reasons and may not be appropriate for your particular research question. You should set your priors according to several factors including your specific research context, theoretical understanding, and existing literature in your field."
  },
  {
    "objectID": "workshops/06.reinforcement_learning/qmd/hierarchical_modeling.html#creating-the-hierarchical-rescorla-wagner-model-in-stan",
    "href": "workshops/06.reinforcement_learning/qmd/hierarchical_modeling.html#creating-the-hierarchical-rescorla-wagner-model-in-stan",
    "title": "Hierarchical Bayesian modeling in Stan",
    "section": "Creating the hierarchical Rescorla-Wagner model in Stan",
    "text": "Creating the hierarchical Rescorla-Wagner model in Stan\nThe conceptual basis for the hierarchical model outlined above has been translated into the appropriate Stan model _scripts/reinforcement_learning_mp_hrch_model.stan.\nWithin this model, the data and transformed data blocks remain unchanged, but we now introduce group-level parameters in the parameters block:\nparameters {\n  real&lt;lower=0,upper=1&gt; lr_mu;\n  real&lt;lower=0,upper=3&gt; tau_mu;\n\n  real&lt;lower=0&gt; lr_sd;\n  real&lt;lower=0&gt; tau_sd;\n\n  real&lt;lower=0,upper=1&gt; lr[nSubjects];\n  real&lt;lower=0,upper=3&gt; tau[nSubjects];  \n}\nWhere we declare our parameters:\n\nlr_mu and tau_mu are our group-level means (\\(\\mu_\\alpha\\) and \\(\\mu_\\tau\\))\nlr_sd and tau_sd are our group-level standard deviations (\\(\\sigma_\\alpha\\) and \\(\\sigma_\\tau\\))\nThe arrays lr[nSubjects] and tau[nSubjects] hold individual-level parameters for each subject.\n\nRemember that the parameters block declares variables and their bounds, but not their probability distributions. Therefore we define the probability distributions (priors) in the model block:\nmodel {\n  lr_sd  ~ cauchy(0,1);\n  tau_sd ~ cauchy(0,3);\n  \n  // give the prior here: how individual-level parameters are connected to the group-level parameters\n  lr ~ normal(lr_mu, lr_sd);\n  tau ~ normal(tau_mu, tau_sd);\n  \n  ...\n  ...\n  \n\nThe lines lr_sd ~ cauchy(0,1) and tau_sd ~ cauchy(0,3) specify our Cauchy priors for the group-level standard deviations. The zero-bounded constraint in the parameters block automatically makes this a half-Cauchy (only positive values).\nlr ~ normal(lr_mu, lr_sd) and tau ~ normal(tau_mu, tau_sd) implement the individual-level prior parameter distributions. The bounds specified in the parameters block automatically make these truncated normal distributions.\n\n\n\n\n\n\n\nSetting the uniform distribution\n\n\n\nRecall that Stan implicitly sets a uniform prior if a distribution if not explicitly stated (i.e., for lr_mu and tau_mu in this model).\n\n\n\n\n\n\n\n\nExercise 10\n\n\n\n1. Run the hierarchical model reinforcement_learning_mp_hrch_model.stan using the script reinforcement_learning_multi_parm_main.R. Examine the posterior density distribution and violin plots generated.\nRemember to only highlight and run the appropriate sections of the script!"
  },
  {
    "objectID": "workshops/06.reinforcement_learning/qmd/hierarchical_modeling.html#footnotes",
    "href": "workshops/06.reinforcement_learning/qmd/hierarchical_modeling.html#footnotes",
    "title": "Hierarchical Bayesian modeling in Stan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAhn, W. Y., Krawitz, A., Kim, W., Busemeyer, J. R., & Brown, J. W. (2013). A model-based fMRI analysis with hierarchical Bayesian parameter estimation.↩︎"
  },
  {
    "objectID": "workshops/06.reinforcement_learning/qmd/intro.html",
    "href": "workshops/06.reinforcement_learning/qmd/intro.html",
    "title": "Reinforcement learning models",
    "section": "",
    "text": "Welcome to the sixth workshop of the BayesCog course!\nUp to this point in the course, we have covered a lot of ground, including the basics of probability, Bayesian statistics (and why it may be preferable to frequentist methods), a basic overview of the Stan programming language, and how it can be applied to model data. We have practically done so using multiple statistical models, including the binomial and Bernoulli choice models as well as linear regression. However, as a student of this course, you are most likely not interested in modeling coin flips, but rather would like to determine the latent processes underlying human behaviour in cognitive tasks!\nThe course is not inherently divided into two sections, but it it were, what we have done up until now would consist the first. In the remainder of the workshops, we will mainly apply our knowledge regarding Bayesian statistics and the Stan programming langauge to specifically model cognitive processes.\nThis workshop will firstly provide a conceptual overview of cognitive modeling and reinforcement learning (RL) as mathematical frameworks for understanding human behaviour. We’ll then implement this practically by programming a simple RL model: the Rescorla-Wagner model and apply it to behavioural choice data.\nThe goals of this workshop are to:\n\nUnderstand the fundamental principles of cognitive modeling and its applications\nLearn the basics of reinforcement learning theory and the Rescorla-Wagner model\nImplement a basic Rescorla-Wagner model in Stan for analyzing choice behaviour\n\n\n\n\n\n\n\nWorking directory for this workshop\n\n\n\nModel code and R scripts for this workshop are located in the (/workshops/06.reinforcement_learning) directory. Remember to use the R.proj file within each folder to avoid manually setting directories!\n\n\nThe copy of this workshop notes can be found on the course GitHub page."
  },
  {
    "objectID": "workshops/09.debugging/qmd/debugging.html",
    "href": "workshops/09.debugging/qmd/debugging.html",
    "title": "Debugging in Stan",
    "section": "",
    "text": "As you’ve likely discovered throughout this course, programming in Stan differs significantly from languages like R or Python. Unlike these languages where we can execute and test code line by line, Stan requires us to write and compile the entire model before we can run it. Therefore, when errors occur, we need to revisit the model code, make corrections, and recompile - making the debugging process more time-consuming and challenging.\nTherefore it is important to develop good practices for writing and debugging Stan code to minimize errors and streamline the troubleshooting process."
  },
  {
    "objectID": "workshops/09.debugging/qmd/debugging.html#best-practices-and-common-errors-in-stan",
    "href": "workshops/09.debugging/qmd/debugging.html#best-practices-and-common-errors-in-stan",
    "title": "Debugging in Stan",
    "section": "Best practices and common errors in Stan",
    "text": "Best practices and common errors in Stan\nWe can roughly divide best practices in Stan into three areas: coding reproducibility, running simulations, and code/model design.\n\nReproducibility\nJust as with any programming language, writing reproducible code should be a top priority. Your code should be clear and understandable not only to you right now, but also to your future self and to others who might need to use or modify it. When writing Stan code, ask yourself: “Would someone who has never seen this code before understand what’s happening?”\nHere are some essential practices for maintaining reproducibility:\n1. Use meaningful comments throughout your code. Comments in Stan use the same syntax as in C++ (// for single-line comments, /* */ for multi-line comments). They should explain the purpose of different model components, any assumptions being made, and the reasoning behind specific parameter constraints or priors.\n2. Set and save your random number generator seed. While this ensures reproducibility on your specific platform, keep in mind that results might still vary across different platforms or Stan versions.\n3. Use clear, descriptive variable names. Instead of generic names like x or y, use names that reflect what the variables represent. For instance, in a reinforcement learning model, instead of a for learning rate, use alpha or learning_rate. This makes your code self-documenting and easier to understand.\n\n\nRunning simulations\nOne of the most powerful aspects of statistical modeling is that our models are generative - they describe a process by which data could be created. We can leverage this feature when developing Stan code by building our model first, using it to simulate data with known parameters and then fitting the model to this simulated data to verify it recovers the true parameters.\nThis approach allows us to validate our model implementation even before data collection. If our model can’t recover known parameters from simulated data, it definitely won’t work properly with real data!\n\n\nDesign Top-Down, code Bottom-Up\nWhen developing complex models, it’s tempting to implement everything at once. However, a more effective approach is to start simple and gradually add complexity. Let’s take reinforcement learning models as an example where one could:\n\nStart with a basic reinforcement learning model (e.g., simple RW model with a single learning rate)\nVerify it works correctly\nAdd complexity one layer at a time (e.g., separate learning rates for positive/negative feedback)\nTest each new addition before moving on\n\nThis incremental approach makes debugging much more manageable. If you add a new component and suddenly encounter errors, you know exactly which addition caused the problem. In contrast, if you implement a complex hierarchical reinforcement learning model all at once and encounter errors, it can be much harder to pinpoint the source of the problem.\n\n\n\n\n\n\nChoosing a text editor\n\n\n\nYou can write Stan code in a number of text editors, including Notepad++, Sublime, Atom, VSCode or even in R directly. Some may be more intuitive to code in than others, so choose accordingly!"
  },
  {
    "objectID": "workshops/09.debugging/qmd/debugging.html#common-errors-and-warnings-in-stan",
    "href": "workshops/09.debugging/qmd/debugging.html#common-errors-and-warnings-in-stan",
    "title": "Debugging in Stan",
    "section": "Common errors and warnings in Stan",
    "text": "Common errors and warnings in Stan\nBelow is a summary of some common errors and warnings that you may encounter when writing Stan models:\n\n\n\nError Type\nWarning Type\n\n\n\n\nForget “;”\nForget last blank line\n\n\nMis-indexing\nUse earlier version of Stan\n\n\nSupport mismatch\nNumerical problems\n\n\nImproper constrain\nDivergent transitions\n\n\nImproper dimension declaration\nHit max_treedepth\n\n\nVectorizing when not supported\nBFMI too low\n\n\nWrong data type\nImproper prior\n\n\nWrong distribution names\n\n\n\nForget priors\n\n\n\nMisspelling\n\n\n\n\nHere are some examples of more common errors and warnings:\n\nErrors\n1. Forgetting a semi-colon\n// Wrong\nparameters {\n  real alpha\n  real beta\n}\n\n// Correct\nparameters {\n  real alpha;\n  real beta;\n}\n2. Mis-indexing (remember that Stan uses 1-based indexing)\n// Wrong\nfor (i in 0:N) {  // starts at 0\n  y[i] = x[i];\n}\n\n// Correct\nfor (i in 1:N) {  // starts at 1\n  y[i] = x[i];\n}\n3. Support mismatch\n// Wrong\nvector[3] x;\nvector[2] y;\nreal z = x * y;  // dimension mismatch\n\n// Correct\nvector[3] x;\nvector[3] y;\nreal z = dot_product(x, y);\n4. Improper constraint\n// Wrong\nparameters {\n  real sigma;  // variance parameter unconstrained\n}\n\n// Correct\nparameters {\n  real&lt;lower=0&gt; sigma;  // variance must be positive\n}\n5. Improper dimension declaration\n// Wrong\nparameters {\n  vector[N] beta;  // N not declared in data block\n}\n\n// Correct\ndata {\n  int&lt;lower=0&gt; N;\n}\nparameters {\n  vector[N] beta;\n}\n6. Wrong data type\n// Wrong\ndata {\n  real trials;  // count data should be integer\n}\n\n// Correct\ndata {\n  int&lt;lower=0&gt; trials;\n}\n7. Wrong distribution names\n// Wrong\ntarget += Normal_lpdf(y | mu, sigma);  // capitalized\ntarget += bernoulli(y | theta);        // missing _lpmf\n\n// Correct\ntarget += normal_lpdf(y | mu, sigma);\ntarget += bernoulli_lpmf(y | theta);\n8. Forgetting to specify priors\n// Wrong\nparameters {\n  real beta;\n}\nmodel {\n  // no prior specified\n  \n  y ~ normal(beta * x, sigma);\n}\n\n// Correct\nparameters {\n  real beta;\n}\nmodel {\n  beta ~ normal(0, 1);  // prior specified\n  y ~ normal(beta * x, sigma);\n}\n\n\n\n\n\n\nImplicit priors\n\n\n\nRecall that Stan also implicitly assigns uniform priors to parameters within their constraints. For unconstrained parameters (declared as real), this means an improper uniform prior over \\([-∞, ∞]\\). For bounded parameters (e.g., real&lt;lower=0&gt; or real&lt;lower=0, upper=1&gt;), Stan assigns a uniform prior over the constrained range. While this allows the model to run, it’s generally better practice to explicitly specify your priors.\n\n\n\n\nWarnings\n\n\n\nHere are some common warnings in Stan:\n1. Numerical problems (Stan cannot handle certain mathematically undefined operations, such as division by zero or taking the log of zero. These will cause your model to fail)\n// Potentially problematic\ntarget += log(x);  // dangerous if x can be very close to 0\n\n// Better\ntarget += log1p(x - 1);  // more stable for x close to 1\n2. Divergent transitions (often due to poor parameterization)\n// Potentially problematic\nparameters {\n  real&lt;lower=0&gt; sigma;\n}\n\n// Better (non-centered parameterization)\nparameters {\n  real sigma_raw;\n}\ntransformed parameters {\n  real&lt;lower=0&gt; sigma = exp(sigma_raw);\n}\n3. Improper prior (try to avoid overly diffuse priors)\n// Poor choice\nbeta ~ normal(0, 1000);  // too diffuse\n\n// Better choice\nbeta ~ normal(0, 5);     // more reasonable scale\nFrom the above examples, three common warnings might appear after running your R script: divergent transitions, hitting maximum tree depth, and low BFMI (Bayesian Fraction of Missing Information), all indicating potential problems with sampling efficiency. These warnings don’t necessarily mean your model is incorrect, but rather that Stan is having difficulty efficiently exploring the posterior distribution.\nWhen you encounter these warnings, your first step should be to go back to your model and try to optimize it i.e., by reparameterizing with more efficient parameter scales or improving prior choices.\nIf you’ve made changes to optimize the model but still see these warnings, you can try to adjust Stan’s sampling parameters:\n\n# Example of adjusting sampling parameters in R\nfit &lt;- stan(\n  file = \"model.stan\",\n  iter = 5000,        # Increase number of iterations\n  chains = 8,         # Increase number of chains\n  control = list(\n    adapt_delta = 0.95,     # Increase adapt_delta (default is 0.8)\n    max_treedepth = 15,     # Adjust max_treedepth if needed\n    stepsize = 0.5          # Decrease stepsize\n  )\n)\n\nRemember that while adjusting these parameters might help, they’re not a substitute for good model design. The priority is to first optimize your model, then fine-tune sampling parameters if necessary.\n\n\n\n\n\n\nGetting help\n\n\n\nEncountering warnings in Stan and knowing how to solve them may be tricky, especially so for new users. You can always refer to the Stan User Guide’s page on Errors and Warnings, the Stan Development Team’s page on Runtime warnings and convergence problems or post your problem on the Stan Discourse."
  },
  {
    "objectID": "workshops/09.debugging/qmd/debugging.html#debugging-in-stan",
    "href": "workshops/09.debugging/qmd/debugging.html#debugging-in-stan",
    "title": "Debugging in Stan",
    "section": "Debugging in Stan",
    "text": "Debugging in Stan\nHere are some key practices for debugging Stan models:\n1. Always use the .stan extension for writing your Stan models\nWhilst we have exclusively written our Stan models in a .stan file, which we then load into the R script, an alternative is to include the Stan model in the R script directly:\nstan_code &lt;- \"\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n}\n\"\n\nfit &lt;- stan(model_code = stan_code, data = data_list)\nWhile this approach can work for very simple models, it becomes unwieldy for more complex models as several features present when writing .stan files are lost, including editor support (see below), syntax highlighting and version control.\n2. Use RStudio’s ‘Check’ button to catch basic syntax errors before attempting to compile.\nIf the model is coded correctly in the .stan file, you will get something like the message below after pressing the Check button:\n&gt; rstan:::rstudio_stanc(\"~/Documents/GitHub/other/BayesCog/workshops/07.optm_rl/_scripts/funnel.stan\")\n~/Documents/GitHub/other/BayesCog/workshops/07.optm_rl/_scripts/funnel.stan is syntactically correct.\nOn the other hand, if there are syntax-related or other errors, these will be printed to the terminal (see the example section below).\n3. Use the lookup() function\nThe lookup() function in R is useful for checking important information about Stan functions:\n\n&gt; lookup(dnorm)\n          StanFunction                                                                    Arguments ReturnType\n415 normal_id_glm_lpdf (real, matrix, real, vector, T);(vector, row_vector, vector, vector, vector)     T;real\n418         normal_log                                     (real, real, T);(vector, vector, vector)     T;real\n419        normal_lpdf                                     (real, real, T);(vector, vector, vector)     T;real\n553    std_normal_lpdf                                                                 (T);(vector)     T;real\n\nThe lookup for dnorm shows several normal distribution-related functions in Stan, and helps us understand the correct function to use in our Stan code. For example, it tells us that we should use normal_lpdf() rather than normal_log() for probability density calculations, and each function shows its expected argument types and return value.\n4. Be careful with copying/pasting\nWhen copying code between blocks, be mindful of variable names and syntax differences. For example, we have previously copy-pasted code from the model block to the generated quatities block to calculate the log-likelihood for our RL model.\n5. Run minimal tests first\nWhen sampling, you can initially start with minimal sampling parameters, as a ‘proof-of-principle’ approach that is less time and computationally burdening:\nfit &lt;- stan(\n  file = \"model.stan\",\n  chains = 1,\n  iter = 1\n)\nIf this runs without errors, then your model should work with the full sampling parameters. You can create a test mode in your analysis script to initially run models using these settings:\ntest_mode &lt;- TRUE\n\nif(test_mode) {\n  chains &lt;- 1\n  iter &lt;- 1\n} else {\n  chains &lt;- 4\n  iter &lt;- 2000\n}\n6. Debugging by printing\nSimilar to Python, Stan’s print statement can help track model execution. In the reinforcement learning model below:\nfor (s in 1:1) {\n    vector[2] v;\n    real pe;\n    v &lt;- initv;\n    \n    for (t in 1:nTrials) {\n        choice[s,t] ~ categorical_logit(tau[s] * v);\n        \n        print(\"s = \", s, \", t = \", t, \", v = \", v);\n        \n        pe &lt;- reward[s,t] - v[choice[s,t]];\n        v[choice[s,t]] &lt;- v[choice[s,t]] + lr[s] * pe;\n    }\n}\nThe line print(\"s = \", s, \", t = \", t, \", v = \", v); above prints the subject (s), trial (t), and value (v) at each iteration, helping to identify where problems might occur in your model execution."
  },
  {
    "objectID": "workshops/09.debugging/qmd/debugging.html#an-example-of-debugging-in-stan-the-memory-retention-model",
    "href": "workshops/09.debugging/qmd/debugging.html#an-example-of-debugging-in-stan-the-memory-retention-model",
    "title": "Debugging in Stan",
    "section": "An example of debugging in Stan: the memory retention model",
    "text": "An example of debugging in Stan: the memory retention model\nLet’s practically explore debugging techniques through a example model involving memory retention1. The data shown below is for a hypothetical memory experiment where three subjects were tested on their ability to remember 18 items across different time intervals (1, 2, 4, 7, 12, 21, 35, 59, and 99 time units).\n\n\n\n\n\n\n\n\n\nData from the experiment (left) and the number of remembered items plotted against time (right)\n\nLooking at the plot, we can see that the pattern for recall strongly suggests an exponential decay in memory retention.\nTo model this pattern, we’ll use a simplified version of the exponential decay model. The model assumes that the probability of remembering an item after time \\(t\\) is given by:\n\\[θt = min(1.0, exp(-αt) + β)\\]\nThis captures several key aspects of memory retention:\n\nThe \\(α\\) parameter represents the decay rate - how quickly memories fade over time, where larger values of \\(α\\) indicate faster forgetting.\nThe \\(β\\) parameter represents a baseline level of remembering - the asymptotic level of performance that remains even after very long delays. This captures the intuition that some memories persist indefinitely.\nThe \\(min(1.0, ...)\\) ensures that the probability never exceeds 1, which would be mathematically impossible.\n\nWhen modeling this practically, we can use a binomial distribution, because at each time point, we’re essentially dealing with a series of yes/no outcomes (remembered vs. not remembered) across multiple items. Each item has the same probability \\(θt\\) of being remembered at time \\(t\\), and the trials are independent of each other.\nThe number of successes (items remembered) out of \\(n\\) trials (total items) therefore follows a binomial distribution.\n\n\n\n\n\n\nExercise 15\n\n\n\nThe Stan model exp_decay_model.stan has been coded for you, but with several errors, meaning that the code will not run.\n1. Examine the Stan model code. How many problems can you see? (There are 9 in total!)\nHINT: Press the ‘Check’ button iteratively to see if the Stan model is coded correctly after each fix.\n\n\n\nProblems with the Stan model code\n\n\n\n\nThe nine problems with the Stan model code (and how to fix them) are provided below:\n1. There is a missing semi-colon at line 25\n// Problem:\nalpha = Phi_approx( alpha_mu_raw  + alpha_sd_raw * alpha_raw )  // missing ;\n\n// Solution:\nalpha = Phi_approx( alpha_mu_raw  + alpha_sd_raw * alpha_raw );\n2. We cannot give explicit bounds to a variable within the model block in line 30\n// Problem:\nmodel {\n  real&lt;lower=0,upper=1&gt; theta[ns,nt];\n\n// Solution:\nmodel {\n  real theta[ns,nt];\nNote that this is a hierarchical model of memory retention. Therefore, when calculating the probability of remembering items (theta) for each subject and time point, we must use alpha[s] and beta[s] to ensure each subject’s data is modeled using their own individual parameters.\nTo this end, there are two further mistakes on line 45:\n3. There is incorrect indexing for alpha[ns] as alpha[ns] means we’re always using the last subject’s alpha value instead of the current subject’s alpha\n4. There is incorrect indexing for beta which is not indexed at all, but is a subject-specific parameter\nWe therefore need to change these to alpha[s] and beta[s].\n// Problem:\ntheta[s,t] = fmin(1.0, exp(-alpha[ns] * intervals[t]) + beta);\n\n// Solution:\ntheta[s,t] = fmin(1.0, exp(-alpha[s] * intervals[t]) + beta[s]);\n5. The intervals variable is declared incorrectly\nThe declaration int&lt;lower=0&gt; intervals defines intervals as a single integer value, when in fact we need to store multiple time points (1, 2, 4, 7, 12, 21, 35, 59, 99) across nt trials.\n// Problem:\nint&lt;lower=0&gt; intervals;\n\n// Solution:\nint&lt;lower=0&gt; intervals[nt];  // storing each trial\n6. The naming for nItem is not consistent between the data and model block\nYou can change either, so long as it matches with the naming used for the data in the R script. Since this is nItem we will simply rename the data block instance.\n// Problem:\nint&lt;lower=0&gt; nitem;\n\n// Solution:\nint&lt;lower=0&gt; nItem;\n7. The sampling statement has incorrect indexing\nWe need to match each observed count k[s,t] (number of items remembered by subject s at time t) with its corresponding probability theta[s,t], rather than trying to use the entire k matrix with a single probability value.\n// Problem:\nk ~ binomial(nItem, theta[s,t]);\n\n// Solution:\nk[s,t] ~ binomial(nItem, theta[s,t]);\n8. The parameter type is incorrectly specified\nThe group-level parameters alpha_mu and beta_mu represent probabilities that can take any value between 0 and 1, so they need to be declared as real rather than int which would only allow values of 0 or 1.\n// Problem:\nint&lt;lower=0,upper=1&gt; alpha_mu;\nint&lt;lower=0,upper=1&gt; beta_mu;\n\n// Solution:\nreal&lt;lower=0,upper=1&gt; alpha_mu;\nreal&lt;lower=0,upper=1&gt; beta_mu;\n9. The standard deviation parameters need proper constraints and priors\nThe standard deviation parameters (alpha_sd_raw, beta_sd_raw) must be positive, so they need a lower bound of 0. Additionally, we need to specify appropriate priors for these parameters in the model block. A Cauchy(0,3) prior is a good prior as it allows for some heavy tails while still being weakly informative.\n// Problem:\nreal alpha_sd_raw;  // unconstrained\nreal beta_sd_raw;   // unconstrained\n\nalpha_mu_raw ~ normal(0,1);  // missing sd priors\nbeta_mu_raw  ~ normal(0,1);\n\n// Solution:\nreal&lt;lower=0&gt; alpha_sd_raw;  // must be positive\nreal&lt;lower=0&gt; beta_sd_raw;   // must be positive\n\nalpha_mu_raw ~ normal(0,1);\nbeta_mu_raw  ~ normal(0,1);\nalpha_sd_raw ~ cauchy(0,3);  // appropriate prior for sd\nbeta_sd_raw  ~ cauchy(0,3);\nThat covers the errors in the Stan model code, meaning that the model should now run. However, if you actually run the model (the correctly formatted Stan code is exp_decay_model_master.stan) using the R script exp_decay_main.R you will see warning messages:\nWarning messages:\n1: There were 1412 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them. \n2: Examine the pairs() plot to diagnose sampling problems\nNow changing some of Stan sampling parameters to optimise sampling can help further!"
  },
  {
    "objectID": "workshops/09.debugging/qmd/debugging.html#footnotes",
    "href": "workshops/09.debugging/qmd/debugging.html#footnotes",
    "title": "Debugging in Stan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLee, M. D., & Wagenmakers, E. J. (2014). Bayesian cognitive modeling: A practical course. Cambridge university press.↩︎"
  },
  {
    "objectID": "workshops/09.debugging/qmd/workflow.html",
    "href": "workshops/09.debugging/qmd/workflow.html",
    "title": "A principled modeling workflow",
    "section": "",
    "text": "We will now look at a practical example of how computational modeling is applied by examining a modeling study1."
  },
  {
    "objectID": "workshops/09.debugging/qmd/workflow.html#crawley-zhang-2020---modeling-flexible-behaviour-in-autism-spectrum-disorder",
    "href": "workshops/09.debugging/qmd/workflow.html#crawley-zhang-2020---modeling-flexible-behaviour-in-autism-spectrum-disorder",
    "title": "A principled modeling workflow",
    "section": "Crawley & Zhang (2020) - Modeling flexible behaviour in Autism Spectrum Disorder",
    "text": "Crawley & Zhang (2020) - Modeling flexible behaviour in Autism Spectrum Disorder\nThe study aimed to uncover how people learn from feedback and adapt their behaviour, comparing individuals with and without autism spectrum disorder (ASD) across different age groups. They used a probabilistic reversal learning task where participants had to learn through trial and error which of two stimuli was more likely to give them rewards.\n\n\n\n\n\n\n\n\n\nDynamics and reward contingencies of the probabilistic task used in Crawley & Zhang (2020)\n\nThe reward feedback was probabilistic, in that the “correct” choice was rewarded 80% of the time and punished 20% of the time and vice versa for the “incorrect” choice. Participants must also adapt their behaviour when the reward contingencies reversed halfway through the task. Importantly, the time of the reversal was not known to the participants beforehand.\n\n\n\n\n\n\n\n\n\nIndividuals with Autism Spectrum Disorder (ASD) perform worse overall during both phases\n\nThe behavioural results indicated that overall, individuals with ASD performed less accurately than typically developing (TD) individuals during both acquisition and reversal phases. The ASD group also showed significantly more perseverative errors compared to the TD group, meaning that they continued choosing the previously rewarded stimulus despite receiving negative feedback after the reversal. This suggests that autistic individuals have more difficulty updating their behaviour in response to changing environments.\nSubsequently, the authors used computational models to uncover the latent cognitive processes driving the observed behaviour. They implemented three models, all based on the classic Rescorla-Wagner (RW) learning framework:\n1. Counterfactual Update (CU) model - Updates values for both the chosen and unchosen options on each trial\n2. Reward-Punishment (R-P) model - Has separate learning rates for rewards and punishments, but only updates the value of the chosen option\n3. Experience-Weighted Attraction (EWA) Model2 - Incorporates how much experience you’ve had with the task. As you gain experience, new information is weighted less heavily, creating a natural decrease in the learning rate over time.\nThe results are shown below:\n\n\n\n\n\n\n\n\nUltimately, modeling analyses revealed that different age groups were best characterized by different models:\n\nChildren were best fit by the CU model\nAdolescents were best fit by the R-P model\nAdults were best fit by the EWA model\n\nImportantly, while ASD and TD groups used the same types of strategies within each age group, when examining the learning rate in children, the ASD group showed less optimal parameter values.\n\n\n\n\n\n\n\n\n\nLearning rates in the task are less optimal for ASD compared to Typically Developing (TD) children\n\nThis suggests the core mechanisms of learning are similar, but their implementation differs, leading to the observed behavioural differences."
  },
  {
    "objectID": "workshops/09.debugging/qmd/workflow.html#model-recovery-and-parameter-recovery-in-crawley-zhang-2020",
    "href": "workshops/09.debugging/qmd/workflow.html#model-recovery-and-parameter-recovery-in-crawley-zhang-2020",
    "title": "A principled modeling workflow",
    "section": "Model recovery and parameter recovery in Crawley & Zhang (2020)",
    "text": "Model recovery and parameter recovery in Crawley & Zhang (2020)\nTwo key steps of any modeling analysis are model recovery and parameter recovery. Recall that parameter recovery tests if we can recover known parameter values when we fit our model to simulated data. This helps to validate that our model can reliably estimate its parameters. On-the-other-hand, model recovery helps to validate that our models are distinguishable from each other, and to identify potential co-linearity between different models.\nWhen we see overlap between models (co-linearity), it suggests they might be capturing similar underlying processes, making it harder to make strong claims about which cognitive mechanism is actually at play.\nYou can see the process and example results for model recovery for the study below:\n\n\n\n\n\n\n\n\n\nThe model recovery process involving three candidate models\n\nThe diagram on the left demonstrates the model recovery process consisting of three steps:\n1. Generative Process: Each model (M1, M2, M3) generates its own dataset (D1, D2, D3) using chosen “ground truth” parameters\n2. Fitting Process: Each dataset is then fit with all models (shown by the arrows connecting datasets to models)\n3. Recovery Analysis: We analyze how well each model fits each dataset\nThe right panel shows the model recovery matrix for the three models from the paper (CU, RP, EWA), where rows represent the fitted models, columns represent the data-generating models and colours indicate fit quality (yellow = good fit, purple = poor fit).\n\nIdeally, we want a diagonal pattern (with good fits only on the diagonal). In this case we can see the CU model’s data (first column) is uniquely fit by the CU model (yellow square), the RP model’s data shows some overlap with other models (less distinct diagonal) and the EWA model shows reasonable identifiability.\n\nConversely with model recovery, parameter recovery is applied to each individual model, generating synthetic data from the model using known (“true”) parameter values, fitting each model to this synthetic data to estimate parameters and then comparing the estimated parameters to the known true values.\nAn example of parameter recovery - from a different study3 - is shown below:\n\n\n\n\n\n\n\n\n\nParameter recovery results for parameters of the winning model from Zhang & Gläscher (2020)\n\nThe figure shows two key visualizations of parameter recovery:\nGroup-Level Parameter Distributions (left)\n\nDepicts histograms of posterior samples for each parameter, with blue bars representing the distribution of recovered parameter values and red vertical lines show the true parameter values used to generate the data.\nGood recovery in this plot is indicated by posterior distributions centered around the true values\n\nIndividual-Level Parameter Recovery (right)\n\nFeatures scatter plots comparing true (x-axis) vs recovered (y-axis) parameter values. Each point represents one simulated participant and the diagonal gray line represents perfect recovery\nGood recovery is shown by points clustering tightly along the diagonal line\n\nIn this example, we see generally good parameter recovery as the posterior distributions in Panel A are generally centered around the true values and the scatter plots in Panel B show strong correlations between true and recovered values.\n\n\n\n\n\n\nParameter recovery and model structure\n\n\n\nNote that parameter recovery was performed at both the group and participant level in the example above. This is because the authors used a hierarchical model. In non-hierarchical models, one can only recover individual-level parameters!\n\n\nThe pipeline for conducting a modeling analysis in Stan is both systematic and rigorous. Depicted below is the example pipeline for the modeling analysis conducted in Crawley & Zhang (2020), from raw data to final model selection:\n\n\n\n\n\n\n\n\n\nThe Stan modeling pipeline for Crawley & Zhang (2020)\n\nThis pipeline, when applied more generally, can be summarised into 10 distinct steps:\n1. Raw data: We begin with raw behavioural data collected from your experiment. This could be choice data, reaction times, learning rates, etc.\n2. Data preprocessing (prl_prep_data): Format the raw data needs appropriately by cleaning, organizing, and structuring the data in a way that’s compatible with Stan.\n3. Behavioural analysis (prl_behav): Before modeling the data, perform standard behavioural analyses to provide a baseline understanding of your data and effects that your models should be able to capture.\n4. Model development (model1.stan, model2.stan, model3.stan): Create multiple .stan models are developed to test different hypotheses about the cognitive processes underlying your data.\n5. Model fitting (prl_run_model): Write a central function in R which runs the Stan models with your data.\n6. Model diagnosis (model_diag): After the model(s) have ran, evaluate them by checking model convergence, examining parameter distributions and running other diagnostics.\n7. Model validation (prl_recovery): Validate the model through parameter recovery (whether the model can accurately recover known parameters from simulated data) and posterior predictive checks (whether the model can generate data that matches key patterns in your observed data).\n8. Model comparison (prl_model_weight): Compare different models to determine which best explains your data, typically using information criteria, exceedance probability or Bayes factors.\n9. Storing results (prl_winning.Rdata): Save the winning model and its results; for reproducibility share openly on OSF or GitHub alongside publication.\n10. Model recovery (prl_par_corr): Perform model recovery to identify co-linearity between different models."
  },
  {
    "objectID": "workshops/09.debugging/qmd/workflow.html#footnotes",
    "href": "workshops/09.debugging/qmd/workflow.html#footnotes",
    "title": "A principled modeling workflow",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCrawley, D., Zhang, L., Jones, E. J., Ahmad, J., Oakley, B., San José Cáceres, A., … & EU-AIMS LEAP group. (2020). Modeling flexible behavior in childhood to adulthood shows age-dependent learning mechanisms and less optimal learning in autism in each age group. PLoS biology, 18(10), e3000908.↩︎\nCamerer, C., & Hua Ho, T. (1999). Experience‐weighted attraction learning in normal form games. Econometrica, 67(4), 827-874.↩︎\nZhang, L., & Gläscher, J. (2020). A brain network supporting social influences in human decision-making. Science advances, 6(34), eabb4159.↩︎"
  },
  {
    "objectID": "workshops/03.bernoulli_coin/qmd/intro.html",
    "href": "workshops/03.bernoulli_coin/qmd/intro.html",
    "title": "Bernoulli and linear regression models",
    "section": "",
    "text": "Welcome to the fifth workshop of the BayesCog course!\nHaving introduced basics of Stan with our binomial model in the previous workshop, we will now implement two new types of models: the Bernoulli model and linear regression. We will understand how the Bernoulli is linked to the binomial model, describing the same underlying process (binary outcomes) but at an individual trial level. Meanwhile, linear regression will introduce us to models with multiple parameters and continuous outcomes. In doing so, we will also build our knowledge of the Stan language further, including variable declaration, control flow and variable scope.\nThe goals of this workshop are to:\n\nLearn how to implement a Bernoulli model in Stan for modeling binary outcome data\nRe-think linear regression as a Bayesian model, implementing it in Stan\nGain a deeper understanding of Stan’s programming features, including vectorization and efficient coding practices\nRun model validation using posterior predictive checks and diagnostic tools\n\n\n\n\n\n\n\nWorking directory for this workshop\n\n\n\nModel code and R scripts for this workshop are located in the (/workshops/03.bernoulli_coin) and (/workshops/04.regression_height) directories. Set your working directory to (/workshops/03.bernoulli_coin) to begin with. Remember to use the R.proj file within each folder to avoid manually setting directories!\n\n\nThe copy of this workshop notes can be found on the course GitHub page."
  },
  {
    "objectID": "workshops/01.R_basics/qmd/intro.html",
    "href": "workshops/01.R_basics/qmd/intro.html",
    "title": "Introduction to R/Rstudio",
    "section": "",
    "text": "Welcome to the first workshop of the BayesCog course!\nThe first workshop is designed to primarily introduce those without any previous experience to R and RStudio so that everyone is on the same page for the subsequent workshops. We will also cover tips and tricks with working in R, so it may also be useful for those already familiar!\nTopics for this workshop include:\n\nUnderstanding R and RStudio’s interface and functionality\nBasic R operations and data types\nWorking with variables and functions\nData structures and manipulation\nControl flow and logical operators\nUsing R packages\nData visualization with ggplot2\n\n\n\n\n\n\n\nWorking directory for this workshop\n\n\n\nModel code and R scripts for this workshop are located in the (/workshops/01.R_basics) directory. Remember to use the R.proj file within each folder to avoid manually setting directories!\n\n\nThe copy of this workshop notes can be found on the course GitHub page."
  },
  {
    "objectID": "workshops/01.R_basics/qmd/probability.html",
    "href": "workshops/01.R_basics/qmd/probability.html",
    "title": "Probability and Bayes’ theorem",
    "section": "",
    "text": "Probability is a mathematical way of describing how likely an outcome or event is to occur. In other words, we assign numbers to a set of possibilities associated with an event.\nProbability has several key properties1:\n\nThe probability \\(p\\) of an event must be between 0 and 1\n\n\\[\np \\in [0,1]\n\\]\n\nThe sum of all outcomes associated with an event should be 1\n\n\\[\n\\sum p = 1\n\\]\n\n\nThere are two types of probability, discrete and continuous, which depend on the type of event.\nIn the event of a fair coin flip, there are two discrete outcomes: heads or tails. If we roll a fair die, there are six possible discrete outcomes, 1, 2, 3, 4, 5 or 6.\nWhen the event is discrete, we describe probability as it’s mass, using a probability mass function (PMF). In the example below2, we’re looking at test scores where students can only get whole numbers of questions correct (0 to 8 correct answers). The graph shows the probability mass for each possible outcome, where each bar represents the probability of a student getting exactly that number of correct answers. For instance, we can see that getting 6 correct answers has the highest probability (around 0.3 or 30%).\n\n\n\n\n\n\nWe can use this PMF to calculate various probabilities, such as:\n\nThe probability of a student getting exactly 5 correct (height of the bar at 5)\nThe probability of getting 5 or more correct (sum of bars from 5 to 8)\nThe probability of getting fewer than 4 correct (sum of bars from 0 to 3)\n\nWhen dealing with continuous probability, where outcomes can take any value within a range (like height, weight, or reaction times), we use density functions to describe probability.\nThere are two key functions which help us work with continuous probabilities:\nThe Probability Density Function (PDF):\n\nShows the relative likelihood of different values occurring\nThe area under the curve between any two points represents the probability of observing a value in that range\nThe total area under the entire curve must equal 1\nWhile the y-axis values can exceed 1, they aren’t probabilities themselves, they are arbitrary values\n\nThe Cumulative Distribution Function (CDF):\n\nShows the probability of observing a value less than or equal to any given point\nAlways increases from 0 to 1\nUseful for finding probabilities of ranges and percentiles\n\n\n\n\n\n\n\n\n\n\nThe PDF and CDF for a normal distribution at two points (65 and 70)\n\nFor example, the graph above3 depicts both the CDF and PDF for some normally distributed data. You can see how the density of the PDF and the value of the CDF both change when we calculate the probability of a value being less than 65 and then 70.\n\n\n\n\n\n\nThe probability of exact values\n\n\n\nRemember that unlike discrete events, the probability of any exact value in a continuous distribution is actually zero - we must instead talk about ranges of values.\n\n\nHere is another example of the difference between discrete and continuous probability:\n\n\n\n\n\n\n\n\nFor discrete data, let’s assume we have a bag of marbles, each labelled with a number from 1 to 100. We then reach into the bag, pull out a marble and place it back into the bag. In this case, we have a total of 100 possible events (\\(X\\)), each with a discrete probability of:\n\\[\nP(X=x) = \\frac{1}{100}\n\\] The sum of all possible events is therefore:\n\\[\n\\sum_{x=1}^{100} P(X=x) = 1\n\\]\nIn the continuous data example, let’s assume we randomly sample the height \\((H)\\) of a number of people in the population. In a continuous variable like height, there are infinitely many possible values between, say, 1.75m and 1.85m. These values can be expressed with infinite precision (e.g., 1.81, 1.801, 1.800000000000…1). Because there are infinitely many values, the probability of any single, exact value (like exactly 1.8m) is zero.\nInstead, we can calculate the probability of having a height between two values (e.g., between 1.75m and 1.85m), by calculating the area under the curve of the PDF over that range using integration:\n\\[\nP(1.75 \\leq H \\leq 1.85) = \\int_{1.75}^{1.85} f(h)dh\n\\]\nwhere \\(f(h)\\) is the probability density function for height.\n\n\n\nThere are several probability functions in R that we can use. The example below demonstrates their use for a normal distribution with mean 0 and standard deviation 1. These four R functions help us work with this distribution in different ways:\n\n\n\n\n\n\n\n\ndnorm() - Density Function: Shows the probability density at any point\npnorm() - Cumulative Probability Function: Shows the probability of getting a value less than or equal to x\nqnorm() - Quantile Function: Given a probability, tells you the corresponding x-value\nrnorm() - Random Number Generator: Generates random numbers following the normal distribution, plotted using a histogram"
  },
  {
    "objectID": "workshops/01.R_basics/qmd/probability.html#introduction-to-probability",
    "href": "workshops/01.R_basics/qmd/probability.html#introduction-to-probability",
    "title": "Probability and Bayes’ theorem",
    "section": "",
    "text": "Probability is a mathematical way of describing how likely an outcome or event is to occur. In other words, we assign numbers to a set of possibilities associated with an event.\nProbability has several key properties1:\n\nThe probability \\(p\\) of an event must be between 0 and 1\n\n\\[\np \\in [0,1]\n\\]\n\nThe sum of all outcomes associated with an event should be 1\n\n\\[\n\\sum p = 1\n\\]\n\n\nThere are two types of probability, discrete and continuous, which depend on the type of event.\nIn the event of a fair coin flip, there are two discrete outcomes: heads or tails. If we roll a fair die, there are six possible discrete outcomes, 1, 2, 3, 4, 5 or 6.\nWhen the event is discrete, we describe probability as it’s mass, using a probability mass function (PMF). In the example below2, we’re looking at test scores where students can only get whole numbers of questions correct (0 to 8 correct answers). The graph shows the probability mass for each possible outcome, where each bar represents the probability of a student getting exactly that number of correct answers. For instance, we can see that getting 6 correct answers has the highest probability (around 0.3 or 30%).\n\n\n\n\n\n\nWe can use this PMF to calculate various probabilities, such as:\n\nThe probability of a student getting exactly 5 correct (height of the bar at 5)\nThe probability of getting 5 or more correct (sum of bars from 5 to 8)\nThe probability of getting fewer than 4 correct (sum of bars from 0 to 3)\n\nWhen dealing with continuous probability, where outcomes can take any value within a range (like height, weight, or reaction times), we use density functions to describe probability.\nThere are two key functions which help us work with continuous probabilities:\nThe Probability Density Function (PDF):\n\nShows the relative likelihood of different values occurring\nThe area under the curve between any two points represents the probability of observing a value in that range\nThe total area under the entire curve must equal 1\nWhile the y-axis values can exceed 1, they aren’t probabilities themselves, they are arbitrary values\n\nThe Cumulative Distribution Function (CDF):\n\nShows the probability of observing a value less than or equal to any given point\nAlways increases from 0 to 1\nUseful for finding probabilities of ranges and percentiles\n\n\n\n\n\n\n\n\n\n\nThe PDF and CDF for a normal distribution at two points (65 and 70)\n\nFor example, the graph above3 depicts both the CDF and PDF for some normally distributed data. You can see how the density of the PDF and the value of the CDF both change when we calculate the probability of a value being less than 65 and then 70.\n\n\n\n\n\n\nThe probability of exact values\n\n\n\nRemember that unlike discrete events, the probability of any exact value in a continuous distribution is actually zero - we must instead talk about ranges of values.\n\n\nHere is another example of the difference between discrete and continuous probability:\n\n\n\n\n\n\n\n\nFor discrete data, let’s assume we have a bag of marbles, each labelled with a number from 1 to 100. We then reach into the bag, pull out a marble and place it back into the bag. In this case, we have a total of 100 possible events (\\(X\\)), each with a discrete probability of:\n\\[\nP(X=x) = \\frac{1}{100}\n\\] The sum of all possible events is therefore:\n\\[\n\\sum_{x=1}^{100} P(X=x) = 1\n\\]\nIn the continuous data example, let’s assume we randomly sample the height \\((H)\\) of a number of people in the population. In a continuous variable like height, there are infinitely many possible values between, say, 1.75m and 1.85m. These values can be expressed with infinite precision (e.g., 1.81, 1.801, 1.800000000000…1). Because there are infinitely many values, the probability of any single, exact value (like exactly 1.8m) is zero.\nInstead, we can calculate the probability of having a height between two values (e.g., between 1.75m and 1.85m), by calculating the area under the curve of the PDF over that range using integration:\n\\[\nP(1.75 \\leq H \\leq 1.85) = \\int_{1.75}^{1.85} f(h)dh\n\\]\nwhere \\(f(h)\\) is the probability density function for height.\n\n\n\nThere are several probability functions in R that we can use. The example below demonstrates their use for a normal distribution with mean 0 and standard deviation 1. These four R functions help us work with this distribution in different ways:\n\n\n\n\n\n\n\n\ndnorm() - Density Function: Shows the probability density at any point\npnorm() - Cumulative Probability Function: Shows the probability of getting a value less than or equal to x\nqnorm() - Quantile Function: Given a probability, tells you the corresponding x-value\nrnorm() - Random Number Generator: Generates random numbers following the normal distribution, plotted using a histogram"
  },
  {
    "objectID": "workshops/01.R_basics/qmd/probability.html#joint-marginal-and-conditional-probability",
    "href": "workshops/01.R_basics/qmd/probability.html#joint-marginal-and-conditional-probability",
    "title": "Probability and Bayes’ theorem",
    "section": "Joint, marginal and conditional probability",
    "text": "Joint, marginal and conditional probability\nSo far, we have only discussed the probability of a single event, i.e., a coin can either be a head or tail, height can be between two values etc. However, in the real world, there are often complex situations where we do not have one type of event, we might have multiple events.\nWhen working with multiple events, we need to understand three important probability concepts:\n\nJoint probability\n\n\n\n\nJoint probability represents the likelihood of two events occurring together.\nFor any two events \\(A\\) and \\(B\\), we write this as \\(p(A,B)\\), which equals \\(p(B,A)\\).\nFor example, if we’re interested in weather conditions, \\(p(\\text{rain}, \\text{cold})\\) represents the probability that it is both raining and cold. This is the probability of rain intersecting with the probability of cold weather.\n\nMarginal probability\n\n\n\n\nMarginal probability is the probability of one event occurring, regardless of the other event. We write this as \\(p(A)\\) for event \\(A\\), representing the probability of \\(A\\) irrespective of \\(B\\).\nUsing our weather example, \\(p(\\text{rain})\\) represents the overall probability of rain, regardless of temperature.\nMathematically, we can express this as:\n\\[p(\\text{rain}) = p(\\text{rain}, \\text{cold}) + p(\\text{rain}, \\text{not cold})\\]\n\nConditional probability\n\n\n\n\nConditional probability is the probability of one event occurring, given that another event has already occurred. We write this as \\(p(A|B)\\), representing the probability of \\(A\\) given that \\(B\\) has occurred.\nThe fundamental equation relating joint and conditional probability is:\n\\[p(A,B) = p(A|B)p(B)\\]\nIn our weather example, \\(p(\\text{rain}|\\text{cold})\\) represents the probability of rain given that it is cold. This gives us:\n\\[p(\\text{rain}, \\text{cold}) = p(\\text{rain}|\\text{cold})p(\\text{cold})\\]\nThis relationship helps us understand how these different types of probability are connected and how we can calculate one from the others.\nLet’s further look at how these probabilities work using concrete examples, where X represents rain (1 = rain, 0 = no rain) and Y represents cold (1 = cold, 0 = not cold).\n\n\n\n\n\n\nJoint Probability\n\n\n\n\nFrom the table, we can read the joint probabilities directly:\n\\(P(X = 0, Y = 1) = 0.1\\) (probability of no rain and cold)\n\\(P(X = 1, Y = 1) = 0.5\\) (probability of rain and cold)\n\\(P(X = 1, Y = 0) = 0.1\\) (probability of rain and not cold)\n\\(P(X = 0, Y = 0) = 0.3\\) (probability of no rain and not cold)\nNote that these sum to 1: 0.1 + 0.5 + 0.1 + 0.3 = 1\n\nMarginal Probability\n\n\n\n\nTo find marginal probabilities, we sum across rows or columns:\nFor Y = 1 (cold):\n\\(P(Y = 1) = 0.5 + 0.1 = 0.6\\) (sum across the top row)\nFor X = 0 (no rain):\n\\(P(X = 0) = 0.1 + 0.3 = 0.4\\) (sum across the second column)\n\nConditional Probability\n\n\n\n\nTo find conditional probabilities, we use the formula:\n\\[\nP(X|Y) = \\frac{P(X,Y)}{P(Y)}\n\\]\nFor example, \\(P(X = 1|Y = 1)\\) (probability of rain given it’s cold):\n\\[\nP(X = 1|Y = 1) = \\frac{P(X = 1, Y = 1)}{P(Y = 1)} = \\frac{0.5}{0.6} = 0.833\n\\]\nThis means that when it’s cold, there’s about an 83.3% chance of rain.\n\nUnderstanding continuous probability distributions\nUnlike discrete probabilities where we can create a simple table of all possible outcomes, continuous events deal with variables that can take on any value within a range.\nFor these events we instead calculate the same three probabilities that we used for discrete events, but as distributions4.\n\n\n\n\n\n\nJoint distribution\n\n\n\n\nWhen we have two continuous variables (x and y), their joint distribution shows how they vary together. The brighter/more intense regions show where combinations of x and y values are more likely to occur.\n\n\n\n\n\n\n\n\n\nConditional distribution\n\n\n\n\nThe conditional distribution shows how one variable is distributed when we fix the other variable at specific values. The shape of these distributions can change depending on which y-value we’re looking at, showing how the two variables are related.\n\n\n\n\n\n\n\n\n\nMarginal distribution\n\n\n\n\nThe marginal distributions (shown on the sides of the joint distribution) represent the overall distribution of each variable independently, regardless of the other variable’s value. Mathematically, these are obtained by integrating the joint distribution over all values of the other variable."
  },
  {
    "objectID": "workshops/01.R_basics/qmd/probability.html#bayes-theorem",
    "href": "workshops/01.R_basics/qmd/probability.html#bayes-theorem",
    "title": "Probability and Bayes’ theorem",
    "section": "Bayes’ theorem",
    "text": "Bayes’ theorem\nSo, now that we know how we calculate the probabilities of continuous events, we can move on to describe Bayes’ theorem.\nBayes’ theorem is a fundamental concept in probability theory that helps us update our beliefs about events based on new evidence. It builds directly on the concepts of conditional, joint, and marginal probability that we discussed earlier.\nTo get a better understanding of what Bayes’ theorem tell us, let’s derive it step by step using what we know about probability:\nWe start with the equation for joint probability that we saw earlier:\n\\[\np(A,B) = p(A|B)p(B)\n\\]\nWe also know that joint probability is symmetric:\n\\[\np(A,B) = p(B,A)\n\\]\nTherefore:\n\\[\np(B,A) = p(B|A)p(A)\n\\]\nAnd since \\(p(A,B) = p(B,A)\\), we can write:\n\\[\np(A|B)p(B) = p(B|A)p(A)\n\\]\nSolving for \\(p(A|B)\\), we get Bayes’ theorem:\n\\[\np(A|B) = \\frac{p(B|A)p(A)}{p(B)}\n\\]\nEach term in Bayes’ theorem has a specific interpretation:\n\n\\(p(A)\\) is the prior probability - our initial belief about event A\n\\(p(B|A)\\) is the likelihood - probability of observing the evidence if A is true\n\\(p(B)\\) is the marginal likelihood - total probability of observing the evidence\n\\(p(A|B)\\) is the posterior probability - what we want to know after observing evidence\n\nSo what Bayes’ theorem tells us is that we can find the probability of an event A happening, given that we observed event B, by using our prior knowledge about both events. It’s equivalent to updating what we believe based on new information.\n\n\n\n\n\n\nBayes’ in a nutshell\n\n\n\nYou can think of Bayes’ theorem as starting with an initial guess about something (prior), seeing some evidence (likelihood), and then making a better guess (posterior) based on that evidence.\n\n\nFor example, let’s return to our weather scenario. If we want to know the probability it’s cold given that it’s raining, we would use:\n\\[\np(\\text{cold}|\\text{rain}) = \\frac{p(\\text{rain}|\\text{cold})p(\\text{cold})}{p(\\text{rain})}\n\\]\nThis allows us to update our belief about the temperature based each new observation of rain.\nLet’s explore how Bayes’ theorem works in practice using a real-world example of the relationship between eye and hair colour in a population5.\n\n\n\n\n\n\n\n\nThe table above displays both joint probabilities for each combination and marginal probabilities for each characteristic, which is exactly what we need for Bayes’ theorem:\nThe joint probabilities \\(p(A,B)\\) show us how likely each specific combination is. For instance:\n\nHaving brown eyes and black hair: \\(p(\\text{brown},\\text{black}) = 0.11\\)\nHaving blue eyes and blond hair: \\(p(\\text{blue},\\text{blond}) = 0.16\\)\n\nThe marginal probabilities in the table give us \\(p(A)\\) and \\(p(B)\\):\n\nFor hair color (bottom row): \\(p(\\text{brunette}) = 0.48\\), \\(p(\\text{blond}) = 0.21\\), etc.\nFor eye color (right column): \\(p(\\text{brown}) = 0.37\\), \\(p(\\text{blue}) = 0.36\\), etc.\n\n\n\n\n\n\n\n\n\nUsing these values, we can then apply Bayes’ theorem, for example, to find the probability of blonde hair given blue eyes:\n\\[\np(\\text{blond}|\\text{blue}) = \\frac{p(\\text{blue}|\\text{blond})p(\\text{blond})}{p(\\text{blue})} = \\frac{0.16}{0.36} = 0.45\n\\]\n\n\n\n\n\n\nWhat did we just do?\n\n\n\nTo summarise, we have gone from the “prior” (marginal) beliefs about hair colour before knowing eye colour, to the “posterior” (conditional) beliefs about hair color given the observed eye colour. Without knowing the eye colour, the probability of blonde hair in the population is 0.21. But knowing that the eyes are blue, the probability of blonde hair is 0.45.\n\n\n\nExercise 4Answer\n\n\nWe will now tackle a problem posed in the excellent textbook: Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan, by John Kruschke.\n“Suppose that in the general population, the probability of having a rare disease is 1/1000. We denote the true presence or absence of the disease as the value of a parameter, \\(\\theta\\), that can have the value \\(\\theta = ☹\\) if disease is present in a person, or the value \\(\\theta = ☺\\) if the disease is absent.\nThe base rate of the disease is therefore denoted \\(p(\\theta = ☹) = 0.001\\).\nSuppose(1): a test for the disease that has a 99% hit rate: \\(p(T=+|\\theta = ☹) = 0.99\\)\nSuppose(2): the test has a false alarm rate of 5%: \\(p(T=+|\\theta = ☺) = 0.05\\)\nQ: Suppose we sample a person at random from the population, administer the test, and it comes up positive. What is the posterior probability that the person has the disease?“\n\n\nLet’s solve this step by step using Bayes’ theorem to find \\(p(\\theta = ☹|T=+)\\):\nFirst, let’s recall Bayes’ theorem: \\[\np(A|B) = \\frac{p(B|A)p(A)}{p(B)}\n\\] In our case: \\[\np(\\theta = ☹|T=+) = \\frac{p(T=+|\\theta = ☹)p(\\theta = ☹)}{p(T=+)}\n\\] We know a few things already:\n\nPrior probability: \\(p(\\theta = ☹) = 0.001\\)\nHit rate: \\(p(T=+|\\theta = ☹) = 0.99\\)\nFalse alarm rate: \\(p(T=+|\\theta = ☺) = 0.05\\)\n\nSo, we have: \\[\np(\\theta = ☹|T=+) = \\frac{(0.99)(0.001)}{p(T=+)}\n\\] Meaning that we still need to calculate \\(p(T=+)\\) (total probability of getting a positive test result) in the denominator: \\[\np(T=+) = p(T=+|\\theta = ☹)p(\\theta = ☹) + p(T=+|\\theta = ☺)p(\\theta = ☺)\n\\]\nWhich gives: \\[p(T=+) = (0.99)(0.001) + (0.05)(0.999) = 0.00099 + 0.04995 = 0.05094\\] Now we can plug everything into Bayes’ theorem: \\[p(\\theta = ☹|T=+) = \\frac{(0.99)(0.001)}{0.05094} = 0.0194\\] Therefore, even though the test came back positive, there is only about a 1.94% chance that the person actually has the disease.\nThis surprising result shows that even with a very accurate test (99% hit rate), if the condition is very rare (0.1% of population), most positive results will actually be false alarms!"
  },
  {
    "objectID": "workshops/01.R_basics/qmd/probability.html#footnotes",
    "href": "workshops/01.R_basics/qmd/probability.html#footnotes",
    "title": "Probability and Bayes’ theorem",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKolmogorov, A. N., & Bharucha-Reid, A. T. (2018). Foundations of the theory of probability: Second English Edition. Courier Dover Publications.↩︎\nFarrell, S., & Lewandowsky, S. (2018). Computational modeling of cognition and behavior. Cambridge University Press.↩︎\nWolfram Demonstrations Project. Connecting the CDF and the PDF.↩︎\nMa, E. J. (2018). Joint, conditional, and marginal probability distributions. Eric J. Ma’s Blog. Retrieved from https://ericmjl.github.io/blog/2018/8/7/joint-conditional-and-marginal-probability-distributions↩︎\nKruschke, J. (2014). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan.↩︎"
  },
  {
    "objectID": "workshops/01.R_basics/qmd/intro_probability.html",
    "href": "workshops/01.R_basics/qmd/intro_probability.html",
    "title": "Probability and an introduction to Bayes’ theorem",
    "section": "",
    "text": "Welcome to the second workshop of the BayesCog course!\nIn this workshop, we’ll explore the fundamental concepts of probability and introduce Bayes’ theorem, which forms the foundation of Bayesian statistics. We’ll start with basic probability concepts and gradually build up to understanding how a Bayesian approach to probability involves updating our beliefs based on evidence.\nTopics for this workshop include:\n\nUnderstanding discrete and continuous probability\nWorking with probability distributions in R\nUnderstanding the concepts of joint, marginal, and conditional probability\nAn introduction to Bayes’ theorem and its applications\n\n\n\n\n\n\n\nWorking directory for this workshop\n\n\n\nModel code and R scripts for this workshop are located in the (/workshops/01.R_basics) directory. Remember to use the R.proj file within each folder to avoid manually setting directories!\n\n\nThe copy of this workshop notes can be found on the course GitHub page."
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/binomial_model.html",
    "href": "workshops/02.binomial_globe/qmd/binomial_model.html",
    "title": "The Globe-Tossing experiment",
    "section": "",
    "text": "Let’s now apply this knowledge practically to solve a ‘real-world’ problem. We will specifically be taking the ‘globe-tossing’ exercise from Richard McElreath’s book: Statistical rethinking: A Bayesian course with examples in R and Stan1.\n“Suppose you have a globe representing our planet, the Earth. This version of the world is small enough to hold in your hands. You are curious how much of the surface is covered in water. You adopt the following strategy: You will toss the globe up in the air. When you catch it, you will record whether or not the surface under your right index finger is water or land. Then you toss the globe up in the air again and repeat the procedure. This strategy generates a sequence of surface samples from the globe.\nThe first nine samples might look like:\n\\[W L W W W L W L W\\]\nwhere \\(W\\) indicates water and \\(L\\) indicates land.\nIn this example you observe six \\(W\\) (water) observations and three \\(L\\) (land) observations. Call this sequence of observations the data.”\nSo, the question is, “What proportion of the globe is covered in water and land?”\nFirstly, let’s look at the frequentist perspective. The frequentist treats probability as the long-run frequency of events in repeated experiments.\nIn our globe-tossing example, this means:\n\\[p(\\text{water}) = \\lim_{n \\to \\infty} \\frac{\\text{number of water observations}}{n}\\]\nwhere \\(n\\) is the total number of tosses.\nFor our observed data of 9 tosses with 6 water observations, a frequentist would therefore calculate:\n\\[\\hat{p}(\\text{water}) = \\frac{6}{9} \\approx 0.67\\]\nThis suggests approximately 67% of the Earth’s surface is water - \\(\\hat{p}\\) indicates this is an estimate of the true proportion.\nOur goal is to make a similar estimate using the Bayesian framework."
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/binomial_model.html#constructing-the-bayesian-globe-tossing-model",
    "href": "workshops/02.binomial_globe/qmd/binomial_model.html#constructing-the-bayesian-globe-tossing-model",
    "title": "The Globe-Tossing experiment",
    "section": "Constructing the Bayesian globe-tossing model",
    "text": "Constructing the Bayesian globe-tossing model\nMcElreath suggests that when constructing any model (including Bayesian), one should incorporate three steps:\n\n\n\n\n\n\n\n\n1. Data story: Motivate the model by narrating how the data might arise.\n2. Update: Educate your model by feeding it the data.\n3. Evaluate: All statistical models require supervision, leading possibly to model revision.\nIn the case that the model needs revising, then we generate a new data story.\nPractically, this workflow may look like this:\n\n\n\n\n\n\n\n\n\nWorkflow diagram of Bayesian model development and analysis, implemented in Stan\n\nWe start with data from our experiment. From our earlier workshop, we know to firstly clean and reshape the data where appropriate (for example by removing missing trials), and to do some exploratory analysis.\nThe upper portion shows the model. This is our data story, i.e., we define our data, parameters and the model structure.\nThe central red box represents the sampling methods that approximate the posterior. This is where the actual model fitting occurs using methods like Markov Chain Monte Carlo, which we explore in future workshops.\nThe end goal of modeling is to determine some likelihood for our latent parameter given the observed data. We can evaluate and refine our modeling at various stages through model diagnosis, evaluation and revision.\nWith this process now in mind, let’s construct our data story.\nLet’s firstly make a number of statements about the parameter of interest and how the data are generated. We can state that:\n\nThere is some true proportion of water, \\(\\theta\\), covering the globe.\nA single toss of the globe has a probability \\(p\\) of producing a water \\((W)\\) observation. It has a probability \\(1 - \\theta\\) of producing a land \\((L)\\) observation.\nEach toss of the globe is independent of the others.\n\nThis is our story. The next stage is to then translate this story into a formal probability model.\nLet’s start with Bayes’ theorem as applied to our globe-tossing example:\n\\[p(\\theta|D) = \\frac{p(D|\\theta)p(\\theta)}{p(D)}\\] To determine the likelihood \\(p(D|\\theta)\\), we need to consider our data-generating process:\n\nEach toss is independent\nEach toss has only two possible outcomes (water or land)\nThe probability of water (\\(\\theta\\)) remains constant across tosses\n\nWe’re interested in the total number of successes (water observations) in a fixed number of trials.\nYou may have thought that this case is very similar to a coin flip, and that’s because it is! The data generating process is the same, whereby each trial can produce one of two outcomes, and are independent of one another. We can therefore use the binomial distribution to determine the likelihood.\nFor our data \\(D\\) consisting of \\(W\\) water observations out of \\(N\\) total tosses, the likelihood is therefore:\n\\[p(D|\\theta) = p(W|N,\\theta) = \\binom{N}{W}\\theta^W(1-\\theta)^{N-W}\\] This binomial likelihood represents the probability of observing exactly \\(W\\) water outcomes in \\(N\\) tosses, given some value of \\(\\theta\\).\nFor our specific example with 6 water observations out of 9 tosses:\n\n\\(N = 9\\) (total tosses)\n\\(W = 6\\) (water observations)\n\nOur likelihood function is:\n\\[p(D|\\theta) = \\binom{9}{6}\\theta^6(1-\\theta)^{3}\\]\nThis function tells us, for any proposed value of \\(\\theta\\), how likely we were to observe our actual data of 6 water observations in 9 tosses.\nHere’s a summary of this process:\n\n\n\n\n\n\n\n\n\nFrom our data-generating process, we declare the appropriate variables and the likelihood function linking data to parameters"
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/binomial_model.html#bayesian-updating",
    "href": "workshops/02.binomial_globe/qmd/binomial_model.html#bayesian-updating",
    "title": "The Globe-Tossing experiment",
    "section": "Bayesian updating",
    "text": "Bayesian updating\nRemember the Bayesian maxim from the last workshop:\n\n“Probability is orderly opinion and inference from data is nothing other than the revision of such opinion in the light of relevant new information.”\n\nSo, in our example, how might our estimates of \\(\\theta\\) change (be updated) trial by trial?\nBefore the first globe-toss, we have some initial plausibility of each possible value of \\(\\theta\\). This may be a uniform prior, given that we do not have any reason to think it is biased towards a particular value. In the graph below, this is represented by the dashed horizontal line. However, after seeing the first toss, which is a \\(W\\), the model updates the plausibilities to the solid line.\n\n\n\n\n\nThe plausibility of \\(\\theta = 0\\) has now fallen to exactly zero — the equivalent of “impossible” because we now know there is some water. Likewise, the plausibility of \\(\\theta &gt; 0.5\\) has increased. This is because there is not yet any evidence that there is land on the globe, so the initial plausibilities are modified to be consistent with this.\nWe then toss the globe for a second time, and the result is land, \\(L\\).\n\n\n\n\n\nNow there is less evidence that the value of \\(\\theta\\) is greater than it currently is, whilst the evidence that it is less has now increased.\n\n\n\n\n\n\nWhich value is most plausible?\n\n\n\nNote that the value with the highest plausibility is 0.5, since we have observed exactly one water and one land observation out of two samples.\n\n\nAnd after a third toss, which is water again:\n\n\n\n\n\nNow there is more evidence that the value of \\(\\theta\\) is greater than currently estimated.\nThe entire sample of nine tosses \\((W L W W W L W L W)\\) would therefore look like this:\n\n\n\n\n\nIn general, Every time a \\(W\\) is seen, the peak of the plausibility curve moves to the right, towards larger values of \\(\\theta\\). Every time an \\(L\\) is seen, it moves in the other direction.\nThe maximum height of the curve also increases with each sample, meaning that fewer values of \\(\\theta\\) amass more plausibility as the amount of evidence increases. Notice that every updated set of plausibilities becomes the initial plausibilities for the next observation. This is the Bayesian update, where the posterior in the previous trial becomes the prior in the next.\n\n\n\n\n\n\nAlternate possibilities\n\n\n\nThese distributions importantly do not rule out the possiblility of alternative values of \\(\\theta\\). After the ninth sample, whilst the maximum plausibility is at around 2/3, there is still a small possibility that the value could still be 0.2, or 0.9.\n\n\nYou can see the importance of the prior through three distinct scenarios, all sharing the same likelihood function but employing different priors:\n\n\n\n\n\nThe first row demonstrates what happens when we use a flat or uniform prior distribution. In this case, the posterior distribution closely mirrors the likelihood function, as we’re letting the data speak for itself with minimal prior influence.\nThe middle row introduces a prior in the form of a step function, which effectively rules out any possibility of \\(θ\\) being less than 0.5. This represents a strong prior belief that fundamentally constrains our posterior distribution, forcing it to be zero for values less than 0.5 regardless of what the data suggests.\nThe final row shows what happens with an informative prior centered at 0.5, taking the form of a bell-shaped curve. In this prior, we believe values around 0.5 are more likely, but we’re not completely ruling out other possibilities. The resulting posterior distribution represents a compromise between this prior belief and the observed data, with its peak shifted slightly compared to the likelihood function."
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/binomial_model.html#solving-the-posterior-using-grid-approximation",
    "href": "workshops/02.binomial_globe/qmd/binomial_model.html#solving-the-posterior-using-grid-approximation",
    "title": "The Globe-Tossing experiment",
    "section": "Solving the posterior using grid approximation",
    "text": "Solving the posterior using grid approximation\nWe now understand the process of Bayesian updating; the updating of the prior to the posterior, given the likelihood. But how exactly does this happen?\nBecause the globe-tossing case is quite straight-forward, as there is only a single parameter that we need to estimate \\((\\theta)\\), that we have a relatively simple model (the binomial model), and we only have a few data points (9), we can solve this using grid approximation.\nGrid approximation works by discretizing (breaking up) the continuous parameter space into a finite number of points.\nRemember the the denominator of Bayes’ theorem for discrete versus continuous parameters:\nFor discrete parameters:\n\\[p(\\theta|D) = \\frac{p(D|\\theta)p(\\theta)}{\\sum_{\\theta} p(D|\\theta)p(\\theta)}\\] And for continuous parameters:\n\\[p(\\theta|D) = \\frac{p(D|\\theta)p(\\theta)}{\\int p(D|\\theta)p(\\theta)d\\theta}\\] In our globe-tossing example, instead of considering every possible value of \\((\\theta)\\) between 0 and 1 (which would be infinite), we instead create a discrete grid of points between these values. We might break this up into 10 or 100 equally spaced points where each point represents a possible value for the proportion of water on the globe.\nThis discretization is what makes the problem computationally tractable - instead of having to integrate over a continuous space (which can be mathematically challenging or impossible), we just need to calculate and sum values at these finite points.\nHowever, this is also what makes it an approximation - we’re essentially trading some precision for computational feasibility. The finer our grid (more points), the better our approximation, but the more calculations we need to perform.\n\n\n\n\n\n\nWhy bother with grid approximation?\n\n\n\nGrid approximation will mainly be useful as a tool to understand the nature of Bayesian updating. But in most of your real modeling, grid approximation isn’t practical. The reason is that it scales very poorly as the number of parameters increases.\n\n\nIn general, grid approximation follows four steps:\n\nDefine the grid by deciding how many points to use in estimating the posterior.\nCompute the value of the prior at each parameter value on the grid.\nCompute the likelihood at each parameter value.\nCompute the un-standardized posterior at each parameter value, by multiplying the prior by the likelihood.\n\nFor our globe tossing example, we use the binomial distribution as our likelihood function:\n\\[p(w|N,\\theta) = \\binom{N}{w}\\theta^w(1-\\theta)^{N-w}\\] where:\n\n\\(w\\) is the number of water observations\n\\(N\\) is the total number of tosses\n\\(\\theta\\) is the proportion of water\n\nWe can implement this in R by coding a grid approximation using 20 points:\n\nrm(list=ls())\n\n# Define our grid parameters\ntheta_start &lt;- 0; theta_end &lt;- 1; n_grid &lt;- 20\n\n# We observe a total of 6 water observations out of a total of 9 trials\nw &lt;- 6; N &lt;- 9\n\n# Create a grid of theta values\ntheta_grid &lt;- seq(from = theta_start, to = theta_end, \n                 length.out = n_grid)\n\n# Define prior (using uniform prior)\nprior &lt;- rep(1, n_grid)\n\n# Compute likelihood at each value in grid\nlikelihood &lt;- dbinom(w, size = N, prob = theta_grid)\n\n# Compute product of likelihood and prior\nunstd.posterior &lt;- likelihood * prior\n\n# Standardize the posterior so it sums to 1\nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\nThe key line is likelihood &lt;- dbinom(w, size = N, prob = theta_grid) which calculates the likelihood from the binomial function for each value of \\(\\theta\\).\nIn other words, we are answering the question: “What is the likelihood that \\(\\theta\\) is any of these values \\(\\theta \\in {0, 0.05, 0.10, ..., 0.95, 1.00}\\), given that we have observed 6 water observations from 9 samples?\nLet’s now plot this posterior distribution:\n\nplot(theta_grid, posterior, type=\"b\",\n  xlab=\"probability of water\", ylab=\"posterior probability\")\nmtext(\"20 points\")\n\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nThe code for running the grid approximation is located within _scripts/binomial_globe_grid.R\n1. Try changing the value of a few of the parameters, or indeed the number of grid points, and see the effect it has on the shape of the posterior.\n2. Plot the unstandardized posterior. How does it differ from the standardized posterior?\n\n\nClick to see the solution\n\n\nplot(theta_grid, unstd.posterior, type=\"b\",\n  xlab=\"probability of water\", ylab=\"posterior probability\")\nmtext(\"20 points\")"
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/binomial_model.html#how-do-i-know-which-likelihood-to-use",
    "href": "workshops/02.binomial_globe/qmd/binomial_model.html#how-do-i-know-which-likelihood-to-use",
    "title": "The Globe-Tossing experiment",
    "section": "How do I know which likelihood to use?",
    "text": "How do I know which likelihood to use?\nIn this example, the use of the binomial distribution may have been fairly intuitive, given that the discrete data is generated from a ‘globe-tossing’ process, which is equivalent to a coin flip.\nBut what if we have data of a different kind? What if we have some continuous height data from a group of participants, or neuron spiking data from an cellular recording of brain activity?\nMore generally, how are we supposed to know which likelihood function to use from the data that we have?\nIn the real world, data can be described by many different distributions:\n\n\n\n\n\n\n\n\n\n\nDistribution Type\nParameters\nDescription\nExample\n\n\n\n\nDiscrete Distributions\n\n\n\n\n\nBernoulli\np (probability)\nModels binary outcomes\nCoin toss\n\n\nBinomial\nn (trials), p (probability)\nSum of independent Bernoulli trials\nHeads in 10 coin tosses\n\n\nPoisson\nλ (rate)\nEvents in fixed interval\nEmails per hour\n\n\nContinuous Distributions\n\n\n\n\n\nNormal\nμ (mean), σ² (variance)\nBell-shaped curve\nPopulation heights\n\n\nExponential\nλ (rate)\nTime between events\nTime until next bus\n\n\nGamma\nα (shape), β (rate/scale)\nGeneralized exponential\nTime until multiple events\n\n\nBeta\nα, β (shape)\nProbabilities in [0,1]\nBayesian priors\n\n\nStudent’s t\nv (degrees of freedom)\nHeavier tails than normal\nSmall-sample testing\n\n\nOther Notable\n\n\n\n\n\nCauchy\nlocation, scale\nHeavy-tailed, undefined mean\nPhysics resonance\n\n\nMultinomial\nn (trials), pᵢ (probabilities)\nMulti-category binomial\nMulti-choice polls\n\n\n\n\nBut thankfully, we dont need to memorize them all!\nThis decision tree, created by Ben Lambert2, is a useful guide in determining which likelihood to use given the type of data that you have:\n\n\n\n\n\nThis may also seem overwhelming, but it is most likely that in your specific work, you will only ever use a specific sub-set of these distributions. For example, in the field of psychology and cognitive neuroscience, and specifically concerning the analysis of human behavioural data, the following distributions are most commonly used:\n\nthe binomial distribution for discrete data (i.e., button press),\nthe normal/cauchy distribution (or log-transformed normal distribution) for continuous data (i.e., reaction time)\nthe beta distribution for modeling probabilities or proportions (i.e., individual differences in response patterns)\n\n\n\n\n\n\n\nThe distribution zoo\n\n\n\nBen also is the creator of The Distribution Zoo which allows for users to interactively generate different distributions. It covers a wide variety of distributions across bth discrete and continuous data, and is particularly useful for seeing the influence of changing parameter values on the distribution shape.\n\n\nThe goal with likelihood functions is not to memorize everything. The goal is knowing where to start, and building intuition on which distribution to use given your data."
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/binomial_model.html#footnotes",
    "href": "workshops/02.binomial_globe/qmd/binomial_model.html#footnotes",
    "title": "The Globe-Tossing experiment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMcElreath, R. (2018). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman and Hall/CRC.↩︎\nLambert, B. (2018). A student’s guide to Bayesian statistics.↩︎"
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/stan_binomial.html",
    "href": "workshops/02.binomial_globe/qmd/stan_binomial.html",
    "title": "An introduction to the Stan programming language",
    "section": "",
    "text": "In our last workshop, we learned about model construction using McElreath’s framework1:\nWe then applied this framework to build a binomial model, where we:\nWhile grid approximation proved effective for our single-parameter model, we discussed its limitations when dealing with multiple parameters. This led us to explore Markov chain Monte Carlo (MCMC) methods - a more scalable approach to sampling from posterior distributions in complex models.\nWe then learned how MCMC works, by combining random sampling (Monte Carlo) to explore the parameter space, and defined state transitions (Markov chains) to guide samples toward regions of high posterior probability. We also demonstrated this through a practical example using the Metropolis algorithm.\nBut how do we program these models?"
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/stan_binomial.html#what-is-stan",
    "href": "workshops/02.binomial_globe/qmd/stan_binomial.html#what-is-stan",
    "title": "An introduction to the Stan programming language",
    "section": "What is Stan?",
    "text": "What is Stan?\nStan2 is a probabilistic programming language designed specifically for statistical modeling and high-performance statistical computation. It was developed by Andrew Gelman, Bob Carpenter, and others at Columbia University, and named after Stanislaw Ulam, who pioneered Monte Carlo methods. It is a popular approach to statistical modeling, with the core Stan paper being cited over 8,300 citations, surpassing the older JAGS program (introduced in 2003, ~7,200 citations). The popularity of Stan has been furthered by the development of user-friendly interfaces, particularly the brms (Bayesian Regression Models using Stan) R package, which also introduced in 2017, has already accumulated over 8,500 citations.\n\n\n\n\n\n\n\n\n\nCitations for JAGS, Stan and brms as of 1/1/2025.\n\nStan is widely used across many scientific disciplines. While its roots are in statistics and probability (and is most used among those in this field), Stan is also popular in experimental psychology and neuroscience.\n\n\n\n\n\n\n\n\n\nWeb of Science, retrieved 05/2021\n\nBut what is Stan and how it is implemented? Stan is it’s own programming language, like R or Python, but also has an interface with other platforms and softwares. You can even run Stan from the command line. In our case, we will specifically be using Stan with R, and so will rely upon the R interface to Stan RStan.\nRStan acts as a bridge between R and Stan - it takes our Stan model code (which we write in a separate .stan file), compiles it into C++, runs the model, and returns the results in a format that’s easy to work with in R. The workflow typically involves:\n\nWriting our model code in Stan\nPreparing our data in R in a format Stan can understand\nUsing RStan functions to compile and run the model\nAccessing and analyzing the results back in R\n\nThis separation between Stan and R is powerful because it lets us use Stan’s efficient computation engine while also being able to use R’s extensive tools for data manipulation and visualization.\n\n\n\n\n\n\nThe relationship between Stan and R. We feed the data from R into Stan, which then samples it using our Stan model. These samples are then fed back into R for analysis and inference."
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/stan_binomial.html#the-basic-stan-model",
    "href": "workshops/02.binomial_globe/qmd/stan_binomial.html#the-basic-stan-model",
    "title": "An introduction to the Stan programming language",
    "section": "The basic Stan model",
    "text": "The basic Stan model\nUnlike R or Python, Stan is a block-based language, consisting of distinct sections of code that serve specific purposes in our model specification and computation. Here’s a breakdown of each block and its purpose:\ndata {\n  // ... read in external data...\n}\n\nparameters {\n  // ... parameters to be sampled by HMC ...\n}\n\nmodel {\n  // ... statistical/cognitive model ...\n}\ndata\nThis block declares and defines the data that your model will use.\n\nHere, you specify the input data that you’ll provide from R. This includes the dimensions (number of observations) and the actual data values.\n\nparameters\nHere you declare the parameters that Stan will estimate.\n\nThese are the unknown quantities in your model that you want to determine. Each parameter needs constraints on its possible values (like being positive or between 0 and 1).\n\nmodel\nThis is where you specify your statistical model. This is how we link the data to the unknown parameter. It includes:\n\nPriors for your parameters\nThe likelihood function relating parameters to data\nAny constraints or relationships between variables\n\nEach block serves a specific purpose in the modeling pipeline, and understanding when to use each one is key to writing efficient Stan programs. There are other blocks - which we will cover in future workshops - but for now, it is enough to understand that these three blocks are the most important when writing your Stan model.\nWith this in mind, let’s revisit the globe tossing example from earlier.\n\n\n\n\n\nRecall that we repeatedly tossed a globe containing an unknown parameter \\(\\theta\\), representing some true proportion of water covering the globe.\nWe tossed the globe nine times, with the nine samples being:\n\\[W L W W W L W L W\\]\nwhere \\(W\\) indicates water and \\(L\\) indicates land.\nSo, we ultimately observed six \\(W\\) (water) observations and three \\(L\\) (land) observations.\nThis data can be more formally expressed using probability notation. If we let \\(N\\) represent the total number of trials, we can alternatively write:\n\\[W \\sim \\text{Binomial}(N, \\theta)\\]\nWhich is equivalent to writing the full probability mass function:\n\\[p(w | N,\\theta) = \\binom{N}{w}\\theta^w(1-\\theta)^{N-w}\\]\nBoth expressions read as: “\\(W\\) is distributed as a binomial distribution, with number of trials \\(N\\), and success rate \\(\\theta\\).”\nThis mathematical notation is particularly useful as it maps directly to how we’ll specify our model in Stan.\n\n\n\n\n\n\nThe \\(\\sim\\)\n\n\n\nThe tilde symbol (\\(\\sim\\)) is used in both mathematical notation and in Stan code to denote that a variable follows a particular probability distribution.\n\n\nOften in papers, you will also see the model graphed visually using a particular notation3.\nOn the left hand side below is a table describing this notation, which separates variables into two categories:\n\nWhether they are continuous or discrete\nWhether they are observed or unobserved\n\nIf the variable is observed, then it is shaded, and if unobserved, it is clear. If the variable is continuous, it is placed within a circle, if it is discrete, it is placed within a square.\n\n\n\n\n\n\n\n\n\n\n\nLet’s apply this notation to our globe tossing model. Remember our variables:\n\n\\(θ\\) is a continuous, unobserved parameter representing the true proportion of water\n\\(N\\) is a discrete, observed value representing the number of tosses\n\\(w\\) is a discrete, observed value, representing the number of water observations\n\nWe can represent this in the graph (a Directed Acrylic Graph, DAG) shown above on the right, which shows the dependencies between variables using arrows:\n\n\\(θ\\) our unknown parameter is at the top, with a uniform prior distribution between 0 and 1\n\\(N\\) is simply the number of trials (tosses)\n\\(w\\) the number of water observations, follows a Binomial distribution with parameters \\(N\\) and \\(θ\\)\n\nRemember that the shaded boxes indicate observed quantities (\\(N\\) and \\(w\\)), whilst unshaded circles indicate parameters we want to estimate (\\(θ\\)).\nUltimately, this graphical representation is useful when constructing our model, as it helps us visualize:\n\nThe hierarchical structure of our model\nWhich variables depend on others\nWhat we observe versus what we need to estimate\nThe probability distributions involved"
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/stan_binomial.html#programming-a-binomial-model-in-stan",
    "href": "workshops/02.binomial_globe/qmd/stan_binomial.html#programming-a-binomial-model-in-stan",
    "title": "An introduction to the Stan programming language",
    "section": "Programming a binomial model in Stan",
    "text": "Programming a binomial model in Stan\nLet’s now build our binomial model using the Stan programming language, and run the model through the RStudio interface.\n\nDefining the data\nOpen the script binomial_globe_main.R, which is the R script that will run our Stan model.\nAt the top of the script, you can see the following code:\nrm(list=ls(all=TRUE))\n\nw &lt;- 6\nN &lt;- 9\ndataList &lt;- list(w=w, N=N)\nHere we clear the workspace, assign our parameter values for the number of samples, N, and the number of water observations, w. Importantly, because we need to send this data to our Stan model, we convert the data into a data list using the list function.\n\n\n\n\n\n\nWhy do we use a data list?\n\n\n\nRemember that stan models, when executed, get compiled to C++. Subsequently, Stan doesn’t understand R’s data frame structure. Data lists are simpler data structures that can be more easily translated between R and Stan’s C++ implementation.\n\n\nNow that the data is properly defined and stored, the next stage is to write our Stan model.\n\n\nCreating the Stan model\nOpen the binomial_globe_model.stan file. The model has already been created for you:\ndata {\n  int&lt;lower=0&gt; w;\n  int&lt;lower=0&gt; N;\n}\n\nparameters {\n  real&lt;lower=0,upper=1&gt; theta;\n}\n\nmodel {\n  // theta ~ uniform(0,1);\n  w ~ binomial(N, theta);\n}\n\n\n\n\n\n\nStan files and the empty line\n\n\n\nNote that there is a blank line at the bottom of the Stan model code. This must be the case for all .stan files!\n\n\nRemember that the three main blocks that we need in any Stan model are the data, parameters and model blocks. But what do each of these blocks mean and why do we choose the certain values that they are?\n\nData\n\n\n\n\nWe use the data block to declare variables we’ll receive from R. Importantly, the names need to match between your R script and Stan model.\nWithin the block, there are a number of technical notes:\n\nint specifies that these are integer values (whole numbers)\n&lt;lower=0&gt; constrains these values to be non-negative, since you can’t have negative counts\nEach line ends with a semicolon (;) because Stan is based on C++, where statements must be terminated with semicolons\n\nUltimately, this code tells the Stan model to expect non-negative integers for our two data variables, \\(w\\) and \\(N\\).\n\nParameters\n\n\n\n\nWe use the parameters block to declare the variables that we would like to estimate.\nAgain, within the block there are specifics that we must declare:\n\nWe declare that the parameter, \\(θ\\), can take decimal values by adding real (i.e., a continuous parameter)\n&lt;lower=0,upper=1&gt; constrains \\(θ\\) to be a probability between 0 and 1\n\nThis code tells the Stan model that we want to estimate \\(θ\\), which can take any decimal value constrained between 0 and 1.\n\nModel\n\n\n\n\nThe model block specifies our statistical model (i.e., our likelihood function).\nw ~ binomial(N, theta) specifies that \\(w\\) follows a binomial distribution with:\n\n\\(N\\) trials\n\\(θ\\) probability of success (observing water)\nThe \\(\\sim\\) operator means “is distributed as”\n\nSo, this block simply translates to: “The number of water observations follows a binomial distribution according to the number of trials and the proportion of water on the globe”.\nHowever, there is an additional line, which is optional in this specific case. Recall that in Bayesian inference, we need a prior (initial belief) for our parameters. In this case, we could (but don’t have to), set a uniform prior between 0 and 1. This reflects an implicit uniform prior which is uninformed. In other words, we have no prior evidence to suggest that the values of \\(θ\\) should be a certain value.\n\n\n\n\n\n\nNot specifying a prior\n\n\n\nThe reason why we do not have to specify our prior in this case is that when no prior is specified, Stan uses a uniform prior by default for bounded parameters. Ultimately, even in this scenario, priors should always be coded for clarity.\n\n\n\n\n\n\n\n\nChecking your Stan model\n\n\n\nIt is recommended to always check if your Stan model has been correctly written. To do this, click on the ‘Check’ tab on the top right of the main R window.\nIf there are no issues, the the following message will appear in the console:\n&gt; rstan:::rstudio_stanc(\"workshops/02.binomial_globe/_scripts/binomial_globe_model.stan\")\nworkshops/02.binomial_globe/_scripts/binomial_globe_model.stan is syntactically correct.\nHowever, if for example, you removed a semi-colon from one of the lines, the following error message will appear:\n&gt; rstan:::rstudio_stanc(\"workshops/02.binomial_globe/_scripts/binomial_globe_model.stan\")\nError in stanc(filename, allow_undefined = TRUE) : 0\nSyntax error in 'string', line 11, column 0 to column 1, parsing error:\n   -------------------------------------------------\n     9:    //p ~ uniform(0,1);\n    10:    w ~ binomial(N, theta)\n    11:  }\n         ^\n   -------------------------------------------------\n\nIll-formed \"~\"-statement. Expected \";\" or \"T[\" optional expression \",\" optional expression \"];\".\nYou can then fix the error and re-check. Debugging in Stan will be covered in more detail in Workshop 9 of the course.\n\n\n\n\nRunning the Stan model in R\nSo now we have both defined our data and created the Stan model, we can now run the model through our R script. But, this is not as straight-forward as simply pressing ‘Run’. We need to set-up our R script to run the model according to our specifications.\nHere is the relevant section of the binomial_globe_main.R script:\n#### Running Stan #### \n\nlibrary(rstan)\nrstan_options(auto_write = TRUE)\noptions(mc.cores = 4)\n\nmodelFile &lt;- '_scripts/binomial_globe_model.stan'\nnIter     &lt;- 2000\nnChains   &lt;- 4 \nnWarmup   &lt;- floor(nIter/2)\nnThin     &lt;- 1\n\ncat(\"Estimating\", modelFile, \"model... \\n\")\nstartTime = Sys.time(); print(startTime)\ncat(\"Calling\", nChains, \"simulations in Stan... \\n\")\n\nfit_globe &lt;- rstan::stan(modelFile,\n                  data    = dataList,\n                  chains  = nChains,\n                  iter    = nIter,\n                  warmup  = nWarmup,\n                  thin    = nThin,\n                  init    = \"random\",\n                  seed    = 1450154626)\n\ncat(\"Finishing\", modelFile, \"model simulation ... \\n\")\nendTime = Sys.time(); print(endTime)  \ncat(\"It took\",as.character.Date(endTime - startTime), \"\\n\")\n\n\n\n\n\n\nInstalling Rstan\n\n\n\nIf running the script, you should double check if rstan is installed. You can do this by:\n\nRun installed.packages() and look for rstan\nSimply try library(rstan) - if it errors, it’s not installed\n\nIf you do not have rstan installed, please follow the RStan Getting Started guide or the renv guide in the ‘Course Overview’ page on this website.\n\n\n\nSet-up and configuration\n\n\n\n\nThe first section mainly specifies our Stan environment. It loads the rstan package and using auto_write = TRUE saves compiled Stan programs to avoid recompilation. Setting mc.cores controls the processing power for the Stan process. Setting this to 4 enables parallel processing across 4 CPU cores for faster sampling.\n\n\n\n\n\n\nDetermining how many cores to use\n\n\n\nYou can check how many cores are available by running parallel::detectCores(). For example:\n&gt; parallel::detectCores()\n[1] 12\nGenerally, it’s recommended to use 1 core per chain. This allows for each chain to be ran by each separate CPU concurrently.\n\n\n\nSampling parameters\n\n\n\n\nThe next section defines our MCMC sampling strategy. After using modelFile to specify the path to our .stan model file we set:\n\nnIter - total iterations per chain (2000) i.e., the number of visits that the MCMC robot makes. We need enough iterations to explore the parameter space thoroughly, and 2000 is often sufficient for simple models like ours\nnChains - number of parallel chains for convergence diagnostics (4). Multiple chains help assess convergence. If chains reach similar distributions from different starting points, then we’re more confident in our results. Four chains is standard practice - enough to assess convergence without excessive computation\nnWarmup - discards first X iterations as burn-in period (1000). The initial samples might be poor if chains start in certain regions of the parameter space. Discarding them ensures we only use samples from when chains have “settled”.\nnThin keeps every sample (no thinning). Thinning means keeping every nth sample, which is useful if samples are highly autocorrelated. Ours is a simple model, so we don’t need to thin.\n\n\nTime tracking\n\n\n\n\nWe simply record the start and the end time to determine how long the model took to run. This is useful for adjusting our settings if the runtime is too long.\n\nModel execution\n\n\n\n\nWe then run our model, through the stan()function and save its output to fit_globe. When we run this function, we’re doing several important things:\n\nFirst, the function reads our Stan model file, compiles it to C++ code, and runs the MCMC sampling process using all the parameters we’ve specified - the number of chains, iterations, warm-up period, and so on.\nTwo additional settings are the init which defines the initial starting point (which is random) and the seed. The seed if set, means that the results are reproducible, but only within our own machine.\nWe assign the output to fit_globe. Because the output is complex, not a collection of numbers or basic results - we create a stanfit object, which is a specialized R object designed specifically for Stan model outputs. This object is like a container that holds everything about our model’s results. It contains all the posterior samples for our parameter \\(θ\\), but it also includes diagnostic information about how well the sampling worked, metrics about whether our chains converged properly, details about the model specification, and various other metadata about the sampling process.\n\n\n\nRunning the model and examining the output\nLet’s now run the Stan model. You can do this by highlighting the code included in the previous code block in the R script.\nWhen it is running, you should see the following output in the console:\nstarting worker pid=56934 on localhost:11017 at 12:53:01.273\nstarting worker pid=56947 on localhost:11017 at 12:53:01.551\nstarting worker pid=56960 on localhost:11017 at 12:53:01.802\nstarting worker pid=56973 on localhost:11017 at 12:53:02.058\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 8e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.007 seconds (Warm-up)\nChain 1:                0.007 seconds (Sampling)\nChain 1:                0.014 seconds (Total)\nChain 1: \nThis visually shows the MCMC chains at work. We can see that the first 1000 samples are labelled as ‘Warmup’ and also how long the process takes.\n\n\n\n\n\n\nModel running times\n\n\n\nThe model upon it’s first run will take around 1-2 minutes, depending on the processing power of your machine. However, if you run it again it will take significantly shorter. This is because we have set the model to avoid recompilation through rstan_options(auto_write = TRUE)\n\n\nOnce the model has finished running, we can examine the output. What exactly is the output? What are the data generated? There’s a lot to cover in that regard, so for now we will just focus on some basic checks.\nprint(fit_globe)\n\nplot_trace_excl_warm_up &lt;- stan_trace(fit_globe, pars = 'theta', inc_warmup = F)\nplot_trace_incl_warm_up &lt;- stan_trace(fit_globe, pars = 'theta', inc_warmup = T)\n\nplot_dens_cmb &lt;- stan_dens(fit_globe, separate_chains = F, fill = 'skyblue')\nplot_dens_sep &lt;- stan_dens(fit_globe, separate_chains = T)\nIf we print fit_globe we get the following:\n&gt; print(fit_globe)\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n       mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\ntheta  0.64    0.00 0.14  0.35  0.55  0.65  0.74  0.87  1483    1\nlp__  -7.71    0.02 0.69 -9.78 -7.86 -7.44 -7.27 -7.21  1886    1\n\nSamples were drawn using NUTS(diag_e) at Tue Dec 24 13:18:04 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\nThis output gives us key information about our binomial model’s parameter estimation. Let’s break down what these numbers mean:\nFor our parameter of interest, \\(θ\\), which represents the proportion of water in our globe-tossing example:\n\nThe posterior mean is 0.64, suggesting it is most likely that 64% of the globe is covered in water\nThe 95% credible interval ranges from 0.35 to 0.87\nThe median (50th percentile) is 0.65, very close to our mean\n\nWe can also see some results for the model diagnostics:\n\nEffective Sample Size (n_eff): This tells us how many truly independent samples we got from our model. Even though we have 4000 total samples, they aren’t all completely independent from each other. The n_eff of 1483 means we effectively have about 1,483 independent samples, which is a good value; a general rule of thumb is that we want this to be above 1000.\nRhat: Rhat (the Gelman-Rubin statistic4) helps us check if our chains are converging to the same distribution. 1 is the ideal value, values above 1.1 are problematic.\n\nThese results tell us we have a well-behaved model that’s giving us reliable estimates of \\(θ\\). The relatively tight credible interval also gives us a good sense of the uncertainty in our estimate.\nWe can also visually examine the samples by plotting them. Checking the MCMC traces is important for determining whether the traces are mixing well and have converged. A good trace plot should look like a “fuzzy caterpillar” - showing random fluctuations around a stable mean value.\nYou can see the MCMC traceplots for the four chains below, both with and without the burn-in period. In the R script, just highlight and run the following code (i.e., to plot the traces do not assign it to a variable).\nstan_trace(fit_globe, pars = 'theta', inc_warmup = F)\nstan_trace(fit_globe, pars = 'theta', inc_warmup = TRUE)\n\n\n\n\n\n\n\n\n\n\nOur traceplots suggest that the chains have performed well. The four chains show good mixing and overlap, indicating they’re exploring the same regions of the parameter space independently. After the warmup period, you can see that the chains are sampling consistently around the posterior mean of 0.64.\nTo more clearly see our samples, we can instead plot them as a histogram, showing the posterior density distribution for our parameter \\(\\theta\\) estimated from our MCMC sampling:\n# highlight and run the following code in the script\nstan_dens(fit_globe, separate_chains = FALSE, fill = 'skyblue')\nstan_dens(fit_globe, separate_chains = TRUE)\n\n\n\n\n\n\nThe posterior mean is highlighted with a black bar.\n\nIn this case, the confidence (blue shaded region) and outer levels (black lines) are set at the default values of 80% and 95% respectively:\nci_level: 0.8 (80% intervals)\nouter_level: 0.95 (95% intervals)"
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/stan_binomial.html#what-is-a-bayesian-model",
    "href": "workshops/02.binomial_globe/qmd/stan_binomial.html#what-is-a-bayesian-model",
    "title": "An introduction to the Stan programming language",
    "section": "What is a Bayesian model?",
    "text": "What is a Bayesian model?\nWhat we have done in this workshop is to estimate the value of some unknown parameter given some experimental data, using a Bayesian approach.\nHowever, if one was to say that they are working on “Bayesian models” of cognition, what does that mean? Bayesian modeling is a wider semantic construct than many realize, encompassing both how we structure our models and how we estimate their parameters. This distinction is crucial for understanding the different types of Bayesian approaches in statistical modeling and cognitive science.\nThe term “Bayesian” can actually refer to two distinct aspects of modeling: the model’s structure and the method used for parameter estimation.\n\nThe model structure determines whether parameters are treated as random variables with probability distributions (Bayesian) or as fixed, unknown constants (non-Bayesian).\nMeanwhile, parameter estimation methods can either produce full probability distributions through Bayesian inference or provide point estimates through non-Bayesian methods (e.g., Maximum Likelihood Estimation).\n\n\n\n\n\n\n\nA Bayesian matrix of modeling approaches.\n\nThis creates an interesting matrix of possibilities in modeling approaches. For instance, consider the Bayesian Brain Hypothesis5, a prominent theory in cognitive science. The theory (in a one-sentence nutshell) proposes that the brain maintains probabilistic models of the world, continuously updating them with new sensory information.\n\nThis is inherently Bayesian in structure because it assumes the brain maintains and updates probability distributions.\nHowever, when researchers implement these models, they might use either Bayesian estimation methods (like MCMC sampling) or non-Bayesian approaches (like maximum likelihood estimation). On the graph, this could be represented by the yellow or green square.\n\nOn-the-other-hand, our globe-tossing example consists of the model generating the data (binomial model using coin flips/globe tosses) which is frequentist, but we’re using Bayesian methods to estimate parameters.\n\nOn the graph, this would be represented by the orange square.\n\nHopefully this matrix helps to clarify what we mean when we say we’re working with Bayesian models, and about which aspects of our approach are Bayesian!"
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/stan_binomial.html#footnotes",
    "href": "workshops/02.binomial_globe/qmd/stan_binomial.html#footnotes",
    "title": "An introduction to the Stan programming language",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMcElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan, 2nd Ed. CRC Press.↩︎\nCarpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., … & Riddell, A. (2017). Stan: A probabilistic programming language. Journal of statistical software, 76.↩︎\nLee, M. D., & Wagenmakers, E. J. (2014). Bayesian cognitive modeling: A practical course. Cambridge university press.↩︎\nGelman, A., & Rubin, D. B. (1992). Inference from iterative simulation using multiple sequences. Statistical science, 7(4), 457-472.↩︎\nColombo, M., & Seriès, P. (2012). Bayes in the brain—on Bayesian modelling in neuroscience. The British journal for the philosophy of science.↩︎"
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/data_and_parameters.html",
    "href": "workshops/02.binomial_globe/qmd/data_and_parameters.html",
    "title": "Linking data to parameters",
    "section": "",
    "text": "“Probability is orderly opinion and inference from data is nothing other than the revision of such opinion in the light of relevant new information.” - Popular Bayesian maxim\nIn our last lecture, we covered fundamental concepts in probability theory.\nWe firstly learned that probability measures how likely events are to occur, with some basic properties, namely that:\nWe also learned about the different types of probability events, discrete and continuous:\nAs well as three important probability concepts:\nAnd finally, we learned about Bayes’ theorem, a formula which allows us to update our beliefs based on new evidence in a probabilistic manner:\n\\[p(A|B) = \\frac{p(B|A)p(A)}{p(B)}\\]\nWhere:\nOr, more intuitively:\n\\[\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Marginal Likelihood}}\\]"
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/data_and_parameters.html#reformulating-bayes-theorem",
    "href": "workshops/02.binomial_globe/qmd/data_and_parameters.html#reformulating-bayes-theorem",
    "title": "Linking data to parameters",
    "section": "Reformulating Bayes’ theorem",
    "text": "Reformulating Bayes’ theorem\nLooking at the conventional Bayes’ theorem formula, we can re-write this to more accurately suit our goal of applying Bayesian inference to our statistical analyses.\nFirstly, there is no particular reason or meaning behind the use of \\(A\\) and \\(B\\). It can be anything we like.\nSecondly, when using Bayes’ theorem practically, we’re typically trying to solve a specific kind of problem: we have some data from an experiment or observation, and we want to learn something about the underlying process that generated that data. This typically involves estimating parameters from data.\n\nParameters (\\(\\theta\\)) represent the unknown quantities we want to learn about (like population means, effects of treatments, or probabilities of events)\nData (\\(D\\)) represents our actual observations or measurements from our experiment\n\nSo, let’s simply recast the theorem in terms of parameters (\\(\\theta\\)) and data (\\(D\\)):\n\\[p(\\theta|D) = \\frac{p(D|\\theta)p(\\theta)}{p(D)}\\]\nYou can see how this reformulation directly maps to real research questions in the image below:\n\n\n\n\n\n\n\n\nLet’s go through each component of this reformulation, starting with the likelihood."
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/data_and_parameters.html#likelihood",
    "href": "workshops/02.binomial_globe/qmd/data_and_parameters.html#likelihood",
    "title": "Linking data to parameters",
    "section": "Likelihood",
    "text": "Likelihood\nLet’s use a simple example to understand the likelihood: flipping a coin. Let’s firstly assume that we know that the coin is unbiased, and so the probability of the coin landing heads up is given by \\(\\theta = 1/2\\)\nWe then flip the coin twice. What are the different outcomes and their probabilities?\nIn this case, there are three outcomes, each with a specific probability:\n\nTT (0 heads) = 1/4\nHT or TH (1 head) = 1/2\nHH (2 heads) = 1/4\n\nWhen we assume a particular value of \\(θ\\) and vary the data (in this case the number of heads obtained), the collection of resultant probabilities forms a probability distribution. So why do we call \\(p(data|\\theta)\\) a likelihood, not a probability?\nThe reason why is that in Bayesian inference we do not keep the parameters of our model fixed; the data are fixed and the parameters vary.\nImagine now that we have a coin with an unknown bias \\((\\theta)\\), representing the probability of getting heads. If we flip the coin twice and get one heads and one tails, we can calculate the likelihood of this data for any possible value of \\(\\theta\\):\n\\[Pr(H,T|\\theta) + Pr(T,H|\\theta) = \\theta(1-\\theta) + \\theta(1-\\theta) = 2\\theta(1-\\theta)\\]\nIn other words, we can calculate the probability of throwing a head for a fixed data sample (one head and one tail) as a function of \\(\\theta\\). We can then plot the likelihood function of \\(\\theta\\) which is equivalent to the probability of that data sample (one head and one tail) given values of \\(\\theta\\)1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSumming likelihoods and probabilities\n\n\n\nRemember: When we fix parameters and vary data outcomes, we get probabilities (summing to 1). When we fix data and vary parameters, we get likelihoods (which don’t necessarily sum to 1). The likelihood is not a probability distribution!\n\n\nLet’s further examine the distinction between likelihoods and probabilities using another coin flipping example.\nWe decide to flip the coin twice. We can then calculate the probability of getting different numbers of heads (\\(X\\)):\n\\(Pr(X = 0|\\theta) = Pr(T,T|\\theta) = Pr(T|\\theta) \\times Pr(T|\\theta) = (1-\\theta)^2\\)\n\\(Pr(X = 1|\\theta) = Pr(H,T|\\theta) + Pr(T,H|\\theta) = 2 \\times Pr(T|\\theta) \\times Pr(H|\\theta) = 2\\theta(1-\\theta)\\)\n\\(Pr(X = 2|\\theta) = Pr(H,H|\\theta) = Pr(H|\\theta) \\times Pr(H|\\theta) = \\theta^2\\)\nNow, let’s consider that probability of heads (\\(\\theta\\)) can only be one of six values: \\({0.0, 0.2, 0.4, 0.6, 0.8, 1.0}\\).\nThe table below demonstrates how the values of \\(X\\) and \\(\\theta\\) vary as a function of the other:\n\n\n\n\n\n\n\n\nThis allows us to see the relationship between the data and parameters:\n\nFixed Parameter (Probability): If we fix \\(\\theta\\) and look at the different possible outcomes (0, 1, or 2 heads), the probabilities sum to 1 - a valid probability distribution.\nFixed Data (Likelihood): If we fix the number of heads we observed and look at different possible values of \\(\\theta\\), the values do not sum to 1 - this is why we call it a likelihood.\n\n\n\n\n\n\n\nWhy work with likelihoods?\n\n\n\nIn Bayesian inference, we work with likelihoods because we:\n\nHave fixed data (our actual observations)\nVary the parameter (\\(\\theta\\))\nUltimately want to find which parameter values best explain our data\n\n\n\n\n\n\n\n\n\nStill confused?\n\n\n\nUnderstanding the difference between the likelihood and probability can be difficult. This video by the YouTube channel ‘StatQuest with Josh Starmer’ nicely explains this difference!"
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/data_and_parameters.html#priors",
    "href": "workshops/02.binomial_globe/qmd/data_and_parameters.html#priors",
    "title": "Linking data to parameters",
    "section": "Priors",
    "text": "Priors\nRemember the purpose of Bayesian analysis concerns updating our initial beliefs in light of data.\n\n\\(\\Large\\text{initial belief}\\) \\(\\Large\\xrightarrow{\\text{Bayes' rule + data}}\\) \\(\\Large\\text{new beliefs}\\)\n\n\n\n\nTherefore, we need an initial belief to begin with!\nA ‘prior’ - \\(p(\\theta)\\) - represents our beliefs about parameters before seeing any data. These beliefs can come from previous research, expert knowledge, or logical constraints about what values are possible.\nThere are two different interpretations of parameter probability distributions when considering a prior2:\n\nThe subjective state of knowledge interpretation, where we use a probability distribution to represent our uncertainty over a parameter’s true value.\nThe more objective population interpretation, where the parameter’s value varies between different samples we take from a population distribution.\n\nIn the state of knowledge viewpoint, the prior probability distribution represents our pre-data uncertainty for a parameter’s true value. For example, imagine that a doctor gives their probability that an individual has a particular disease before the results of a blood test become available. Using their knowledge of the patient’s history, and their expertise on the particular condition, they could assign a prior disease probability of 75%.\nAlternatively, we could use the proportion of the UK population that has this disease as a prior. In this case, the prior is continuous and represents our beliefs for the prevalence.\nThe choice of prior should be ultimately guided by several factors including:\n\nAvailable previous knowledge (e.g., from previous studies)\nThe desire to be conservative\nLogical constraints of the parameter (e.g., \\(p\\) when flipping a coin must be constrained between 0 and 1)\n\nThere are three main types of priors:\n1. Informative Priors - Based on existing evidence or expert knowledge\n\nUsed when we have reliable previous information\nExample: Using established medical research about cancer development rates\n\n2. Weakly Informative Priors - Provide some constraints but remain relatively open to data\n\nUsed when we have general knowledge but want to be conservative\nExample: Only specifying that cancer development time must be positive and within human lifespan\n\n3. Non-informative/Flat Priors - Make minimal assumptions about parameters\n\nUsed when we want data to dominate our conclusions\nExample: Assigning equal probability to all possible development times (though this would be inappropriate here)\n\nLet’s examine this using a real example about determining priors for the mean time (\\(\\mu\\)) for smokers to develop lung cancer after they start smoking[^1^]:\n\n\n\n\n\n\n\n\nThe graph depicts two priors:\n\nA ‘uniform prior’ assuming a constant probability density for most values, but which becomes zero for values greater than 100. This is to avoid infinite total probability.\nA ‘weakly informative prior’ assuming a higher probability density earlier in time, and decaying after a peak value of 20 years.\n\n\n\n\n\n\n\nConstructing the weakly informative prior\n\n\n\nThe weakly informative prior in this circumstance may be constructed based on a combination of medical knowledge, existing data and logical constraints."
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/data_and_parameters.html#the-evidence",
    "href": "workshops/02.binomial_globe/qmd/data_and_parameters.html#the-evidence",
    "title": "Linking data to parameters",
    "section": "The evidence",
    "text": "The evidence\nThe denominator of Bayes’ theorem, \\(p(D)\\), is often called the “evidence” or “marginal likelihood.” The evidence ensures that our posterior probabilities are valid probability distributions that sum/integrate to 1. Without this normalization, we would only have un-normalized posterior probabilities.\nFor discrete parameters, this means:\n\\[\\sum_{\\theta} p(\\theta|D) = 1\\] and for continuous parameters:\n\\[\\int p(\\theta|D)d\\theta = 1\\]\nwhere:\n\n\\(θ\\) represents the parameter(s) we’re trying to estimate\n\\(D\\) represents the observed data\n\\(p(\\theta|D)\\) represents the posterior probability mass/density function of \\(θ\\) given \\(D\\)\n\nThe evidence \\(p(D)\\) represents the total probability of observing our data under all possible parameter values. It can be interpreted as a weighted average of the likelihood over all parameter values, where the weights are given by the prior probabilities.\nFor discrete parameters, we calculate this as:\n\\[p(D) = \\sum_{\\theta} p(D|\\theta)p(\\theta)\\] and for continuous parameters:\n\\[p(D) = \\int p(D|\\theta)p(\\theta)d\\theta\\] However, when we have multiple parameters (as is often the case), calculating the evidence becomes more complex. For two discrete parameters \\(\\theta_1\\) and \\(\\theta_2\\), we must sum over all possible combinations:\n\\[p(D) = \\sum_{\\theta_1}\\sum_{\\theta_2} p(D|\\theta_1,\\theta_2)p(\\theta_1,\\theta_2)\\] And for two continuous parameters, we need multiple integrals:\n\\[p(D) = \\int\\int p(D|\\theta_1,\\theta_2)p(\\theta_1,\\theta_2)d\\theta_1d\\theta_2\\] As the number of parameters increases, calculating the evidence becomes increasingly difficult. This is particularly so for continuous parameters, of which the computational complexity grows exponentially with the number of parameters.\nFor three parameters: \\[p(D) = \\int \\int \\int p(D|\\theta_1,\\theta_2,\\theta_3)p(\\theta_1,\\theta_2,\\theta_3)d\\theta_1d\\theta_2d\\theta_3\\] For ten parameters: \\[p(D) = \\int \\int ... \\int \\int p(D|\\theta_1,\\theta_2, ...\\theta_9,\\theta_{10})p(\\theta_1,\\theta_2, ...\\theta_9,\\theta_{10})d\\theta_1d\\theta_2...\\theta_9d\\theta_{10}\\] With the general case for \\(n\\) parameters being:\n\\[p(D) = \\underbrace{\\int \\int ... \\int}_\\text{n times} p(D|\\theta_1,...,\\theta_n)p(\\theta_1,...,\\theta_n)d\\theta_1...d\\theta_n\\] This challenge is known as the “curse of dimensionality” in Bayesian statistics.\nHowever, to overcome these computational challenges, modern Bayesian analysis uses sampling methods instead of directly calculating the evidence. Instead of computing \\(p(D)\\) directly, we can work with:\n\\[p(\\theta|D) \\propto p(D|\\theta)p(\\theta)\\]\n\n\n\nThe idea behind sampling is to approximate the continuous posterior distribution by generating a large number of discrete samples that represent it. These samples form a histogram that approaches the shape of the true posterior distribution as the number of samples increases. Once we have these samples, we can use them to calculate any summary statistics about the posterior distribution, like means, variances, or credible intervals, without ever needing to compute the normalizing constant \\(p(D)\\).\n\n\n\n\n\n\nSampling and proportionality\n\n\n\nThis proportionality relationship contains all the information needed about the shape of the posterior distribution to generate samples from it. The denominator merely tells us about its height.\n\n\nWe will learn more about sampling methods in future workshops.\nUltimately, Bayesian statistics provides a powerful framework for analyzing experimental data, particularly when we want to update our understanding of underlying processes as we collect new evidence. This approach is especially valuable in scientific experiments where we want to learn about unobservable parameters from observable data.\n\n\n\n\n\n\n\n\n\nBayesian statistics as a means of statistical inference in experiments\n\nThe relationship between parameters (\\(\\theta\\)) and data (\\(D\\)) in statistical inference can be viewed as a two-way process:\n\nForward direction (Experiment): Parameters generate observable data through an experimental process. We can think of this as moving from cause (\\(\\theta\\)) to effect (Data).\nReverse direction (Statistical Inference): We use Bayes’ theorem to reason backwards from observed data to learn about the underlying parameters that generated them.\n\nBayes’ theorem provides the mathematical framework for this inference."
  },
  {
    "objectID": "workshops/02.binomial_globe/qmd/data_and_parameters.html#footnotes",
    "href": "workshops/02.binomial_globe/qmd/data_and_parameters.html#footnotes",
    "title": "Linking data to parameters",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLambert, B. (2018). A student’s guide to Bayesian statistics.↩︎\nGelman, A., & Shalizi, C. R. (2013). Philosophy and the practice of Bayesian statistics. British Journal of Mathematical and Statistical Psychology, 66(1), 8-38.↩︎"
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "“If I have seen further, it is by standing on the shoulders of giants.” - Isaac Newton (1675)"
  },
  {
    "objectID": "conclusion.html#acknowledgments",
    "href": "conclusion.html#acknowledgments",
    "title": "Acknowledgements",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThe course material would not be possible without the prior work of many others. In particular we would like to acknowledge:\n\nSimon Farrell (University of Western Australia), John Kruschke (Indiana University), Ben Lambert (University of Oxford), Stephan Lewandowsky (University of Bristol), Michael Lee (University of California, Irvine), Richard McElreath (Max Planck Institute for Evolutionary Anthropology), and Eric-Jan Wagenmakers (University of Amsterdam) for their excellent textbooks which have heavily shaped the course content and structure\nWoo-Young Ahn (Seoul National University), Nate Haines (Ohio State University), Jan Gläscher (UKE Hamburg) and Antonius Wiehler (ICM, Paris) for their …"
  },
  {
    "objectID": "conclusion.html#future-content",
    "href": "conclusion.html#future-content",
    "title": "Acknowledgements",
    "section": "Future content",
    "text": "Future content\nWe are hoping to further expand the content covered in this course. Examples for additional workshops include:\n\nApplying more computational models (delay discounting, intertemporal choice)\nComputational modeling of decision-making tasks using the hBayesDM package1 in R\nPractical examples of implementing model-based fMRI\n\nSo make sure to stay up-to-date with the website by starring ⭐ the GitHub repository!"
  },
  {
    "objectID": "conclusion.html#footnotes",
    "href": "conclusion.html#footnotes",
    "title": "Acknowledgements",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAhn, W. Y., Haines, N., & Zhang, L. (2017). Revealing neurocomputational mechanisms of reinforcement learning and decision-making with the hBayesDM package. Computational Psychiatry (Cambridge, Mass.), 1, 24.↩︎"
  },
  {
    "objectID": "paper.html",
    "href": "paper.html",
    "title": "BayesCog: A freely available course in Bayesian statistics and Hierarchical Bayesian Modeling for Psychological Science",
    "section": "",
    "text": "We present a semester-long educational course (see 14-week schedule) and Jupyter Book that provides introductory training in specifying, implementing, and interpreting computational models that characterize human social behavior and neuroscience. Through readings, discussions, and labs (e.g., Jupyter Notebook tutorials using Python), students will receive hands-on training in using mathematical models to test specific theories and hypotheses and explain unobservable aspects of complex social cognitive processes and behaviors. These aspects broadly include learning from and for others, learning about others, and social influences on decision-making and mental states.\nBackground in introductory psychological science and basic research methods/statistics is highly recommended prior to participating in this course. Programming experience is helpful, but not required or expected. Although the course content is tailored for undergraduate students, it was developed to be easily adapted for trainees at higher career stages."
  },
  {
    "objectID": "paper.html#module-01---scientific-python",
    "href": "paper.html#module-01---scientific-python",
    "title": "BayesCog: A freely available course in Bayesian statistics and Hierarchical Bayesian Modeling for Psychological Science",
    "section": "Module 01 - Scientific Python",
    "text": "Module 01 - Scientific Python\n\n\n\n\nRun\nView\n\n\n\n\nTutorial: Jupyter Notebooks\nOpen in Colab\nOpen in Book\n\n\nTutorial: Python Basics\nOpen in Colab\nOpen in Book\n\n\nTutorial: Working with Data\nOpen in Colab\nOpen in Book\n\n\nExercise\nOpen in Colab\nOpen in Book"
  },
  {
    "objectID": "paper.html#module-02---introduction-to-modeling",
    "href": "paper.html#module-02---introduction-to-modeling",
    "title": "BayesCog: A freely available course in Bayesian statistics and Hierarchical Bayesian Modeling for Psychological Science",
    "section": "Module 02 - Introduction to Modeling",
    "text": "Module 02 - Introduction to Modeling\n\n\n\n\nRun\nView\n\n\n\n\nTutorial: Linear Modeling\nOpen in Colab\nOpen in Book\n\n\nTutorial: Nonlinear Modeling\nOpen in Colab\nOpen in Book\n\n\nExercise\nOpen in Colab\nOpen in Book"
  },
  {
    "objectID": "paper.html#module-03---reinforcement-learning",
    "href": "paper.html#module-03---reinforcement-learning",
    "title": "BayesCog: A freely available course in Bayesian statistics and Hierarchical Bayesian Modeling for Psychological Science",
    "section": "Module 03 - Reinforcement Learning",
    "text": "Module 03 - Reinforcement Learning\n\n\n\n\nRun\nView\n\n\n\n\nTutorial: Two-Armed Bandit\nOpen in Colab\nOpen in Book\n\n\nTutorial: Models of Learning\nOpen in Colab\nOpen in Book\n\n\nExercise\nOpen in Colab\nOpen in Book"
  },
  {
    "objectID": "paper.html#module-04---social-learning",
    "href": "paper.html#module-04---social-learning",
    "title": "BayesCog: A freely available course in Bayesian statistics and Hierarchical Bayesian Modeling for Psychological Science",
    "section": "Module 04 - Social Learning",
    "text": "Module 04 - Social Learning\n\n\n\n\nRun\nView\n\n\n\n\nTutorial: Prosocial Learning\nOpen in Colab\nOpen in Book\n\n\nExercise\nOpen in Colab\nOpen in Book\n\n\n\nStudents have the option to interact with each tutorial notebook using one of two ways. The first option entails installing Anaconda 3 on their local computers with a custom environment made for this course and then opening each notebook using Jupyter Notebook. Teachers can walk students through the installation steps using the instructions provided on the Getting Started page. However, this can be rather difficult to implement with students’ different machines, operating systems, background, etc. As a fail-safe, students also have the option to use Google Colaboratory, which runs the code on Google’s cloud servers for free. There is a button at the top of each tutorial page that opens each Jupyter Book page in Google Colaboratory.\nOnce students are able to run notebooks (locally or on the cloud), they can interact with all of the code in the tutorials. Teachers should guide students through all of the “tutorials”, which, for example, will allow them to demonstrate to students how tweaking certain variables in real-time would change the outputs. The “exercises” at the end of each module were created for the students to apply what they have learned. They can work on these by themselves or in groups. Links to example solutions for all exercises are provided on each page. In addition, teachers should familiarize themselves with the recommended readings associated with each module and use them to introduce students to each topic, generate discussions and ideas among students, and provide relevant background for each tutorial. For example, the Wilson & Collins (2019) paper can be assigned for reading and discussion prior to beginning any notebook tutorials for Module 03.\nFor additional background on the content covered in each tutorial (technical or theoretical), teachers and students are encouraged to review the references and/or resources listed at the beginning of each tutorial or the non-exhaustive list of resources on the Resources page. Content-related questions from students, teachers, and contributors related to the course can also be submitted directly on the GitHub repository. However, we request that individuals make a good faith effort searching for their answer in the relevant readings or resources listed at the beginning of each tutorial before submitting questions."
  }
]