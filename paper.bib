
@article{palminteri_importance_2017,
	title = {The {Importance} of {Falsification} in {Computational} {Cognitive} {Modeling}},
	volume = {21},
	issn = {1364-6613, 1879-307X},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(17)30054-2},
	doi = {10.1016/j.tics.2017.03.011},
	language = {English},
	number = {6},
	urldate = {2025-01-06},
	journal = {Trends in Cognitive Sciences},
	author = {Palminteri, Stefano and Wyart, Valentin and Koechlin, Etienne},
	month = jun,
	year = {2017},
	pmid = {28476348},
	note = {Publisher: Elsevier},
	pages = {425--433},
}

@book{farrell_computational_2018,
	title = {Computational {Modeling} of {Cognition} and {Behavior}},
	isbn = {978-1-108-54824-3},
	abstract = {Computational modeling is now ubiquitous in psychology, and researchers who are not modelers may find it increasingly difficult to follow the theoretical developments in their field. This book presents an integrated framework for the development and application of models in psychology and related disciplines. Researchers and students are given the knowledge and tools to interpret models published in their area, as well as to develop, fit, and test their own models. Both the development of models and key features of any model are covered, as are the applications of models in a variety of domains across the behavioural sciences. A number of chapters are devoted to fitting models using maximum likelihood and Bayesian estimation, including fitting hierarchical and mixture models. Model comparison is described as a core philosophy of scientific inference, and the use of models to understand theories and advance scientific discourse is explained.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Farrell, Simon and Lewandowsky, Stephan},
	month = feb,
	year = {2018},
	note = {Google-Books-ID: VMhJDwAAQBAJ},
	keywords = {Psychology / Cognitive Psychology \& Cognition, Psychology / General, Psychology / Movements / Jungian, Psychology / Research \& Methodology, Science / Life Sciences / Neuroscience, Social Science / Sociology / General},
}

@article{de_hollander_different_2016,
	title = {Different {Ways} of {Linking} {Behavioral} and {Neural} {Data} via {Computational} {Cognitive} {Models}},
	volume = {1},
	issn = {2451-9022},
	url = {https://www.sciencedirect.com/science/article/pii/S2451902215000166},
	doi = {10.1016/j.bpsc.2015.11.004},
	abstract = {Cognitive neuroscientists sometimes apply formal models to investigate how the brain implements cognitive processes. These models describe behavioral data in terms of underlying, latent variables linked to hypothesized cognitive processes. A goal of model-based cognitive neuroscience is to link these variables to brain measurements, which can advance progress in both cognitive and neuroscientific research. However, the details and the philosophical approach for this linking problem can vary greatly. We propose a continuum of approaches that differ in the degree of tight, quantitative, and explicit hypothesizing. We describe this continuum using four points along it, which we dub qualitative structural, qualitative predictive, quantitative predictive, and single model linking approaches. We further illustrate by providing examples from three research fields (decision making, reinforcement learning, and symbolic reasoning) for the different linking approaches.},
	number = {2},
	urldate = {2025-01-06},
	journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
	author = {de Hollander, Gilles and Forstmann, Birte U. and Brown, Scott D.},
	month = mar,
	year = {2016},
	keywords = {Cognition, Computational models, Functional neuroimaging, Joint modeling, Linking, Mathematical models},
	pages = {101--109},
}

@article{zhang_using_2020,
	title = {Using reinforcement learning models in social neuroscience: frameworks, pitfalls and suggestions of best practices},
	volume = {15},
	issn = {1749-5016},
	shorttitle = {Using reinforcement learning models in social neuroscience},
	url = {https://doi.org/10.1093/scan/nsaa089},
	doi = {10.1093/scan/nsaa089},
	abstract = {The recent years have witnessed a dramatic increase in the use of reinforcement learning (RL) models in social, cognitive and affective neuroscience. This approach, in combination with neuroimaging techniques such as functional magnetic resonance imaging, enables quantitative investigations into latent mechanistic processes. However, increased use of relatively complex computational approaches has led to potential misconceptions and imprecise interpretations. Here, we present a comprehensive framework for the examination of (social) decision-making with the simple Rescorla–Wagner RL model. We discuss common pitfalls in its application and provide practical suggestions. First, with simulation, we unpack the functional role of the learning rate and pinpoint what could easily go wrong when interpreting differences in the learning rate. Then, we discuss the inevitable collinearity between outcome and prediction error in RL models and provide suggestions of how to justify whether the observed neural activation is related to the prediction error rather than outcome valence. Finally, we suggest posterior predictive check is a crucial step after model comparison, and we articulate employing hierarchical modeling for parameter estimation. We aim to provide simple and scalable explanations and practical guidelines for employing RL models to assist both beginners and advanced users in better implementing and interpreting their model-based analyses.},
	number = {6},
	urldate = {2025-01-06},
	journal = {Social Cognitive and Affective Neuroscience},
	author = {Zhang, Lei and Lengersdorff, Lukas and Mikus, Nace and Gläscher, Jan and Lamm, Claus},
	month = jul,
	year = {2020},
	pages = {695--707},
}

@article{eckstein_what_2021,
	series = {Value based decision-making},
	title = {What do reinforcement learning models measure? {Interpreting} model parameters in cognition and neuroscience},
	volume = {41},
	issn = {2352-1546},
	shorttitle = {What do reinforcement learning models measure?},
	url = {https://www.sciencedirect.com/science/article/pii/S2352154621001236},
	doi = {10.1016/j.cobeha.2021.06.004},
	abstract = {Reinforcement learning (RL) is a concept that has been invaluable to fields including machine learning, neuroscience, and cognitive science. However, what RL entails differs between fields, leading to difficulties when interpreting and translating findings. After laying out these differences, this paper focuses on cognitive (neuro)science to discuss how we as a field might overinterpret RL modeling results. We too often assume—implicitly—that modeling results generalize between tasks, models, and participant populations, despite negative empirical evidence for this assumption. We also often assume that parameters measure specific, unique (neuro)cognitive processes, a concept we call interpretability, when evidence suggests that they capture different functions across studies and tasks. We conclude that future computational research needs to pay increased attention to implicit assumptions when using RL models, and suggest that a more systematic understanding of contextual factors will help address issues and improve the ability of RL to explain brain and behavior.},
	urldate = {2025-01-06},
	journal = {Current Opinion in Behavioral Sciences},
	author = {Eckstein, Maria K and Wilbrecht, Linda and Collins, Anne GE},
	month = oct,
	year = {2021},
	pages = {128--137},
}

@article{carpenter_stan_2017,
	title = {Stan: {A} {Probabilistic} {Programming} {Language}},
	volume = {76},
	issn = {1548-7660},
	shorttitle = {Stan},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9788645/},
	doi = {10.18637/jss.v076.i01},
	abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm., Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible., Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
	urldate = {2025-01-06},
	journal = {Journal of statistical software},
	author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus A. and Guo, Jiqiang and Li, Peter and Riddell, Allen},
	year = {2017},
	pmid = {36568334},
	pmcid = {PMC9788645},
	pages = {1},
}

@article{lee_how_2011,
	series = {Special {Issue} on {Hierarchical} {Bayesian} {Models}},
	title = {How cognitive modeling can benefit from hierarchical {Bayesian} models},
	volume = {55},
	issn = {0022-2496},
	url = {https://www.sciencedirect.com/science/article/pii/S0022249610001148},
	doi = {10.1016/j.jmp.2010.08.013},
	abstract = {Hierarchical Bayesian modeling provides a flexible and interpretable way of extending simple models of cognitive processes. To introduce this special issue, we discuss four of the most important potential hierarchical Bayesian contributions. The first involves the development of more complete theories, including accounting for variation coming from sources like individual differences in cognition. The second involves the capability to account for observed behavior in terms of the combination of multiple different cognitive processes. The third involves using a few key psychological variables to explain behavior on a wide range of cognitive tasks. The fourth involves the conceptual unification and integration of disparate cognitive models. For all of these potential contributions, we outline an appropriate general hierarchical Bayesian modeling structure. We also highlight current models that already use the hierarchical Bayesian approach, as well as identifying research areas that could benefit from its adoption.},
	number = {1},
	urldate = {2025-01-06},
	journal = {Journal of Mathematical Psychology},
	author = {Lee, Michael D.},
	month = feb,
	year = {2011},
	pages = {1--7},
}

@article{huys_computational_2016,
	title = {Computational psychiatry as a bridge from neuroscience to clinical applications},
	volume = {19},
	copyright = {2016 Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn.4238},
	doi = {10.1038/nn.4238},
	abstract = {The complexity of problems and data in psychiatry requires powerful computational approaches. Computational psychiatry is an emerging field encompassing mechanistic theory-driven models and theoretically agnostic data-driven analyses that use machine-learning techniques. Clinical applications will benefit from relating theoretically meaningful process variables to complex psychiatric outcomes through data-driven techniques.},
	language = {en},
	number = {3},
	urldate = {2025-01-06},
	journal = {Nature Neuroscience},
	author = {Huys, Quentin J. M. and Maia, Tiago V. and Frank, Michael J.},
	month = mar,
	year = {2016},
	note = {Publisher: Nature Publishing Group},
	keywords = {Learning algorithms, Psychiatric disorders},
	pages = {404--413},
}

@article{hauser_promise_2022,
	title = {The promise of a model-based psychiatry: building computational models of mental ill health},
	volume = {4},
	issn = {2589-7500},
	shorttitle = {The promise of a model-based psychiatry},
	url = {https://www.thelancet.com/journals/landig/article/PIIS2589-7500(22)00152-2/fulltext?trk=organization_guest_main-feed-card_feed-article-content},
	doi = {10.1016/S2589-7500(22)00152-2},
	language = {English},
	number = {11},
	urldate = {2025-01-06},
	journal = {The Lancet Digital Health},
	author = {Hauser, Tobias U. and Skvortsova, Vasilisa and Choudhury, Munmun De and Koutsouleris, Nikolaos},
	month = nov,
	year = {2022},
	pmid = {36229345},
	note = {Publisher: Elsevier},
	pages = {e816--e828},
}

@article{annis_bayesian_2018,
	title = {Bayesian statistical approaches to evaluating cognitive models},
	volume = {9},
	copyright = {© 2017 Wiley Periodicals, Inc.},
	issn = {1939-5086},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wcs.1458},
	doi = {10.1002/wcs.1458},
	abstract = {Cognitive models aim to explain complex human behavior in terms of hypothesized mechanisms of the mind. These mechanisms can be formalized in terms of mathematical structures containing parameters that are theoretically meaningful. For example, in the case of perceptual decision making, model parameters might correspond to theoretical constructs like response bias, evidence quality, response caution, and the like. Formal cognitive models go beyond verbal models in that cognitive mechanisms are instantiated in terms of mathematics and they go beyond statistical models in that cognitive model parameters are psychologically interpretable. We explore three key elements used to formally evaluate cognitive models: parameter estimation, model prediction, and model selection. We compare and contrast traditional approaches with Bayesian statistical approaches to performing each of these three elements. Traditional approaches rely on an array of seemingly ad hoc techniques, whereas Bayesian statistical approaches rely on a single, principled, internally consistent system. We illustrate the Bayesian statistical approach to evaluating cognitive models using a running example of the Linear Ballistic Accumulator model of decision making (Brown SD, Heathcote A. The simplest complete model of choice response time: linear ballistic accumulation. Cogn Psychol 2008, 57:153–178). WIREs Cogn Sci 2018, 9:e1458. doi: 10.1002/wcs.1458 This article is categorized under: Neuroscience {\textgreater} Computation Psychology {\textgreater} Reasoning and Decision Making Psychology {\textgreater} Theory and Methods},
	language = {en},
	number = {2},
	urldate = {2025-01-06},
	journal = {WIREs Cognitive Science},
	author = {Annis, Jeffrey and Palmeri, Thomas J.},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wcs.1458},
	pages = {e1458},
}

@article{guest_how_2021,
	title = {How {Computational} {Modeling} {Can} {Force} {Theory} {Building} in {Psychological} {Science}},
	volume = {16},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691620970585},
	doi = {10.1177/1745691620970585},
	abstract = {Psychology endeavors to develop theories of human capacities and behaviors on the basis of a variety of methodologies and dependent measures. We argue that one of the most divisive factors in psychological science is whether researchers choose to use computational modeling of theories (over and above data) during the scientific-inference process. Modeling is undervalued yet holds promise for advancing psychological science. The inherent demands of computational modeling guide us toward better science by forcing us to conceptually analyze, specify, and formalize intuitions that otherwise remain unexamined—what we dub open theory. Constraining our inference process through modeling enables us to build explanatory and predictive theories. Here, we present scientific inference in psychology as a path function in which each step shapes the next. Computational modeling can constrain these steps, thus advancing scientific inference over and above the stewardship of experimental practice (e.g., preregistration). If psychology continues to eschew computational modeling, we predict more replicability crises and persistent failure at coherent theory building. This is because without formal modeling we lack open and transparent theorizing. We also explain how to formalize, specify, and implement a computational model, emphasizing that the advantages of modeling can be achieved by anyone with benefit to all.},
	language = {en},
	number = {4},
	urldate = {2025-01-06},
	journal = {Perspectives on Psychological Science},
	author = {Guest, Olivia and Martin, Andrea E.},
	month = jul,
	year = {2021},
	note = {Publisher: SAGE Publications Inc},
	pages = {789--802},
}

@article{rocca_putting_2021,
	title = {Putting {Psychology} to the {Test}: {Rethinking} {Model} {Evaluation} {Through} {Benchmarking} and {Prediction}},
	volume = {4},
	issn = {2515-2459},
	shorttitle = {Putting {Psychology} to the {Test}},
	url = {https://doi.org/10.1177/25152459211026864},
	doi = {10.1177/25152459211026864},
	abstract = {Consensus on standards for evaluating models and theories is an integral part of every science. Nonetheless, in psychology, relatively little focus has been placed on defining reliable communal metrics to assess model performance. Evaluation practices are often idiosyncratic and are affected by a number of shortcomings (e.g., failure to assess models’ ability to generalize to unseen data) that make it difficult to discriminate between good and bad models. Drawing inspiration from fields such as machine learning and statistical genetics, we argue in favor of introducing common benchmarks as a means of overcoming the lack of reliable model evaluation criteria currently observed in psychology. We discuss a number of principles benchmarks should satisfy to achieve maximal utility, identify concrete steps the community could take to promote the development of such benchmarks, and address a number of potential pitfalls and concerns that may arise in the course of implementation. We argue that reaching consensus on common evaluation benchmarks will foster cumulative progress in psychology and encourage researchers to place heavier emphasis on the practical utility of scientific models.},
	language = {en},
	number = {3},
	urldate = {2025-01-06},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Rocca, Roberta and Yarkoni, Tal},
	month = jul,
	year = {2021},
	note = {Publisher: SAGE Publications Inc},
	pages = {25152459211026864},
}

@article{feldmanhall_computational_2021,
	title = {The computational challenge of social learning},
	volume = {25},
	issn = {1364-6613, 1879-307X},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(21)00229-1},
	doi = {10.1016/j.tics.2021.09.002},
	language = {English},
	number = {12},
	urldate = {2025-01-06},
	journal = {Trends in Cognitive Sciences},
	author = {FeldmanHall, Oriel and Nassar, Matthew R.},
	month = dec,
	year = {2021},
	pmid = {34583876},
	note = {Publisher: Elsevier},
	keywords = {computational modeling, coordination, emotion, inference, reward, social learning, uncertainty},
	pages = {1045--1057},
}

@book{lambert_students_2018,
	title = {A {Student}’s {Guide} to {Bayesian} {Statistics}},
	isbn = {978-1-5264-1828-9},
	abstract = {Supported by a wealth of learning features, exercises, and visual elements as well as online video tutorials and interactive simulations, this book is the first student-focused introduction to Bayesian statistics.  Without sacrificing technical integrity for the sake of simplicity, the author draws upon accessible, student-friendly language to provide approachable instruction perfectly aimed at statistics and Bayesian newcomers. Through a logical structure that introduces and builds upon key concepts in a gradual way and slowly acclimatizes students to using R and Stan software, the book covers:  An introduction to probability and Bayesian inference Understanding Bayes′ rule  Nuts and bolts of Bayesian analytic methods Computational Bayes and real-world Bayesian analysis Regression analysis and hierarchical methods  This unique guide will help students develop the statistical confidence and skills to put the Bayesian formula into practice, from the basic concepts of statistical inference to complex applications of analyses.},
	language = {en},
	publisher = {SAGE},
	author = {Lambert, Ben},
	month = apr,
	year = {2018},
	note = {Google-Books-ID: ZvBUDwAAQBAJ},
	keywords = {Mathematics / Probability \& Statistics / Bayesian Analysis, Reference / Research, Social Science / Methodology, Social Science / Research},
}

@book{mcelreath_statistical_2018,
	address = {New York},
	title = {Statistical {Rethinking}: {A} {Bayesian} {Course} with {Examples} in {R} and {Stan}},
	isbn = {978-1-315-37249-5},
	shorttitle = {Statistical {Rethinking}},
	abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds readers’ knowledge of and confidence in statistical modeling. Reflecting the need for even minor programming in today’s model-based statistics, the book pushes readers to perform step-by-step calculations that are usually automated. This unique computational approach ensures that readers understand enough of the details to make reasonable choices and interpretations in their own modeling work.

The text presents generalized linear multilevel models from a Bayesian perspective, relying on a simple logical interpretation of Bayesian probability and maximum entropy. It covers from the basics of regression to multilevel models. The author also discusses measurement error, missing data, and Gaussian process models for spatial and network autocorrelation.

By using complete R code examples throughout, this book provides a practical foundation for performing statistical inference. Designed for both PhD students and seasoned professionals in the natural and social sciences, it prepares them for more advanced or specialized statistical modeling. 
Web ResourceThe book is accompanied by an R package (rethinking) that is available on the author’s website and GitHub. The two core functions (map and map2stan) of this package allow a variety of statistical models to be constructed from standard model formulas.},
	publisher = {Chapman and Hall/CRC},
	author = {McElreath, Richard},
	month = jan,
	year = {2018},
	doi = {10.1201/9781315372495},
}

@book{kruschke_doing_2014,
	title = {Doing {Bayesian} {Data} {Analysis}: {A} {Tutorial} with {R}, {JAGS}, and {Stan}},
	isbn = {978-0-12-405916-0},
	shorttitle = {Doing {Bayesian} {Data} {Analysis}},
	abstract = {Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan, Second Edition provides an accessible approach for conducting Bayesian data analysis, as material is explained clearly with concrete examples. Included are step-by-step instructions on how to carry out Bayesian data analyses in the popular and free software R and WinBugs, as well as new programs in JAGS and Stan. The new programs are designed to be much easier to use than the scripts in the first edition. In particular, there are now compact high-level scripts that make it easy to run the programs on your own data sets. The book is divided into three parts and begins with the basics: models, probability, Bayes' rule, and the R programming language. The discussion then moves to the fundamentals applied to inferring a binomial probability, before concluding with chapters on the generalized linear model. Topics include metric-predicted variable on one or two groups; metric-predicted variable with one metric predictor; metric-predicted variable with multiple metric predictors; metric-predicted variable with one nominal predictor; and metric-predicted variable with multiple nominal predictors. The exercises found in the text have explicit purposes and guidelines for accomplishment. This book is intended for first-year graduate students or advanced undergraduates in statistics, data analysis, psychology, cognitive science, social sciences, clinical sciences, and consumer sciences in business. - Accessible, including the basics of essential concepts of probability and random sampling - Examples with R programming language and JAGS software - Comprehensive coverage of all scenarios addressed by non-Bayesian textbooks: t-tests, analysis of variance (ANOVA) and comparisons in ANOVA, multiple regression, and chi-square (contingency table analysis) - Coverage of experiment planning - R and JAGS computer programming code on website - Exercises have explicit purposes and guidelines for accomplishment - Provides step-by-step instructions on how to conduct Bayesian data analyses in the popular and free software R and WinBugs},
	language = {en},
	publisher = {Academic Press},
	author = {Kruschke, John},
	month = nov,
	year = {2014},
	note = {Google-Books-ID: FzvLAwAAQBAJ},
	keywords = {Mathematics / Mathematical Analysis, Mathematics / Probability \& Statistics / General},
}

@book{lee_bayesian_2014,
	title = {Bayesian {Cognitive} {Modeling}: {A} {Practical} {Course}},
	isbn = {978-1-107-65391-7},
	shorttitle = {Bayesian {Cognitive} {Modeling}},
	abstract = {Bayesian inference has become a standard method of analysis in many fields of science. Students and researchers in experimental psychology and cognitive science, however, have failed to take full advantage of the new and exciting possibilities that the Bayesian approach affords. Ideal for teaching and self study, this book demonstrates how to do Bayesian modeling. Short, to-the-point chapters offer examples, exercises, and computer code (using WinBUGS or JAGS, and supported by Matlab and R), with additional support available online. No advance knowledge of statistics is required and, from the very start, readers are encouraged to apply and adjust Bayesian analyses by themselves. The book contains a series of chapters on parameter estimation and model selection, followed by detailed case studies from cognitive science. After working through this book, readers should be able to build their own Bayesian models, apply the models to their own data, and draw their own conclusions.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Lee, Michael D. and Wagenmakers, Eric-Jan},
	month = apr,
	year = {2014},
	note = {Google-Books-ID: Gq6kAgAAQBAJ},
	keywords = {Computers / Artificial Intelligence / General, Psychology / Cognitive Psychology \& Cognition, Psychology / Experimental Psychology, Psychology / General, Psychology / Research \& Methodology},
}

@article{lockwood_computational_2021,
	title = {Computational modelling of social cognition and behaviour-a reinforcement learning primer},
	volume = {16},
	copyright = {cc by},
	issn = {1749-5024},
	url = {https://europepmc.org/articles/PMC8343561},
	doi = {10.1093/scan/nsaa040},
	abstract = {Social neuroscience aims to describe the neural systems that underpin social cognition and behaviour. Over the past decade, researchers have begun to combine computational models with neuroimaging to link social computations to the brain. Inspired by approaches from reinforcement learning theory, which describes how decisions are driven by the unexpectedness of outcomes, accounts of the neural basis of prosocial learning, observational learning, mentalizing and impression formation have been developed. Here we provide an introduction for researchers who wish to use these models in their studies. We consider both theoretical and practical issues related to their implementation, with a focus on specific examples from the field.},
	language = {eng},
	number = {8},
	urldate = {2025-01-06},
	journal = {Social cognitive and affective neuroscience},
	author = {Lockwood, Patricia L and Klein-Flügge, Miriam C},
	month = aug,
	year = {2021},
	pmid = {32232358},
	pmcid = {PMC8343561},
	keywords = {Computational Modelling, Model Fitting, Model Selection, Reinforcement Learning, Reward, Social},
	pages = {761--771},
}

@article{wilson_ten_2019,
	title = {Ten simple rules for the computational modeling of behavioral data},
	volume = {8},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.49547},
	doi = {10.7554/eLife.49547},
	abstract = {Computational modeling of behavior has revolutionized psychology and neuroscience. By fitting models to experimental data we can probe the algorithms underlying behavior, find neural correlates of computational variables and better understand the effects of drugs, illness and interventions. But with great power comes great responsibility. Here, we offer ten simple rules to ensure that computational modeling is used with care and yields meaningful insights. In particular, we present a beginner-friendly, pragmatic and details-oriented introduction on how to relate models to data. What, exactly, can a model tell us about the mind? To answer this, we apply our rules to the simplest modeling techniques most accessible to beginning modelers and illustrate them with examples and code available online. However, most rules apply to more advanced techniques. Our hope is that by following our guidelines, researchers will avoid many pitfalls and unleash the power of computational modeling on their own data.},
	urldate = {2025-01-06},
	journal = {eLife},
	author = {Wilson, Robert C and Collins, Anne GE},
	editor = {Behrens, Timothy E},
	month = nov,
	year = {2019},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {computational modeling, model fitting, reproducibility, validation},
	pages = {e49547},
}

@article{alessandroni_ten_2022,
	title = {Ten {Strategies} to {Foster} {Open} {Science} in {Psychology} and {Beyond}},
	volume = {8},
	issn = {2474-7394},
	url = {https://doi.org/10.1525/collabra.57545},
	doi = {10.1525/collabra.57545},
	abstract = {The scientific community has long recognized the benefits of open science. Today, governments and research agencies worldwide are increasingly promoting and mandating open practices for scientific research. However, for open science to become the by-default model for scientific research, researchers must perceive open practices as accessible and achievable. A significant obstacle is the lack of resources providing a clear direction on how researchers can integrate open science practices in their day-to-day workflows. This article outlines and discusses ten concrete strategies that can help researchers use and disseminate open science. The first five strategies address basic ways of getting started in open science that researchers can put into practice today. The last five strategies are for researchers who are more advanced in open practices to advocate for open science. Our paper will help researchers navigate the transition to open science practices and support others in shifting toward openness, thus contributing to building a better science.},
	number = {1},
	urldate = {2025-01-06},
	journal = {Collabra: Psychology},
	author = {Alessandroni, Nicolás and Byers-Heinlein, Krista},
	editor = {Corker, Katie},
	month = dec,
	year = {2022},
	pages = {57545},
}

@book{marr_vision_2010,
	title = {Vision: {A} {Computational} {Investigation} into the {Human} {Representation} and {Processing} of {Visual} {Information}},
	isbn = {978-0-262-28898-9},
	shorttitle = {Vision},
	abstract = {Available again, an influential book that offers a framework for understanding visual perception and considers fundamental questions about the brain and its functions.David Marr's posthumously published Vision (1982) influenced a generation of brain and cognitive scientists, inspiring many to enter the field. In Vision, Marr describes a general framework for understanding visual perception and touches on broader questions about how the brain and its functions can be studied and understood. Researchers from a range of brain and cognitive sciences have long valued Marr's creativity, intellectual power, and ability to integrate insights and data from neuroscience, psychology, and computation. This MIT Press edition makes Marr's influential work available to a new generation of students and scientists. In Marr's framework, the process of vision constructs a set of representations, starting from a description of the input image and culminating with a description of three-dimensional objects in the surrounding environment. A central theme, and one that has had far-reaching influence in both neuroscience and cognitive science, is the notion of different levels of analysis—in Marr's framework, the computational level, the algorithmic level, and the hardware implementation level. Now, thirty years later, the main problems that occupied Marr remain fundamental open problems in the study of perception. Vision provides inspiration for the continuing efforts to integrate knowledge from cognition and computation to understand vision and the brain.},
	language = {en},
	publisher = {MIT Press},
	author = {Marr, David},
	month = jul,
	year = {2010},
	note = {Google-Books-ID: D8XxCwAAQBAJ},
	keywords = {Medical / Neuroscience, Psychology / Cognitive Psychology \& Cognition},
}

@article{press_building_2022,
	title = {Building better theories},
	volume = {32},
	issn = {0960-9822},
	url = {https://www.cell.com/current-biology/abstract/S0960-9822(21)01551-7},
	doi = {10.1016/j.cub.2021.11.027},
	language = {English},
	number = {1},
	urldate = {2025-01-06},
	journal = {Current Biology},
	author = {Press, Clare and Yon, Daniel and Heyes, Cecilia},
	month = jan,
	year = {2022},
	pmid = {35015984},
	note = {Publisher: Elsevier},
	pages = {R13--R17},
}

@article{baribault_troubleshooting_2023,
	title = {Troubleshooting {Bayesian} cognitive models.},
	copyright = {http://www.apa.org/pubs/journals/resources/open-access.aspx},
	issn = {1939-1463, 1082-989X},
	url = {https://doi.apa.org/doi/10.1037/met0000554},
	doi = {10.1037/met0000554},
	abstract = {Using Bayesian methods to apply computational models of cognitive processes, or Bayesian cognitive modeling, is an important new trend in psychological research. The rise of Bayesian cognitive modeling has been accelerated by the introduction of software that efficiently automates the Markov chain Monte Carlo sampling used for Bayesian model fitting — including the popular Stan and PyMC packages, which automate the dynamic Hamiltonian Monte Carlo and No-U-Turn Sampler (HMC/NUTS) algorithms that we spotlight here. Unfortunately, Bayesian cognitive models can struggle to pass the growing number of diagnostic checks required of Bayesian models. If any failures are left undetected, inferences about cognition based on the model’s output may be biased or incorrect. As such, Bayesian cognitive models almost always require troubleshooting before being used for inference. Here, we present a deep treatment of the diagnostic checks and procedures that are critical for effective troubleshooting, but are often left underspecified by tutorial papers. After a conceptual introduction to Bayesian cognitive modeling and HMC/NUTS sampling, we outline the diagnostic metrics, procedures, and plots necessary to detect problems in model output with an emphasis on how these requirements have recently been changed and extended. Throughout, we explain how uncovering the exact nature of the problem is often the key to identifying solutions. We also demonstrate the troubleshooting process for an example hierarchical Bayesian model of reinforcement learning, including supplementary code. With this comprehensive guide to techniques for detecting, identifying, and overcoming problems in fitting Bayesian cognitive models, psychologists across subfields can more confidently build and use Bayesian cognitive models in their research.},
	language = {en},
	urldate = {2025-01-06},
	journal = {Psychological Methods},
	author = {Baribault, Beth and Collins, Anne G. E.},
	month = mar,
	year = {2023},
}

@article{glascher_model-based_2010,
	title = {Model-based approaches to neuroimaging: combining reinforcement learning theory with {fMRI} data},
	volume = {1},
	copyright = {Copyright © 2010 John Wiley \& Sons, Ltd.},
	issn = {1939-5086},
	shorttitle = {Model-based approaches to neuroimaging},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wcs.57},
	doi = {10.1002/wcs.57},
	abstract = {The combination of functional magnetic resonance imaging (fMRI) with computational models for a given cognitive process provides a powerful framework for testing hypotheses about the neural computations underlying such processes in the brain. Here, we outline the steps involved in implementing this approach with reference to the application of reinforcement learning (RL) models that can account for human choice behavior during value-based decision making. The model generates internal variables which can be used to construct fMRI predictor variables and regressed against individual subjects' fMRI data. The resulting regression coefficients reflect the strength of the correlation with blood oxygenation level dependent (BOLD) activity and the relevant internal variables from the model. In the second part of this review, we describe human neuroimaging studies that have employed this analysis strategy to identify brain regions involved in the computations mediating reward-related decision making. Copyright © 2010 John Wiley \& Sons, Ltd. This article is categorized under: Neuroscience {\textgreater} Cognition},
	language = {en},
	number = {4},
	urldate = {2025-01-06},
	journal = {WIREs Cognitive Science},
	author = {Gläscher, Jan P. and O'Doherty, John P.},
	year = {2010},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wcs.57},
	pages = {501--510},
}

@article{ahn_revealing_2017,
	title = {Revealing {Neurocomputational} {Mechanisms} of {Reinforcement} {Learning} and {Decision}-{Making} {With} the {hBayesDM} {Package}},
	volume = {1},
	issn = {2379-6227},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5869013/},
	doi = {10.1162/CPSY_a_00002},
	abstract = {Reinforcement learning and decision-making (RLDM) provide a quantitative framework and computational theories with which we can disentangle psychiatric conditions into the basic dimensions of neurocognitive functioning. RLDM offer a novel approach to assessing and potentially diagnosing psychiatric patients, and there is growing enthusiasm for both RLDM and computational psychiatry among clinical researchers. Such a framework can also provide insights into the brain substrates of particular RLDM processes, as exemplified by model-based analysis of data from functional magnetic resonance imaging (fMRI) or electroencephalography (EEG). However, researchers often find the approach too technical and have difficulty adopting it for their research. Thus, a critical need remains to develop a user-friendly tool for the wide dissemination of computational psychiatric methods. We introduce an R package called hBayesDM (hierarchical Bayesian modeling of Decision-Making tasks), which offers computational modeling of an array of RLDM tasks and social exchange games. The hBayesDM package offers state-of-the-art hierarchical Bayesian modeling, in which both individual and group parameters (i.e., posterior distributions) are estimated simultaneously in a mutually constraining fashion. At the same time, the package is extremely user-friendly: users can perform computational modeling, output visualization, and Bayesian model comparisons, each with a single line of coding. Users can also extract the trial-by-trial latent variables (e.g., prediction errors) required for model-based fMRI/EEG. With the hBayesDM package, we anticipate that anyone with minimal knowledge of programming can take advantage of cutting-edge computational-modeling approaches to investigate the underlying processes of and interactions between multiple decision-making (e.g., goal-directed, habitual, and Pavlovian) systems. In this way, we expect that the hBayesDM package will contribute to the dissemination of advanced modeling approaches and enable a wide range of researchers to easily perform computational psychiatric research within different populations.},
	urldate = {2025-01-06},
	journal = {Computational Psychiatry (Cambridge, Mass.)},
	author = {Ahn, Woo-Young and Haines, Nathaniel and Zhang, Lei},
	month = oct,
	year = {2017},
	pmid = {29601060},
	pmcid = {PMC5869013},
	pages = {24--57},
}

@article{maia_theory-based_2017,
	title = {Theory-{Based} {Computational} {Psychiatry}},
	volume = {82},
	issn = {0006-3223, 1873-2402},
	url = {https://www.biologicalpsychiatryjournal.com/article/S0006-3223(17)31816-4/fulltext},
	doi = {10.1016/j.biopsych.2017.07.016},
	language = {English},
	number = {6},
	urldate = {2025-01-06},
	journal = {Biological Psychiatry},
	author = {Maia, Tiago V. and Huys, Quentin J. M. and Frank, Michael J.},
	month = sep,
	year = {2017},
	note = {Publisher: Elsevier},
	pages = {382--384},
}
